{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE REQUIREMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrasterio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrasterio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m     19\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m uniform_filter\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# !pip install python-dotenv\n",
    "# load_dotenv()\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings \n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as rio\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy.ndimage.filters import uniform_filter\n",
    "from scipy.ndimage.measurements import variance\n",
    "from skimage.filters import threshold_minimum\n",
    "from datacube.utils.geometry import Geometry\n",
    "\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.plotting import display_map, rgb\n",
    "from deafrica_tools.areaofinterest import define_area\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-Drive Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import google.auth\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# If modifying these scopes, delete the file credentials.\n",
    "SCOPES = [\"https://www.googleapis.com/auth/drive\"]\n",
    "credential_path = '../creds/credentials_2.json'\n",
    "u_credential_path = '../creds/token.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "\n",
    "Connect to the datacube so we can access DEA data.\n",
    "The `app` parameter is a unique name for the analysis which is based on the notebook file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"Radar_water_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G-DRIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-Drive Folder IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOOD_MEAN_ID = \"196YsHy1SXjNDnja6LjVhhlvoIt-Jg91l\"\n",
    "FLOOD_MEDIAN_ID = \"1Qhum99pKi1Qyon8hcp4K8S_DGaJh5a5e\"\n",
    "PREFLOOD_MEAN_ID = \"1K0KqGlLxdUCsXg4771k_uf_x2UwbHlqa\"\n",
    "PREFLOOD_MEDIAN_ID = \"1Ovu5Q58xZRGpKsvowVr49klOrgVoZ0XQ\"\n",
    "\n",
    "FLOOD_MEAN_TEST_ID = \"1bv76i244wyJzfpWN57wM7Yh4jQ38gove\"\n",
    "PREFLOOD_MEAN_TEST_ID = \"1msS6VuX_8UptluKIuQG5PJNOrwqYtacY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-Drive Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_token():\n",
    "    creds = None\n",
    "    if os.path.exists(u_credential_path):\n",
    "        creds = Credentials.from_authorized_user_file(u_credential_path, SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "          creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(credential_path, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open(u_credential_path, \"w\") as token:\n",
    "        token.write(creds.to_json())\n",
    "    return creds\n",
    "    \n",
    "def create_token():\n",
    "    ''''\n",
    "        credential: provide the json creditials you would get from google service.\n",
    "    '''\n",
    "    creds = None\n",
    "    creds = service_account.Credentials.from_service_account_file(credential_path, scopes=SCOPES)\n",
    "    return creds\n",
    "\n",
    "def list_gdrive():\n",
    "    '''\n",
    "    List the 10 recent files from the google drive\n",
    "    '''\n",
    "    # creds = create_token()\n",
    "    u_creds = create_user_token()\n",
    "    try:\n",
    "        service = build(\"drive\", \"v3\", credentials=u_creds)\n",
    "      \n",
    "        results = (service.files().list(pageSize=20, fields=\"nextPageToken, files(id, name)\").execute())\n",
    "        items = results.get(\"files\", [])\n",
    "\n",
    "        if not items:\n",
    "            print(\"No files found.\")\n",
    "            return\n",
    "        print(\"Files:\")\n",
    "        for item in items:\n",
    "            print(f\"{item['name']} ({item['id']})\")\n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "\n",
    "\n",
    "def upload_to_gdrive(file_paths, folder_id=None):\n",
    "    '''\n",
    "        Uploading files to google drive\n",
    "    '''\n",
    "    creds = create_token()\n",
    "\n",
    "    for f_path in file_paths:\n",
    "        try:\n",
    "            # create drive api client\n",
    "            service = build(\"drive\", \"v3\", credentials=creds)\n",
    "            folder_path = '../Supplementary_data/DriveCredentials/{}'.format(folder_id)\n",
    "            \n",
    "            f_name = os.path.basename(f_path)\n",
    "            file_metadata = {\"name\": f_name, \"parents\": [folder_id]}\n",
    "            media = MediaFileUpload(f_path, resumable=True)\n",
    "            # pylint: disable=maybe-no-member\n",
    "            \n",
    "            file = (service.files().create(body=file_metadata, media_body=media).execute())\n",
    "            print('\\033[32m' + \"{} UPLOADED SUCCESSFULLY\".format(f_name) + '\\033[0m')\n",
    "    \n",
    "            # DELETE FROM SANDBOX TO SAVE DISC MEMORY\n",
    "            os.remove(f_path)\n",
    "        except HttpError as error:\n",
    "            print(f\"An error occurred: {error}\")\n",
    "            file = None\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1l\u001b>Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=975171912910-05mf229112maahcr4pn6fvvuhmdt1jlm.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A52075%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&state=IKzulGbOARVtq6m0z5BkuoFtp200KP&access_type=offline                               \u001b[5;1H                                                                                \u001b[6;1H                                                                                \u001b[7;1H                                                                                \u001b[8;1H                                                                                \u001b[9;1H                                                                                \u001b[10;1H                                                                                \u001b[11;1H                                                                                \u001b[12;1H                                                                                \u001b[13;1H                                                                                \u001b[14;1H                                                                                \u001b[15;1H                                                                                \u001b[16;1H                                                                                \u001b[17;1H                                                                                \u001b[18;1H                                                                                \u001b[19;1H                                                                                \u001b[20;1H                                                                                \u001b[21;1H                                                                                \u001b[22;1H                                                                                \u001b[23;1H                                                                                \u001b[24;1H                                                                              \u001b[4h\u001b[37m\u001b[40m \u001b[4l\u001b[H\u001b[m\u001b[m\u001b[37m\u001b[40m\u001b[m\u001b[m\u001b[21B\u001b[33m\u001b[44m\u001b[1mGetting https://accounts.google.com/o/oauth2/auth?response_type=code&client_id= \u001b[22;80H\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "list_gdrive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUSTOMIZE DATA\n",
    "In the next cells define the flood dates, add the threshold value and grid vector file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Flooding Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE PERIODS\n",
    "#timeframe\n",
    "timerange = ('2024-02', '2024-09')\n",
    "\n",
    "pre_flood = ['2024-02', '2024-03', '2024-04'] # 3 MONTHS PRIOR\n",
    "flood = ['2024-05', '2024-06', '2024-07', '2024-08', '2024-09'] # MONTHS DURING\n",
    "# post_flood = ['2024-09-10', '2024-10-15']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Threshold from `1.aoi-threshold.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_aoi = -27.395682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload file and calculate centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FILE FROM SANDBOX\n",
    "# grid = gpd.read_file(\"input/Lake Chad.geojson\")\n",
    "grid = gpd.read_file(\"input/TCD_55KM_4CTEST.geojson\")\n",
    "# grid = gpd.read_file(\"input/TCD_55KM_BASE.geojson\")\n",
    "\n",
    "# Calculate centroids and store in centroid list c[]. The array c[] will be used to loop all grid cells\n",
    "c = [] # INIT EMPTY GRID CENTROID\n",
    "g = grid.centroid # STORE ALL GRID CENTROIDS in g\n",
    "for i in g:\n",
    "    c.append([round(i.x, 5), round(i.y, 5)]) # EXTRACT x and y FROM POINT(x,y) AND APPEND TO c[]\n",
    "\n",
    "# AOI MOSAIC COVERING LAKE CHAD\n",
    "aoi_m = []\n",
    "for i in c:\n",
    "    # aoi_m.append(define_area(i[1], i[0], buffer=0.05))\n",
    "    aoi_m.append(define_area(i[1], i[0], buffer=0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(aoi_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RWD FUNCTION DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speckle Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to apply lee filtering on S1 image \n",
    "def lee_filter(da, size):\n",
    "    \"\"\"\n",
    "    Apply lee filter of specified window size.\n",
    "    Adapted from https://stackoverflow.com/questions/39785970/speckle-lee-filter-in-python\n",
    "\n",
    "    \"\"\"\n",
    "    img = da.values\n",
    "    img_mean = uniform_filter(img, size)\n",
    "    img_sqr_mean = uniform_filter(img**2, size)\n",
    "    img_variance = img_sqr_mean - img_mean**2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "    \n",
    "    return img_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S1_water_classifier(da, threshold):\n",
    "    water_data_array = da < threshold\n",
    "    return water_data_array.to_dataset(name=\"s1_water\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main For Loop Interation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32mGRID PROCESSED AND UPLOADED SUCCESSFULLY\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "grid_val = 1\n",
    "e_log =[]\n",
    "\n",
    "for aoi in aoi_m:\n",
    "    geopolygon = Geometry(aoi[\"features\"][0][\"geometry\"], crs=\"epsg:4326\")\n",
    "    geopolygon_gdf = gpd.GeoDataFrame(geometry=[geopolygon], crs=geopolygon.crs)\n",
    "    g = geopolygon_gdf.centroid\n",
    "    print(\"\\n\\n\"+ '\\033[32m' + \"PROCESSING GRID CELL {}/{} CENTROID ({}, {})\".format(grid_val, len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)) + '\\033[0m')\n",
    "\n",
    "    # Get the latitude and longitude range of the geopolygon\n",
    "    lat_range = (geopolygon_gdf.total_bounds[1], geopolygon_gdf.total_bounds[3])\n",
    "    lon_range = (geopolygon_gdf.total_bounds[0], geopolygon_gdf.total_bounds[2])\n",
    "\n",
    "    # LOAD SENTINEL-1 DATA\n",
    "    try:\n",
    "        S1 = load_ard(dc=dc,\n",
    "                    products=[\"s1_rtc\"],\n",
    "                    measurements=['vv', 'vh'],\n",
    "                    y=lat_range,\n",
    "                    x=lon_range,\n",
    "                    time=timerange,\n",
    "                    output_crs = \"EPSG:6933\",\n",
    "                    resolution = (-20,20),\n",
    "                    group_by=\"solar_day\",\n",
    "                    dtype='native'\n",
    "                    )\n",
    "    except:\n",
    "        # LOG ERROR aoi CENTROIDS AND CONTINUE LOOPING\n",
    "        e_log.append([g.y[0], g.x[0], \"P\"])\n",
    "        print(\"\\n\\n\"+ '\\033[31m' + \"ERROR PROCESSING GRID CELL {}/{} CENTROID ({}, {}). LOGGED CENTROID INFO in e_log\".format(grid_val, len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)) + '\\033[0m')\n",
    "        continue\n",
    "    \n",
    "    # INIT TIMESTEPS \n",
    "    timesteps = [2,4,6,9,11] \n",
    "\n",
    "    # The lee filter above doesn't handle null values\n",
    "    # We therefore set null values to 0 before applying the filter\n",
    "    valid = np.isfinite(S1)\n",
    "    S1 = S1.where(valid, 0)\n",
    "\n",
    "    # Create a new entry in dataset corresponding to filtered VV and VH data\n",
    "    # S1[\"filtered_vv\"] = S1.vv.groupby(\"time\").apply(lee_filter, size=7)\n",
    "    S1[\"filtered_vh\"] = S1.vh.groupby(\"time\").apply(lee_filter, size=7)\n",
    "\n",
    "    # Null pixels should remain null\n",
    "    # S1['filtered_vv'] = S1.filtered_vv.where(valid.vv)\n",
    "    S1['filtered_vh'] = S1.filtered_vh.where(valid.vh)   \n",
    "\n",
    "    # Convert the digital numbers to dB\n",
    "    # S1['filtered_vv'] = 10 * np.log10(S1.filtered_vv)\n",
    "    S1['filtered_vh'] = 10 * np.log10(S1.filtered_vh)\n",
    "\n",
    "    threshold_vh = th_aoi\n",
    "\n",
    "    S1['water'] = S1_water_classifier(S1.filtered_vh, threshold_vh).s1_water\n",
    "    FS1 = S1.water\n",
    "    PRFS1 = S1.water\n",
    "\n",
    "    \n",
    "    # CREATE OUTPUTS\n",
    "    # EXPORT TO RASTERS - UPLOAD TO G-DRIVE - DELETE FROM SANDBOX\n",
    "    #PREFLOOD\n",
    "    S1_PreFlood = PRFS1.sel(time = pre_flood, method = 'nearest').mean(dim = 'time')\n",
    "    preflood_val = 'CELL_' + str(grid_val) + '_PRE_FLOOD_MEAN'\n",
    "    preflood_name = preflood_val +'.tif'\n",
    "    preflood_out = 'output/preflood/' + preflood_name\n",
    "    S1_PreFlood.rio.to_raster(preflood_out)\n",
    "    #PREFLOOD META\n",
    "    prf_meta_text = '### Meta Data - GRID CELL ID = ' + str(grid_val) + ' ###' + '\\n' 'Time Range: ' + pre_flood[0] + ' - ' + pre_flood[-1] + '\\n' + 'Lat Range: ' +  str(lat_range) + ' Lon Range: ' + str(lon_range) + '\\n' + 'Coordinate Reference System: ' + str(geopolygon.crs)\n",
    "    text_flie_name = preflood_val + '_META.txt'\n",
    "    prf_meta_path = 'output/preflood/' + text_flie_name\n",
    "    with open(prf_meta_path, mode='w') as f:\n",
    "         f.write(prf_meta_text)\n",
    "    try:\n",
    "        # upload_to_gdrive([preflood_out, prf_meta_path], os.getenv(\"PREFLOOD_MEAN_ID\"))\n",
    "        upload_to_gdrive([preflood_out, prf_meta_path], PREFLOOD_MEAN_TEST_ID)\n",
    "    except:\n",
    "        e_log.append([g.y[0], g.x[0], \"U\"])\n",
    "        print(\"\\n\\n\"+ '\\033[31m' + \"ERROR UPLOADING GRID CELL {}/{} CENTROID ({}, {}). LOGGED CENTROID INFO in e_log\".format(grid_val, len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)) + '\\033[0m')\n",
    "        continue\n",
    "\n",
    "    \n",
    "    #FLOOD\n",
    "    S1_Flood = FS1.sel(time = flood, method = 'nearest').mean(dim = 'time')\n",
    "    flood_val = 'CELL_' + str(grid_val) + '_FLOOD_MEAN'\n",
    "    flood_name = flood_val + '.tif'\n",
    "    flood_out = 'output/flood/' + flood_name\n",
    "    S1_Flood.rio.to_raster(flood_out)\n",
    "    #FLOOD META\n",
    "    f_meta_text = '### Meta Data - GRID CELL ID = ' + str(grid_val) + ' ###' + '\\n' 'Time Range: ' + flood[0] + ' - ' + flood[-1] + '\\n' + 'Lat Range: ' +  str(lat_range) + ' Lon Range: ' + str(lon_range) + '\\n' + 'Coordinate Reference System: ' + str(geopolygon.crs)\n",
    "    text_flie_name = flood_val + '_META.txt'\n",
    "    f_meta_path = 'output/flood/' + text_flie_name\n",
    "    with open(f_meta_path, mode='w') as f:\n",
    "         f.write(f_meta_text)\n",
    "    try:\n",
    "        # upload_to_gdrive([flood_out, f_meta_path], os.getenv(\"FLOOD_MEAN_ID\"))\n",
    "        upload_to_gdrive([flood_out, f_meta_path], FLOOD_MEAN_TEST_ID)\n",
    "    except:\n",
    "        e_log.append([g.y[0], g.x[0], \"U\"])\n",
    "        print(\"\\n\\n\"+ '\\033[31m' + \"ERROR UPLOADING GRID CELL {}/{} CENTROID ({}, {}). LOGGED CENTROID INFO in e_log\".format(grid_val, len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)) + '\\033[0m')\n",
    "        continue\n",
    "\n",
    "    grid_val += 1\n",
    "    clear_output()\n",
    "\n",
    "if len(e_log) == 0:\n",
    "    print(\"\\n\\n\"+ '\\033[32m' + \"GRID PROCESSED AND UPLOADED SUCCESSFULLY\" + '\\033[0m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] 0\n"
     ]
    }
   ],
   "source": [
    "# PRINT ERROR DATA\n",
    "print(e_log, len(e_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Outputs To Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['flood', 'preflood', 'postflood']\n",
    "\n",
    "for dir in dirs:\n",
    "    loc = \"output/\" + dir\n",
    "    out = \"output/{}/Merged_{}.tif\".format(dir, dir)\n",
    "    extension = \"*.tif\"\n",
    "    q = os.path.join(loc, extension)\n",
    "    files = glob.glob(q)\n",
    "\n",
    "    r =[]\n",
    "    for f in files:\n",
    "        s = rasterio.open(f)\n",
    "        r.append(s)\n",
    "    if len(r)>0:\n",
    "        mosaic, out_trans = merge(r)\n",
    "        out_meta = s.meta.copy()\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                    \"height\": mosaic.shape[1],\n",
    "                    \"width\": mosaic.shape[2],\n",
    "                    \"transform\": out_trans\n",
    "                    })\n",
    "        with rasterio.open(out, \"w\", **out_meta) as dest:\n",
    "            dest.write(mosaic)\n",
    "            # upload_to_gdrive(out, \"flood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'storageQuota': {'limit': '16106127360',\n",
       "  'usage': '24051582',\n",
       "  'usageInDrive': '24051582',\n",
       "  'usageInDriveTrash': '0'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creds = create_token()\n",
    "service = build(\"drive\", \"v3\", credentials=creds)\n",
    "results = service.files().list(fields = \"files(id)\", pageSize = 1000).execute()\n",
    "items = results.get('files')\n",
    "print(len(items))\n",
    "storage = service.about().get(fields = \"storageQuota\").execute()\n",
    "storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "File Deleted\n",
      "An error occurred: <HttpError 403 when requesting https://www.googleapis.com/drive/v3/files/1GAQ4RSKt1s0Da6Uwm2ewv6-u7lYRwNil? returned \"The user does not have sufficient permissions for this file.\". Details: \"[{'message': 'The user does not have sufficient permissions for this file.', 'domain': 'global', 'reason': 'insufficientFilePermissions'}]\">\n",
      "An error occurred: <HttpError 403 when requesting https://www.googleapis.com/drive/v3/files/1rfvJIJ2NlM3m8mDKq8FXMzrMTgOh6xSC? returned \"The user does not have sufficient permissions for this file.\". Details: \"[{'message': 'The user does not have sufficient permissions for this file.', 'domain': 'global', 'reason': 'insufficientFilePermissions'}]\">\n",
      "An error occurred: <HttpError 403 when requesting https://www.googleapis.com/drive/v3/files/1zS-Ecrbf01sBaM02yEVNJEWcYQRULtg2? returned \"The user does not have sufficient permissions for this file.\". Details: \"[{'message': 'The user does not have sufficient permissions for this file.', 'domain': 'global', 'reason': 'insufficientFilePermissions'}]\">\n",
      "An error occurred: <HttpError 403 when requesting https://www.googleapis.com/drive/v3/files/1K0KqGlLxdUCsXg4771k_uf_x2UwbHlqa? returned \"The user does not have sufficient permissions for this file.\". Details: \"[{'message': 'The user does not have sufficient permissions for this file.', 'domain': 'global', 'reason': 'insufficientFilePermissions'}]\">\n",
      "An error occurred: <HttpError 403 when requesting https://www.googleapis.com/drive/v3/files/196YsHy1SXjNDnja6LjVhhlvoIt-Jg91l? returned \"The user does not have sufficient permissions for this file.\". Details: \"[{'message': 'The user does not have sufficient permissions for this file.', 'domain': 'global', 'reason': 'insufficientFilePermissions'}]\">\n",
      "An error occurred: <HttpError 403 when requesting https://www.googleapis.com/drive/v3/files/1Ovu5Q58xZRGpKsvowVr49klOrgVoZ0XQ? returned \"The user does not have sufficient permissions for this file.\". Details: \"[{'message': 'The user does not have sufficient permissions for this file.', 'domain': 'global', 'reason': 'insufficientFilePermissions'}]\">\n",
      "An error occurred: <HttpError 403 when requesting https://www.googleapis.com/drive/v3/files/1-GUFFarvB1jpuTBIJ-qJAPkVroVDLXk8? returned \"The user does not have sufficient permissions for this file.\". Details: \"[{'message': 'The user does not have sufficient permissions for this file.', 'domain': 'global', 'reason': 'insufficientFilePermissions'}]\">\n",
      "An error occurred: <HttpError 403 when requesting https://www.googleapis.com/drive/v3/files/10rwk3OW_fBL8ws7K7kt4P5EFjzpuyWp4? returned \"The user does not have sufficient permissions for this file.\". Details: \"[{'message': 'The user does not have sufficient permissions for this file.', 'domain': 'global', 'reason': 'insufficientFilePermissions'}]\">\n",
      "An error occurred: <HttpError 403 when requesting https://www.googleapis.com/drive/v3/files/1Qhum99pKi1Qyon8hcp4K8S_DGaJh5a5e? returned \"The user does not have sufficient permissions for this file.\". Details: \"[{'message': 'The user does not have sufficient permissions for this file.', 'domain': 'global', 'reason': 'insufficientFilePermissions'}]\">\n",
      "An error occurred: <HttpError 403 when requesting https://www.googleapis.com/drive/v3/files/1NDNCfsMa4Pyv9dA3lUeXAUKt6se1F-bU? returned \"The user does not have sufficient permissions for this file.\". Details: \"[{'message': 'The user does not have sufficient permissions for this file.', 'domain': 'global', 'reason': 'insufficientFilePermissions'}]\">\n",
      "An error occurred: <HttpError 403 when requesting https://www.googleapis.com/drive/v3/files/1DQdjTy5aICTQUVLM4rHcNoS0kxZ6L6GD? returned \"The user does not have sufficient permissions for this file.\". Details: \"[{'message': 'The user does not have sufficient permissions for this file.', 'domain': 'global', 'reason': 'insufficientFilePermissions'}]\">\n",
      "An error occurred: <HttpError 403 when requesting https://www.googleapis.com/drive/v3/files/1YzCONDhWPK3IAcx7a7yjCCkElIUpWKer? returned \"The user does not have sufficient permissions for this file.\". Details: \"[{'message': 'The user does not have sufficient permissions for this file.', 'domain': 'global', 'reason': 'insufficientFilePermissions'}]\">\n",
      "An error occurred: <HttpError 403 when requesting https://www.googleapis.com/drive/v3/files/1UHgLmijEZ_OS2NipJAcFB9--JYQX9aaX? returned \"The user does not have sufficient permissions for this file.\". Details: \"[{'message': 'The user does not have sufficient permissions for this file.', 'domain': 'global', 'reason': 'insufficientFilePermissions'}]\">\n"
     ]
    }
   ],
   "source": [
    "for item in items:\n",
    "    try:\n",
    "        response = service.files().delete(fileId=item['id']).execute()\n",
    "        print('File Deleted')\n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sahel_floods",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

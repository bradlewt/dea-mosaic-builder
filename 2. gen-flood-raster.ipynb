{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE REQUIREMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings \n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as rio\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy.ndimage.filters import uniform_filter\n",
    "from scipy.ndimage.measurements import variance\n",
    "from skimage.filters import threshold_minimum\n",
    "from datacube.utils.geometry import Geometry\n",
    "\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.plotting import display_map, rgb\n",
    "from deafrica_tools.areaofinterest import define_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-Drive Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import google.auth\n",
    "\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# If modifying these scopes, delete the file credentials.\n",
    "SCOPES = [\"https://www.googleapis.com/auth/drive\"]\n",
    "credential_path = '../Supplementary_data/DriveCredentials/credentials.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "\n",
    "Connect to the datacube so we can access DEA data.\n",
    "The `app` parameter is a unique name for the analysis which is based on the notebook file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"Radar_water_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G-Drive Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token():\n",
    "    ''''\n",
    "        credential: provide the json creditials you would get from google service.\n",
    "    '''\n",
    "    creds = None\n",
    "    creds = service_account.Credentials.from_service_account_file(credential_path, scopes=SCOPES)\n",
    "    return creds\n",
    "\n",
    "def list_gdrive():\n",
    "    '''\n",
    "    List the 10 recent files from the google drive\n",
    "    '''\n",
    "    creds = create_token()\n",
    "    try:\n",
    "        service = build(\"drive\", \"v3\", credentials=creds)\n",
    "      \n",
    "        results = (service.files().list(pageSize=20, fields=\"nextPageToken, files(id, name)\").execute())\n",
    "        items = results.get(\"files\", [])\n",
    "\n",
    "        if not items:\n",
    "            print(\"No files found.\")\n",
    "            return\n",
    "        print(\"Files:\")\n",
    "        for item in items:\n",
    "            print(f\"{item['name']} ({item['id']})\")\n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "\n",
    "\n",
    "def upload_to_gdrive(file_path=None, folder_id=None):\n",
    "    '''\n",
    "        Uploading files to google drive\n",
    "    '''\n",
    "    creds = create_token()\n",
    "    try:\n",
    "        # create drive api client\n",
    "        service = build(\"drive\", \"v3\", credentials=creds)\n",
    "        folder_path = '../Supplementary_data/DriveCredentials/{}'.format(folder_id)\n",
    "        #read the first line of the file\n",
    "        # folder_id = open(folder_path, \"r\").readline()\n",
    "\n",
    "        file_metadata = {\"name\": file_path, \"parents\": [folder_id]}\n",
    "        media = MediaFileUpload(file_path, resumable=True)\n",
    "        # pylint: disable=maybe-no-member\n",
    "        file = (service.files().create(body=file_metadata, media_body=media).execute())\n",
    "        print('File Uploaded successful')\n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        file = None\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUSTOMIZE DATA\n",
    "In the next three cells define the flood dates, threshold value and grid vector file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Flooding Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE PERIODS\n",
    "pre_flood = ['2024-04', '2024-05', '2024-06'] # 3 MONTHS PRIOR\n",
    "flood = ['2024-07', '2024-08', '2024-09', '2024-10-15'] # 4 MONTHS DURING\n",
    "# post_flood = ['2024-09-10', '2024-10-15']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define G-Drive Output Folder IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Threshold from 1. `aoi-threshold.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_aoi = -27.395682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload file and calculate centroids\n",
    "\n",
    "Upload grid file with grid of ~10km x 10km or 0.1 degree. Here grid file of Lake Chad is used with 0.1 degree grid size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FILE FROM SANDBOX\n",
    "grid = gpd.read_file(\"input/Lake Chad.geojson\")\n",
    "# grid = gpd.read_file(\"input/TCD_55KM_TILE.geojson\")\n",
    "\n",
    "# Calculate centroids and store in centroid list c[]. The array c[] will be used to loop all grid cells\n",
    "c = [] # INIT EMPTY GRID CENTROID\n",
    "g = grid.centroid # STORE ALL GRID CENTROIDS in g\n",
    "for i in g:\n",
    "    c.append([round(i.x, 5), round(i.y, 5)]) # EXTRACT x and y FROM POINT(x,y) AND APPEND TO c[]\n",
    "\n",
    "# AOI MOSAIC COVERING LAKE CHAD\n",
    "aoi_m = []\n",
    "for i in c:\n",
    "    aoi_m.append(define_area(i[1], i[0], buffer=0.05))\n",
    "    # aoi_m.append(define_area(i[1], i[0], buffer=0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speckle Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to apply lee filtering on S1 image \n",
    "def lee_filter(da, size):\n",
    "    \"\"\"\n",
    "    Apply lee filter of specified window size.\n",
    "    Adapted from https://stackoverflow.com/questions/39785970/speckle-lee-filter-in-python\n",
    "\n",
    "    \"\"\"\n",
    "    img = da.values\n",
    "    img_mean = uniform_filter(img, size)\n",
    "    img_sqr_mean = uniform_filter(img**2, size)\n",
    "    img_variance = img_sqr_mean - img_mean**2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "    \n",
    "    return img_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S1_water_classifier(da, threshold):\n",
    "    water_data_array = da < threshold\n",
    "    return water_data_array.to_dataset(name=\"s1_water\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main For Loop Interation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32mPROCESSING GRID CELL 1/5\u001b[0m\n",
      "Using pixel quality parameters for Sentinel 1\n",
      "Finding datasets\n",
      "    s1_rtc\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 42 time steps\n",
      "-38.336662\n",
      "File Uploaded successful\n",
      "File Uploaded successful\n",
      "\n",
      "\n",
      "\u001b[32mPROCESSING GRID CELL 2/5\u001b[0m\n",
      "Using pixel quality parameters for Sentinel 1\n",
      "Finding datasets\n",
      "    s1_rtc\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 42 time steps\n",
      "-25.997452\n",
      "File Uploaded successful\n",
      "File Uploaded successful\n",
      "\n",
      "\n",
      "\u001b[32mPROCESSING GRID CELL 3/5\u001b[0m\n",
      "Using pixel quality parameters for Sentinel 1\n",
      "Finding datasets\n",
      "    s1_rtc\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 42 time steps\n",
      "-25.845648\n",
      "File Uploaded successful\n",
      "File Uploaded successful\n",
      "\n",
      "\n",
      "\u001b[32mPROCESSING GRID CELL 4/5\u001b[0m\n",
      "Using pixel quality parameters for Sentinel 1\n",
      "Finding datasets\n",
      "    s1_rtc\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 42 time steps\n",
      "-25.63038\n",
      "File Uploaded successful\n",
      "File Uploaded successful\n",
      "\n",
      "\n",
      "\u001b[32mPROCESSING GRID CELL 5/5\u001b[0m\n",
      "Using pixel quality parameters for Sentinel 1\n",
      "Finding datasets\n",
      "    s1_rtc\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 42 time steps\n",
      "-21.168266\n",
      "File Uploaded successful\n",
      "File Uploaded successful\n"
     ]
    }
   ],
   "source": [
    "grid_val = 1\n",
    "\n",
    "for aoi in aoi_m:\n",
    "    print(\"\\n\\n\"+ '\\033[32m' + \"PROCESSING GRID CELL {}/{}\".format(grid_val, len(aoi_m)) + '\\033[0m')\n",
    "    geopolygon = Geometry(aoi[\"features\"][0][\"geometry\"], crs=\"epsg:4326\")\n",
    "    geopolygon_gdf = gpd.GeoDataFrame(geometry=[geopolygon], crs=geopolygon.crs)\n",
    "\n",
    "    # Get the latitude and longitude range of the geopolygon\n",
    "    lat_range = (geopolygon_gdf.total_bounds[1], geopolygon_gdf.total_bounds[3])\n",
    "    lon_range = (geopolygon_gdf.total_bounds[0], geopolygon_gdf.total_bounds[2])\n",
    "\n",
    "    #timeframe\n",
    "    timerange = ('2024-01-01', '2024-10-01')\n",
    "\n",
    "    # LOAD SENTINEL-1 DATA\n",
    "    S1 = load_ard(dc=dc,\n",
    "                products=[\"s1_rtc\"],\n",
    "                measurements=['vv', 'vh'],\n",
    "                y=lat_range,\n",
    "                x=lon_range,\n",
    "                time=timerange,\n",
    "                output_crs = \"EPSG:6933\",\n",
    "                resolution = (-20,20),\n",
    "                group_by=\"solar_day\",\n",
    "                dtype='native'\n",
    "                )\n",
    "    \n",
    "    # INIT TIMESTEPS \n",
    "    timesteps = [2,4,6,9,11]\n",
    "\n",
    "    # VH/VV is a potentially useful third feature after VV and VH \n",
    "    S1['vh/vv'] = S1.vh/S1.vv  \n",
    "\n",
    "    # MEDIAN VALUES FOR SIMILAR RANGE FOR VISUALIZATION\n",
    "    med_s1 = S1[['vv','vh','vh/vv']].median()  \n",
    "\n",
    "    # The lee filter above doesn't handle null values\n",
    "    # We therefore set null values to 0 before applying the filter\n",
    "    valid = np.isfinite(S1)\n",
    "    S1 = S1.where(valid, 0)\n",
    "\n",
    "    # Create a new entry in dataset corresponding to filtered VV and VH data\n",
    "    S1[\"filtered_vv\"] = S1.vv.groupby(\"time\").apply(lee_filter, size=7)\n",
    "    S1[\"filtered_vh\"] = S1.vh.groupby(\"time\").apply(lee_filter, size=7)\n",
    "\n",
    "    # Null pixels should remain null\n",
    "    S1['filtered_vv'] = S1.filtered_vv.where(valid.vv)\n",
    "    S1['filtered_vh'] = S1.filtered_vh.where(valid.vh)   \n",
    "\n",
    "    # Convert the digital numbers to dB\n",
    "    S1['filtered_vv'] = 10 * np.log10(S1.filtered_vv)\n",
    "    S1['filtered_vh'] = 10 * np.log10(S1.filtered_vh)\n",
    "\n",
    "    try:\n",
    "        threshold_vh = threshold_minimum(S1.filtered_vh.values)\n",
    "    except:\n",
    "        threshold_vh = th_aoi\n",
    "    print(threshold_vh)\n",
    "\n",
    "    S1['water'] = S1_water_classifier(S1.filtered_vh, threshold_vh).s1_water\n",
    "\n",
    "    # CREATE FLOODING OUTPUTS\n",
    "    S1['water_APR_JUL_24'] = S1.water.sel(time = pre_flood, method = 'nearest').mean(dim = 'time')\n",
    "    S1['water_JUL_OCT_24'] = S1.water.sel(time = flood, method = 'nearest').mean(dim = 'time')\n",
    "    # S1['water_SEP_OCT_24'] = S1.water.sel(time = post_flood, method = 'nearest').mean(dim = 'time')\n",
    "\n",
    "    # EXPORT TO RASTERS\n",
    "    preflood_name = 'CELL_' + str(grid_val) + '_PRE_FLOOD.tif'\n",
    "    src_out = 'output/preflood/' + preflood_name\n",
    "    S1.water_APR_JUL_24.rio.to_raster(src_out)\n",
    "    upload_to_gdrive(src_out, preflood_id)\n",
    "\n",
    "    flood_name = 'CELL_' + str(grid_val) + '_FLOOD.tif'\n",
    "    src_out = 'output/flood/' + flood_name\n",
    "    S1.water_JUL_OCT_24.rio.to_raster(src_out)\n",
    "    upload_to_gdrive(src_out, flood_id)\n",
    "\n",
    "    # postflood_name = 'CELL_' + str(grid_val) + '_POST_FLOOD_Mean.tif'\n",
    "    # src_out = 'output/postflood/' + postflood_name\n",
    "    # S1.water_SEP_OCT_24.rio.to_raster(src_out)\n",
    "    # upload_to_gdrive(postflood_name, postflood_id)\n",
    "\n",
    "    grid_val += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Outputs To Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files:\n",
      "postflood (1YzCONDhWPK3IAcx7a7yjCCkElIUpWKer)\n",
      "preflood (1rfvJIJ2NlM3m8mDKq8FXMzrMTgOh6xSC)\n",
      "flood (1zS-Ecrbf01sBaM02yEVNJEWcYQRULtg2)\n",
      "input (1UHgLmijEZ_OS2NipJAcFB9--JYQX9aaX)\n",
      "output (1DQdjTy5aICTQUVLM4rHcNoS0kxZ6L6GD)\n",
      "Animated_timeseries.ipynb (1JoU_wHD__wmkm99_sd6NG3YLvdkY3_-W)\n",
      "dea (1GAQ4RSKt1s0Da6Uwm2ewv6-u7lYRwNil)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m g_dirs \u001b[38;5;241m=\u001b[39m list_gdrive()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m g_dirs:\n\u001b[1;32m      3\u001b[0m     r\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "g_dirs = list_gdrive()\n",
    "for r in g_dirs:\n",
    "    r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['flood', 'preflood', 'postflood']\n",
    "\n",
    "for dir in dirs:\n",
    "    loc = \"output/\" + dir\n",
    "    out = \"output/{}/Merged_{}.tif\".format(dir, dir)\n",
    "    extension = \"*.tif\"\n",
    "    q = os.path.join(loc, extension)\n",
    "    files = glob.glob(q)\n",
    "\n",
    "    r =[]\n",
    "    for f in files:\n",
    "        s = rasterio.open(f)\n",
    "        r.append(s)\n",
    "    if len(r)>0:\n",
    "        mosaic, out_trans = merge(r)\n",
    "        out_meta = s.meta.copy()\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                    \"height\": mosaic.shape[1],\n",
    "                    \"width\": mosaic.shape[2],\n",
    "                    \"transform\": out_trans\n",
    "                    })\n",
    "        with rasterio.open(out, \"w\", **out_meta) as dest:\n",
    "            dest.write(mosaic)\n",
    "            # upload_to_gdrive(out, \"flood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

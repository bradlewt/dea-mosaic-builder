{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE REQUIREMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# !pip install python-dotenv\n",
    "# load_dotenv()\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "import os, glob, warnings, datacube, rasterio, folium, json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as rio\n",
    "import pandas as pd\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "from scipy.ndimage import uniform_filter\n",
    "from scipy.ndimage import variance\n",
    "from skimage.filters import threshold_minimum\n",
    "from datacube.utils.geometry import Geometry\n",
    "\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.plotting import display_map, rgb\n",
    "from deafrica_tools.areaofinterest import define_area\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local modules\n",
    "from tools.gdrive import GDrive\n",
    "\n",
    "gd = GDrive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Drive Storage\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loc</th>\n",
       "      <th>Size in GB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capacity</td>\n",
       "      <td>2199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Available</td>\n",
       "      <td>1838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Usage</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Drive Usage</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trash Usage</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Loc  Size in GB\n",
       "0     Capacity        2199\n",
       "1    Available        1838\n",
       "2        Usage         361\n",
       "3  Drive Usage         361\n",
       "4  Trash Usage           0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd.get_storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "\n",
    "Connect to the datacube so we can access DEA data.\n",
    "The `app` parameter is a unique name for the analysis which is based on the notebook file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"Radar_water_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-Drive Folder IDs and Timerange Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder ids by copying the ID from the g-drive folder url\n",
    "# TEST/\n",
    "F_MN_FID = \"1KBig_UZLT0fgFXsACMHQTb7c_Poao-wh\"\n",
    "F_MD_FID = \"1dWde-mzh8Sc9BIEMNUByq7Z6Yags0JZh\"\n",
    "\n",
    "PRF_MN_FID = \"1MVAMwv0E3sZ6qK8E93MlnGcPTr6yXge4\"\n",
    "PRF_MD_FID = \"1oROFY5hKi3w7pbkrCnHtGLIJT_nBraog\"\n",
    "\n",
    "POF_MN_FID = None\n",
    "POF_MD_FID = None\n",
    "\n",
    "FE_MN_FID = \"1RpE2-Pe-KihKdCk0TKLvTzZgrv5QGOeu\"\n",
    "FE_MD_FID = \"1KDTiwmmdaV5LBh6Dj1qAB5KTRNQu_-Xq\"\n",
    "\n",
    "MF_MX_FID = \"100Xf41aDRLwZHiPxXzSMHbiCh4JWosaD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main time period of analysis\n",
    "timerange = (\"2023-11\", \"2024-11\")\n",
    "\n",
    "# Define sub-periods of analysis - should be within main time period\n",
    "pre_flood = [\"2024-02\", \"2024-03\", \"2024-04\"]\n",
    "flood = [\"2024-05\", \"2024-06\", \"2024-07\", \"2024-08\", \"2024-09\"]\n",
    "post_flood = []\n",
    "max_flood = [\"2023-11\", \"2024-11\"]\n",
    "\n",
    "# Run 1. aoi-threshold.ipynb to get the value of th_aoi and store it here.\n",
    "th_aoi = -27.395682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Classifier Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply lee filtering on S1 image. Speckle Filter\n",
    "def lee_filter(da, size):\n",
    "    \"\"\"\n",
    "    Apply lee filter of specified window size.\n",
    "    Adapted from https://stackoverflow.com/questions/39785970/speckle-lee-filter-in-python\n",
    "\n",
    "    \"\"\"\n",
    "    img = da.values\n",
    "    img_mean = uniform_filter(img, size)\n",
    "    img_sqr_mean = uniform_filter(img**2, size)\n",
    "    img_variance = img_sqr_mean - img_mean**2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "\n",
    "    return img_output\n",
    "\n",
    "\n",
    "# Classifier Function\n",
    "def S1_water_classifier(da, threshold):\n",
    "    water_data_array = da < threshold\n",
    "    return water_data_array.to_dataset(name=\"s1_water\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operational Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate raster outputs\n",
    "def gen_output(\n",
    "    DS: xr.DataArray,\n",
    "    i: list,\n",
    "    poly: Geometry,\n",
    "    cell: int,\n",
    "    aoi_m: list,\n",
    "    period: Literal[\"preflood\", \"flood\", \"postflood\", \"flood_extents\", \"maxflood\"],\n",
    "    measure: Literal[\"mean\", \"median\", \"max\"],\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Generates file names and uploads tp Google Drive\n",
    "\n",
    "    Parameters:\n",
    "    DS: xr.DataArray, required\n",
    "        Dataset to be converted into raster output.\n",
    "    i: list, required\n",
    "        Index of centroid list.\n",
    "    poly: Geometry, required\n",
    "        Polygon feature used to create the DS.\n",
    "    cell: int, required\n",
    "        Cell number being processed.\n",
    "    aoi_m: list, required\n",
    "        List of geojson feature collection (cells) that make up the entire grid.\n",
    "    period: Literal[\"preflood\", \"flood\", \"postflood\", \"flood_extents\"], required\n",
    "        Time period of the process - PRE_FLOOD, FLOOD.\n",
    "    measure: Literal[\"mean\", \"median\"], required\n",
    "        Central tendency measurement of the DS - mean, median.\n",
    "\n",
    "    Returns:\n",
    "    err: list\n",
    "        Error log list of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload.\n",
    "    \"\"\"\n",
    "\n",
    "    err = []\n",
    "\n",
    "    pm_dict = {\n",
    "        \"preflood_mean\": [pre_flood, PRF_MN_FID],\n",
    "        \"preflood_median\": [pre_flood, PRF_MD_FID],\n",
    "        \"flood_mean\": [flood, F_MN_FID],\n",
    "        \"flood_median\": [flood, F_MD_FID],\n",
    "        \"postflood_mean\": [post_flood, POF_MN_FID],\n",
    "        \"postflood_median\": [post_flood, POF_MD_FID],\n",
    "        \"flood_extents_mean\": [timerange, FE_MN_FID],\n",
    "        \"flood_extents_median\": [timerange, FE_MD_FID],\n",
    "        \"maxflood_max\": [timerange, MF_MX_FID],\n",
    "    }\n",
    "\n",
    "    poly_gdf = gpd.GeoDataFrame(geometry=[poly], crs=poly.crs)\n",
    "    lat_range = (poly_gdf.total_bounds[1], poly_gdf.total_bounds[3])\n",
    "    lon_range = (poly_gdf.total_bounds[0], poly_gdf.total_bounds[2])\n",
    "    g = poly_gdf.centroid\n",
    "\n",
    "    data_val = \"CELL_\" + str(i[2]) + \"_{}_{}\".format(period.upper(), measure.upper())\n",
    "    data_name = data_val + \".tif\"\n",
    "    data_out = \"output/{}/\".format(period) + data_name\n",
    "    DS.rio.to_raster(data_out)\n",
    "\n",
    "    # preflood meta\n",
    "    data_dict = {\n",
    "        \"GRID_CELL_ID\": i[2],\n",
    "        \"start_time\": pm_dict[\"{}_{}\".format(period, measure)][0][0],  # pre_flood[0]\n",
    "        \"end_time\": pm_dict[\"{}_{}\".format(period, measure)][0][-1],  # pre_flood[-1]\n",
    "        \"lat\": lat_range,\n",
    "        \"lon\": lon_range,\n",
    "        \"centroid\": \"{}, {}\".format(g.y[0], g.x[0]),\n",
    "        \"crs\": str(poly.crs),\n",
    "    }\n",
    "\n",
    "    text_flie_name = data_val + \"_META.json\"\n",
    "    data_meta_path = \"output/{}/\".format(period) + text_flie_name\n",
    "    with open(data_meta_path, \"w\") as f:\n",
    "        json.dump(data_dict, f)\n",
    "\n",
    "    try:\n",
    "        gd.upload_files(\n",
    "            [data_out, data_meta_path], pm_dict[\"{}_{}\".format(period, measure)][1]\n",
    "        )  # pm_dict['preflood_mean'][1] = FOLDER_ID\n",
    "    except Exception as e:\n",
    "        err.append([g.x[0], g.y[0], i[2], \"U-PRF\"])\n",
    "        print(\n",
    "            \"\\n\\n\"\n",
    "            + \"\\033[31m\"\n",
    "            + \"ERROR UPLOADING GRID CELL ID {} NO.  {}/{} CENTROID ({}, {}). LOGGED CENTROID INFO in e_log\".format(\n",
    "                i[2], cell, len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)\n",
    "            )\n",
    "            + \"\\033[0m\"\n",
    "        )\n",
    "        print(\"UPLOAD ERROR: {}\".format(e))\n",
    "\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_elog(e_log: list) -> list:\n",
    "    \"\"\"\n",
    "    Writes a the error log json file and uploads it to the google drive folder ID, if specified.\n",
    "\n",
    "    Parameters:\n",
    "    e_log: list, required\n",
    "        Error log list of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload.\n",
    "\n",
    "    Returns:\n",
    "    e_log: list\n",
    "        Error log list having the same values as the input parameter\n",
    "    \"\"\"\n",
    "    e_log = np.array(e_log)\n",
    "    with open(\"error_centroids.json\", \"w\") as filehandle:\n",
    "        json.dump(e_log.tolist(), filehandle)\n",
    "\n",
    "    # read error log from disk\n",
    "    with open(\"error_centroids.json\") as f:\n",
    "        e_log = json.load(f)\n",
    "    for e in e_log:\n",
    "        e[0] = float(e[0])\n",
    "        e[1] = float(e[1])\n",
    "        e[2] = int(e[2])\n",
    "\n",
    "    try:\n",
    "        gd.upload_files([\"error_centroids.json\"], ERR_FOLDER_ID, False)\n",
    "    except Exception as e:\n",
    "        print(\"FAILED TO UPLOAD ERROR LOG FILE REASON:{}\".format(e))\n",
    "\n",
    "    return e_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the input grid\n",
    "def iterate_grid(aoi_m: list, c: list) -> list:\n",
    "    \"\"\"\n",
    "    Iterates through every feature (cell) in the AOI grid.\n",
    "\n",
    "    Parameters:\n",
    "    aoi_m: list, required\n",
    "        List of geojson feature collection (cells) that make up the entire grid.\n",
    "    c: list, required\n",
    "        List of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload, if executio fails\n",
    "\n",
    "    Returns:\n",
    "    e_log: list\n",
    "        Error log list of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload.\n",
    "    \"\"\"\n",
    "\n",
    "    e_log = []\n",
    "    cell = 1\n",
    "    for aoi, i in zip(aoi_m, c):\n",
    "        geopolygon = Geometry(aoi[\"features\"][0][\"geometry\"], crs=\"epsg:4326\")\n",
    "        geopolygon_gdf = gpd.GeoDataFrame(geometry=[geopolygon], crs=geopolygon.crs)\n",
    "        g = geopolygon_gdf.centroid\n",
    "        print(\n",
    "            \"\\n\\n\"\n",
    "            + \"\\033[32m\"\n",
    "            + \"PROCESSING GRID CELL ID {} NO. {}/{} CENTROID ({}, {})\".format(\n",
    "                i[2], cell, len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)\n",
    "            )\n",
    "            + \"\\033[0m\"\n",
    "        )\n",
    "\n",
    "        # Get the latitude and longitude range of the geopolygon\n",
    "        lat_range = (geopolygon_gdf.total_bounds[1], geopolygon_gdf.total_bounds[3])\n",
    "        lon_range = (geopolygon_gdf.total_bounds[0], geopolygon_gdf.total_bounds[2])\n",
    "\n",
    "        # Load Sentinel1 data\n",
    "        try:\n",
    "            S1 = load_ard(\n",
    "                dc=dc,\n",
    "                products=[\"s1_rtc\"],\n",
    "                # measurements=[\"vv\", \"vh\"],\n",
    "                measurements=[\"vh\"],\n",
    "                y=lat_range,\n",
    "                x=lon_range,\n",
    "                time=timerange,\n",
    "                output_crs=\"EPSG:6933\",\n",
    "                resolution=(-20, 20),\n",
    "                group_by=\"solar_day\",\n",
    "                dtype=\"native\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Log error aoi centroids and keep looping\n",
    "            e_log.append([g.x[0], g.y[0], i[2], \"P\"])\n",
    "            print(\n",
    "                \"\\n\\n\"\n",
    "                + \"\\033[31m\"\n",
    "                + \"ERROR PROCESSING GRID CELL {}/{} CENTROID ({}, {}). LOGGED CENTROID INFO in e_log\".format(\n",
    "                    i[2], len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)\n",
    "                )\n",
    "                + \"\\033[0m\"\n",
    "            )\n",
    "            print(\"PROCESS ERROR: {}\".format(e))\n",
    "            cell += 1\n",
    "            continue\n",
    "\n",
    "        # timesteps = [2, 4, 6, 9, 11]\n",
    "\n",
    "        # The lee filter above doesn't handle null values\n",
    "        # We therefore set null values to 0 before applying the filter\n",
    "        valid = np.isfinite(S1)\n",
    "        S1 = S1.where(valid, 0)\n",
    "\n",
    "        # Create a new entry in dataset corresponding to filtered VV and VH data\n",
    "        S1[\"filtered_vh\"] = S1.vh.groupby(\"time\").apply(lee_filter, size=7)\n",
    "\n",
    "        # Null pixels should remain null\n",
    "        S1[\"filtered_vh\"] = S1.filtered_vh.where(valid.vh)\n",
    "\n",
    "        # Convert the digital numbers to dB\n",
    "        S1[\"filtered_vh\"] = 10 * np.log10(S1.filtered_vh)\n",
    "\n",
    "        threshold_vh = th_aoi\n",
    "\n",
    "        S1[\"water\"] = S1_water_classifier(S1.filtered_vh, threshold_vh).s1_water\n",
    "        S1Water = S1.water\n",
    "        S1_BIN = S1Water.where(S1Water > 0)\n",
    "        FS1 = S1_BIN\n",
    "        PRFS1 = S1_BIN\n",
    "\n",
    "        # Creating outputs\n",
    "        # Export to raster - upload to g-drive - delete from sandbox\n",
    "\n",
    "        print(\"Uploading...\")\n",
    "        # -------------- maxflood ----------------\n",
    "        if i[3] in [None, \"P\", \"U-PRF\"]:\n",
    "            S1_MX = S1_BIN.sel(time=max_flood, method=\"nearest\").max(dim=\"time\")\n",
    "            err = gen_output(S1_MX, i, geopolygon, cell, aoi_m, \"maxflood\", \"max\")\n",
    "\n",
    "        # # -------------- preflood ----------------\n",
    "        # if i[3] in [None, \"P\", \"U-PRF\"]:\n",
    "        #     S1_PRF_MD = PRFS1.sel(time=pre_flood, method=\"nearest\").median(dim=\"time\")\n",
    "        #     S1_PRF_MN = PRFS1.sel(time=pre_flood, method=\"nearest\").mean(dim=\"time\")\n",
    "        #     err = gen_output(\n",
    "        #         S1_PRF_MD, i, geopolygon, cell, aoi_m, \"preflood\", \"median\"\n",
    "        #     )\n",
    "        #     if len(err) > 0:\n",
    "        #         e_log.extend(err)\n",
    "        #     err = gen_output(S1_PRF_MN, i, geopolygon, cell, aoi_m, \"preflood\", \"mean\")\n",
    "        #     if len(err) > 0:\n",
    "        #         e_log.extend(err)\n",
    "\n",
    "        # # --------------- flood ------------------\n",
    "        # if i[3] in [None, \"P\", \"U-F\"]:\n",
    "        #     S1_F_MD = FS1.sel(time=flood, method=\"nearest\").median(dim=\"time\")\n",
    "        #     S1_F_MN = FS1.sel(time=flood, method=\"nearest\").mean(dim=\"time\")\n",
    "        #     err = gen_output(S1_F_MD, i, geopolygon, cell, aoi_m, \"flood\", \"median\")\n",
    "        #     if len(err) > 0:\n",
    "        #         e_log.extend(err)\n",
    "        #     err = gen_output(S1_F_MN, i, geopolygon, cell, aoi_m, \"flood\", \"mean\")\n",
    "        #     if len(err) > 0:\n",
    "        #         e_log.extend(err)\n",
    "\n",
    "        # # ------------ flood-extents --------------\n",
    "        # if i[3] in [None, \"P\", \"U-FE\"]:\n",
    "        #     S1_FE_MD = S1_F_MD - S1_PRF_MD\n",
    "        #     S1_FE_MN = S1_F_MN - S1_PRF_MN\n",
    "        #     err = gen_output(\n",
    "        #         S1_FE_MD, i, geopolygon, cell, aoi_m, \"flood_extents\", \"median\"\n",
    "        #     )\n",
    "        #     if len(err) > 0:\n",
    "        #         e_log.extend(err)\n",
    "        #     err = gen_output(\n",
    "        #         S1_FE_MN, i, geopolygon, cell, aoi_m, \"flood_extents\", \"mean\"\n",
    "        #     )\n",
    "        #     if len(err) > 0:\n",
    "        #         e_log.extend(err)\n",
    "\n",
    "        cell += 1\n",
    "        # clear_output()\n",
    "\n",
    "    if len(e_log) == 0:\n",
    "        print(\n",
    "            \"\\n\\n\"\n",
    "            + \"\\033[32m\"\n",
    "            + \"GRID PROCESSED AND UPLOADED SUCCESSFULLY\"\n",
    "            + \"\\033[0m\"\n",
    "            + \"\\n\\n\"\n",
    "        )\n",
    "\n",
    "    # e_log = gen_elog(e_log)\n",
    "\n",
    "    # return e_log to be run again\n",
    "    return e_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crete the aoi-mosaic - aoi_m\n",
    "def gen_aoim(c: list, b: float) -> list:\n",
    "    \"\"\"\n",
    "    Generates the feature collection list (list of cells) using centroid coordinates and a buffer distance. Calls the main iterator for execution as well.\n",
    "\n",
    "    Parameters:\n",
    "    c: list, required\n",
    "        List of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload, if executio fails.\n",
    "    b: float, required\n",
    "        Cell half-dimension in degrees (EPSG:4326). Creates a cell by adding this distance to the centroid coordinates.\n",
    "\n",
    "    Returns:\n",
    "    e_log: list\n",
    "        Error log list of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload.\n",
    "    \"\"\"\n",
    "    aoi_m = []\n",
    "    for i in c:\n",
    "        aoi_m.append(define_area(i[1], i[0], buffer=b))\n",
    "    # print(c, len(aoi_m))\n",
    "    e_log = iterate_grid(aoi_m, c)\n",
    "\n",
    "    # return e_log to be run again\n",
    "    return e_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize input file\n",
    "def view_input(gdf_list: list[gpd.GeoDataFrame], grid_c: list) -> None:\n",
    "    \"\"\"\n",
    "    Visualizes cells and respective IDs  on a basemap.\n",
    "\n",
    "    Parameters:\n",
    "    gdf_list:list[gpd.GeoDataFrame], required\n",
    "        List of geodataframes to be visualized.\n",
    "    grid_c:list, required\n",
    "        List of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload, if executio fails.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Visualizing data...\")\n",
    "    p = gdf_list[0].dissolve()\n",
    "    center = p.centroid\n",
    "    map = folium.Map(location=[center.y, center.x], tiles=\"CartoDB Positron\")\n",
    "\n",
    "    for gdf in gdf_list:\n",
    "        folium.GeoJson(gdf, name=\"{}\".format(gdf)).add_to(map)\n",
    "\n",
    "    for c in grid_c:\n",
    "        folium.Marker(\n",
    "            location=[c[1], c[0]],\n",
    "            popup=f\"Centroid: {c[1]}, {c[0]}\",\n",
    "            icon=folium.DivIcon(\n",
    "                icon_size=(10, 10),\n",
    "                icon_anchor=(0, 0),\n",
    "                html='<div style=\"font-size: 10pt\">{}</div>'.format(c[2]),\n",
    "            ),\n",
    "        ).add_to(map)\n",
    "\n",
    "    bounds = gdf_list[0].total_bounds.tolist()\n",
    "    map.fit_bounds([bounds[:2][::-1], bounds[2:][::-1]])\n",
    "    display(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid\n",
    "from shapely import intersection as intersect\n",
    "\n",
    "\n",
    "def create_grid(adm0: gpd.GeoDataFrame, size: float) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Divides adm0 AOI vectorfile into square grid based on size\n",
    "\n",
    "    Parameters:\n",
    "    adm0:gpd.GeoDataFrame, required\n",
    "        AMD0 GeoDataFrame created from ADM0 input vector file\n",
    "    size:float, required\n",
    "        Grid cell size in degrees (EPSG:4326)\n",
    "\n",
    "    Returns:\n",
    "    grid: gpd.GeoDataFrame\n",
    "        The generated grid GeoDataFrame\n",
    "    \"\"\"\n",
    "    bounds = adm0.bounds\n",
    "    minx = bounds.minx[0]  # only 1 feature at the 0th index\n",
    "    miny = bounds.miny[0]\n",
    "    maxx = bounds.maxx[0]\n",
    "    maxy = bounds.maxy[0]\n",
    "\n",
    "    grid = gpd.GeoDataFrame()\n",
    "    for x0 in np.arange(minx, maxx, size):\n",
    "        for y0 in np.arange(miny, maxy, size):\n",
    "            x1 = x0 + size\n",
    "            y1 = y0 + size\n",
    "            d = {\"geometry\": [Polygon([(x0, y0), (x1, y0), (x1, y1), (x0, y1)])]}\n",
    "            cell = gpd.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
    "            flag = adm0.intersection(cell)\n",
    "            if flag[0].is_empty == False:\n",
    "                grid = pd.concat([grid, cell])\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CRS and convert to 4326 if required\n",
    "def crs_check(shp: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Checks input GeoDataFrame CRS and converts to EPSG 4326, if different.\n",
    "\n",
    "    Parameters:\n",
    "    shp: gpd.GeoDataFrame, required\n",
    "        Input GeoDataFrame to check.\n",
    "\n",
    "    Returns:\n",
    "    shp: gpd.GeoDataFrame\n",
    "        As is or converted GeoDataFrame.\n",
    "    \"\"\"\n",
    "    if shp.crs != \"EPSG:4326\":\n",
    "        print(\"Added ADM0 CRS is {}. Converting to EPSG:4326...\".format(shp.crs))\n",
    "        shp = shp.to_crs(\"EPSG:4326\")\n",
    "        if shp.crs == \"EPSG:4326\":\n",
    "            print(\"Done\")\n",
    "\n",
    "    return shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_inp(x: str) -> None:\n",
    "    \"\"\"\n",
    "    Checks if input is \"y\" or \"n\"\n",
    "\n",
    "    Parameters:\n",
    "    x: str, required\n",
    "        String input to be checked\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if x not in [\"y\", \"n\"]:\n",
    "        raise ValueError(\"Invalid input, must be 'y' or 'n'\")\n",
    "    elif x == \"n\":\n",
    "        raise RuntimeError(\n",
    "            \"Excecution terminated. Make necessary changes before running again\"\n",
    "        )\n",
    "\n",
    "\n",
    "def exec_checks() -> None:\n",
    "    \"\"\"\n",
    "    Performs checks and Run the entire application\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    inst = \"\"\"\n",
    "Before running the execution, ensure all requirements have been met:\n",
    "1. Create appropriate folders in Google Drive and add their Folder IDs here\n",
    "2. Check input shapefile\n",
    "3. Check grid size\n",
    "    \n",
    "    \"\"\"\n",
    "    print(inst)\n",
    "\n",
    "    x = input(\"Folder IDs verified? (y/n):\")\n",
    "    check_inp(x)\n",
    "\n",
    "    x = input(\"Input shapefile/geojson verified? (y/n):\")\n",
    "    check_inp(x)\n",
    "\n",
    "    input(\"Grid (size) verified? (y/n):\")\n",
    "    check_inp(x)\n",
    "\n",
    "    z = input(\"\\nBegin execution for entire input shapefile/geojson? (y/n):\")\n",
    "    if z not in [\"y\", \"n\"]:\n",
    "        raise ValueError(\"Invalid input, must be 'y' or 'n'\")\n",
    "    elif z == \"n\":\n",
    "        raise RuntimeError(\n",
    "            \"Excecution terminated. Make necessary changes before running again\"\n",
    "        )\n",
    "    elif z == \"y\":\n",
    "        print(\"Starting execution...\")\n",
    "        # get e_log with centroids, cell_id and error message\n",
    "        # Calling gen_aoim will run the entire Application\n",
    "        e_log = gen_aoim(c, buffer)\n",
    "        print(len(e_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_files(path: str, ext: str) -> None:\n",
    "    \"\"\"\n",
    "    Deletes all files with specified extention at specified folder path\n",
    "\n",
    "    Parameters:\n",
    "    path:str, required\n",
    "        Folder path.\n",
    "    ext:str, required\n",
    "        File extension. \"*\" for all files.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    res_files = False\n",
    "    loc = os.path.join(path, ext)\n",
    "    files = glob.glob(loc)\n",
    "    if len(files) > 0:\n",
    "        res_files = True\n",
    "        print(\"Found {} files. Deleting...\".format(len(files)))\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "\n",
    "    return res_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dirs() -> None:\n",
    "    \"\"\"\n",
    "    Creates directories if they dont exist or deletes residual files if they exist.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    dirs_exist = False\n",
    "    dir_dict = {\n",
    "        \"sd_flood\": \"output/flood\",\n",
    "        \"sd_preflood\": \"output/preflood\",\n",
    "        \"sd_flood_extents\": \"output/flood_extents\",\n",
    "    }\n",
    "\n",
    "    for k in dir_dict:\n",
    "        if not os.path.exists(dir_dict[k]):\n",
    "            os.makedirs(dir_dict[k])\n",
    "        else:\n",
    "            dirs_exist = True\n",
    "            r = del_files(dir_dict[k], \"*\")\n",
    "\n",
    "    if dirs_exist:\n",
    "        print(\"Output folders alredy exist.\")\n",
    "    else:\n",
    "        print(\"Output folders created.\")\n",
    "\n",
    "    if not r:\n",
    "        print(\"No residual files to delete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Gridded Vector File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folders alredy exist.\n",
      "No residual files to delete.\n",
      "Visualizing data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap-glyphicons.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_4019341d1301fd38950201c433c2cb82 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_4019341d1301fd38950201c433c2cb82&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_4019341d1301fd38950201c433c2cb82 = L.map(\n",
       "                &quot;map_4019341d1301fd38950201c433c2cb82&quot;,\n",
       "                {\n",
       "                    center: [13.450370788999997, 14.523475456],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 10,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_4c3b6079d998efa1e0cefeacf8385dde = L.tileLayer(\n",
       "                &quot;https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}{r}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors \\u0026copy; \\u003ca href=\\&quot;https://carto.com/attributions\\&quot;\\u003eCARTO\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 20, &quot;maxZoom&quot;: 20, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abcd&quot;, &quot;tms&quot;: false}\n",
       "            );\n",
       "        \n",
       "    \n",
       "            tile_layer_4c3b6079d998efa1e0cefeacf8385dde.addTo(map_4019341d1301fd38950201c433c2cb82);\n",
       "        \n",
       "    \n",
       "\n",
       "        function geo_json_d61ee14529f1710f58ec9c32c804fd62_onEachFeature(feature, layer) {\n",
       "            layer.on({\n",
       "            });\n",
       "        };\n",
       "        var geo_json_d61ee14529f1710f58ec9c32c804fd62 = L.geoJson(null, {\n",
       "                onEachFeature: geo_json_d61ee14529f1710f58ec9c32c804fd62_onEachFeature,\n",
       "            \n",
       "        });\n",
       "\n",
       "        function geo_json_d61ee14529f1710f58ec9c32c804fd62_add (data) {\n",
       "            geo_json_d61ee14529f1710f58ec9c32c804fd62\n",
       "                .addData(data);\n",
       "        }\n",
       "            geo_json_d61ee14529f1710f58ec9c32c804fd62_add({&quot;bbox&quot;: [14.273475456, 12.950370789, 14.773475456, 13.950370789], &quot;features&quot;: [{&quot;bbox&quot;: [14.273475456, 12.950370789, 14.523475456, 13.200370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.273475456, 12.950370789], [14.523475456, 12.950370789], [14.523475456, 13.200370789], [14.273475456, 13.200370789], [14.273475456, 12.950370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.273475456, 13.200370789, 14.523475456, 13.450370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.273475456, 13.200370789], [14.523475456, 13.200370789], [14.523475456, 13.450370789], [14.273475456, 13.450370789], [14.273475456, 13.200370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.273475456, 13.450370789, 14.523475456, 13.700370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.273475456, 13.450370789], [14.523475456, 13.450370789], [14.523475456, 13.700370789], [14.273475456, 13.700370789], [14.273475456, 13.450370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.273475456, 13.700370789, 14.523475456, 13.950370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.273475456, 13.700370789], [14.523475456, 13.700370789], [14.523475456, 13.950370789], [14.273475456, 13.950370789], [14.273475456, 13.700370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.523475456, 12.950370789, 14.773475456, 13.200370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.523475456, 12.950370789], [14.773475456, 12.950370789], [14.773475456, 13.200370789], [14.523475456, 13.200370789], [14.523475456, 12.950370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.523475456, 13.200370789, 14.773475456, 13.450370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.523475456, 13.200370789], [14.773475456, 13.200370789], [14.773475456, 13.450370789], [14.523475456, 13.450370789], [14.523475456, 13.200370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.523475456, 13.450370789, 14.773475456, 13.700370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.523475456, 13.450370789], [14.773475456, 13.450370789], [14.773475456, 13.700370789], [14.523475456, 13.700370789], [14.523475456, 13.450370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.523475456, 13.700370789, 14.773475456, 13.950370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.523475456, 13.700370789], [14.773475456, 13.700370789], [14.773475456, 13.950370789], [14.523475456, 13.950370789], [14.523475456, 13.700370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}], &quot;type&quot;: &quot;FeatureCollection&quot;});\n",
       "        geo_json_d61ee14529f1710f58ec9c32c804fd62.setStyle(function(feature) {return feature.properties.style;});\n",
       "\n",
       "        \n",
       "    \n",
       "            geo_json_d61ee14529f1710f58ec9c32c804fd62.addTo(map_4019341d1301fd38950201c433c2cb82);\n",
       "        \n",
       "    \n",
       "\n",
       "        function geo_json_f71438e97ca28aba3aadbba1a6a138a4_onEachFeature(feature, layer) {\n",
       "            layer.on({\n",
       "            });\n",
       "        };\n",
       "        var geo_json_f71438e97ca28aba3aadbba1a6a138a4 = L.geoJson(null, {\n",
       "                onEachFeature: geo_json_f71438e97ca28aba3aadbba1a6a138a4_onEachFeature,\n",
       "            \n",
       "        });\n",
       "\n",
       "        function geo_json_f71438e97ca28aba3aadbba1a6a138a4_add (data) {\n",
       "            geo_json_f71438e97ca28aba3aadbba1a6a138a4\n",
       "                .addData(data);\n",
       "        }\n",
       "            geo_json_f71438e97ca28aba3aadbba1a6a138a4_add({&quot;bbox&quot;: [14.273475456, 12.950370789, 14.773475456, 13.850370789], &quot;features&quot;: [{&quot;bbox&quot;: [14.273475456, 12.950370789, 14.773475456, 13.850370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.473475456, 12.950370789], [14.453872027934088, 12.95133384366556], [14.434457391596773, 12.954213732919355], [14.415418520549107, 12.958982721853559], [14.396938769526981, 12.965594882497744], [14.3791961086348, 12.97398653613033], [14.36236140939608, 12.98407686653949], [14.34659679916727, 12.995768698327453], [14.33205409976269, 13.008949432762691], [14.318873365327452, 13.023492132167272], [14.30718153353949, 13.03925674239608], [14.297091203130329, 13.0560914416348], [14.288699549497743, 13.073834102526982], [14.282087388853558, 13.092313853549108], [14.277318399919354, 13.111352724596774], [14.27443851066556, 13.130767360934088], [14.273475456, 13.150370789], [14.273475456, 13.650370789], [14.27443851066556, 13.669974217065912], [14.277318399919354, 13.689388853403226], [14.282087388853558, 13.708427724450893], [14.288699549497743, 13.726907475473018], [14.297091203130329, 13.7446501363652], [14.30718153353949, 13.76148483560392], [14.318873365327452, 13.777249445832728], [14.33205409976269, 13.79179214523731], [14.346596799167271, 13.804972879672547], [14.36236140939608, 13.81666471146051], [14.3791961086348, 13.82675504186967], [14.396938769526981, 13.835146695502257], [14.415418520549107, 13.841758856146441], [14.434457391596773, 13.846527845080645], [14.453872027934088, 13.84940773433444], [14.473475456, 13.850370789], [14.573475456, 13.850370789], [14.593078884065912, 13.84940773433444], [14.612493520403227, 13.846527845080645], [14.631532391450893, 13.841758856146441], [14.650012142473019, 13.835146695502257], [14.6677548033652, 13.82675504186967], [14.684589502603922, 13.81666471146051], [14.700354112832729, 13.804972879672547], [14.71489681223731, 13.79179214523731], [14.728077546672548, 13.777249445832728], [14.73976937846051, 13.76148483560392], [14.749859708869671, 13.7446501363652], [14.758251362502257, 13.726907475473018], [14.764863523146442, 13.708427724450893], [14.769632512080646, 13.689388853403226], [14.77251240133444, 13.669974217065912], [14.773475456, 13.650370789], [14.773475456, 13.150370789], [14.77251240133444, 13.130767360934088], [14.769632512080646, 13.111352724596774], [14.764863523146442, 13.092313853549108], [14.758251362502257, 13.073834102526982], [14.749859708869671, 13.0560914416348], [14.73976937846051, 13.03925674239608], [14.728077546672548, 13.023492132167272], [14.71489681223731, 13.008949432762691], [14.700354112832729, 12.995768698327453], [14.684589502603922, 12.98407686653949], [14.6677548033652, 12.97398653613033], [14.650012142473019, 12.965594882497744], [14.631532391450893, 12.958982721853559], [14.612493520403227, 12.954213732919355], [14.593078884065912, 12.95133384366556], [14.573475456, 12.950370789], [14.473475456, 12.950370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}], &quot;type&quot;: &quot;FeatureCollection&quot;});\n",
       "        geo_json_f71438e97ca28aba3aadbba1a6a138a4.setStyle(function(feature) {return feature.properties.style;});\n",
       "\n",
       "        \n",
       "    \n",
       "            geo_json_f71438e97ca28aba3aadbba1a6a138a4.addTo(map_4019341d1301fd38950201c433c2cb82);\n",
       "        \n",
       "    \n",
       "            var marker_d8b5493af594aa7ca1857bd739b568a7 = L.marker(\n",
       "                [13.07537, 14.39848],\n",
       "                {}\n",
       "            ).addTo(map_4019341d1301fd38950201c433c2cb82);\n",
       "        \n",
       "    \n",
       "            var div_icon_3c3e77691a89e1b173e80259433aa117 = L.divIcon({&quot;className&quot;: &quot;empty&quot;, &quot;html&quot;: &quot;\\u003cdiv style=\\&quot;font-size: 10pt\\&quot;\\u003e1\\u003c/div\\u003e&quot;, &quot;iconAnchor&quot;: [0, 0], &quot;iconSize&quot;: [10, 10]});\n",
       "            marker_d8b5493af594aa7ca1857bd739b568a7.setIcon(div_icon_3c3e77691a89e1b173e80259433aa117);\n",
       "        \n",
       "    \n",
       "        var popup_3c667d5263521708753f37130150e94a = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_93736c03f7c90047b1dbe618c094a82c = $(`&lt;div id=&quot;html_93736c03f7c90047b1dbe618c094a82c&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Centroid: 13.07537, 14.39848&lt;/div&gt;`)[0];\n",
       "                popup_3c667d5263521708753f37130150e94a.setContent(html_93736c03f7c90047b1dbe618c094a82c);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_d8b5493af594aa7ca1857bd739b568a7.bindPopup(popup_3c667d5263521708753f37130150e94a)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_5dd776a51f9a79c5a01096dc5649a8e7 = L.marker(\n",
       "                [13.32537, 14.39848],\n",
       "                {}\n",
       "            ).addTo(map_4019341d1301fd38950201c433c2cb82);\n",
       "        \n",
       "    \n",
       "            var div_icon_915789386bdcedc49954374dd60fbdb9 = L.divIcon({&quot;className&quot;: &quot;empty&quot;, &quot;html&quot;: &quot;\\u003cdiv style=\\&quot;font-size: 10pt\\&quot;\\u003e2\\u003c/div\\u003e&quot;, &quot;iconAnchor&quot;: [0, 0], &quot;iconSize&quot;: [10, 10]});\n",
       "            marker_5dd776a51f9a79c5a01096dc5649a8e7.setIcon(div_icon_915789386bdcedc49954374dd60fbdb9);\n",
       "        \n",
       "    \n",
       "        var popup_00226fe429e1cec17be497def30d7969 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_fb4956725e3ef021bcbc59f52554bc87 = $(`&lt;div id=&quot;html_fb4956725e3ef021bcbc59f52554bc87&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Centroid: 13.32537, 14.39848&lt;/div&gt;`)[0];\n",
       "                popup_00226fe429e1cec17be497def30d7969.setContent(html_fb4956725e3ef021bcbc59f52554bc87);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_5dd776a51f9a79c5a01096dc5649a8e7.bindPopup(popup_00226fe429e1cec17be497def30d7969)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_3d64b1ece0a2f0936783f86d753feba1 = L.marker(\n",
       "                [13.57537, 14.39848],\n",
       "                {}\n",
       "            ).addTo(map_4019341d1301fd38950201c433c2cb82);\n",
       "        \n",
       "    \n",
       "            var div_icon_623b992661123220aa050e105ed91345 = L.divIcon({&quot;className&quot;: &quot;empty&quot;, &quot;html&quot;: &quot;\\u003cdiv style=\\&quot;font-size: 10pt\\&quot;\\u003e3\\u003c/div\\u003e&quot;, &quot;iconAnchor&quot;: [0, 0], &quot;iconSize&quot;: [10, 10]});\n",
       "            marker_3d64b1ece0a2f0936783f86d753feba1.setIcon(div_icon_623b992661123220aa050e105ed91345);\n",
       "        \n",
       "    \n",
       "        var popup_09d209e2a73a6eb897d555645b541f0b = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_6625324f089ec19dd2c10ffc5bd1ff01 = $(`&lt;div id=&quot;html_6625324f089ec19dd2c10ffc5bd1ff01&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Centroid: 13.57537, 14.39848&lt;/div&gt;`)[0];\n",
       "                popup_09d209e2a73a6eb897d555645b541f0b.setContent(html_6625324f089ec19dd2c10ffc5bd1ff01);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_3d64b1ece0a2f0936783f86d753feba1.bindPopup(popup_09d209e2a73a6eb897d555645b541f0b)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            map_4019341d1301fd38950201c433c2cb82.fitBounds(\n",
       "                [[12.950370789, 14.273475456], [13.950370789, 14.773475456]],\n",
       "                {}\n",
       "            );\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f41863e3640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load file from sandbox disc. file should be present in 'input' folder\n",
    "# grid = gpd.read_file(\"input/Lake Chad.geojson\")\n",
    "# grid = gpd.read_file(\"input/TCD_55KM_4CTEST.geojson\")\n",
    "# grid = gpd.read_file(\"input/TCD_55KM_BASE.geojson\")\n",
    "# grid = gpd.read_file(\"input/TCD_55KM_ERR.geojson\")\n",
    "\n",
    "# Create and clean sandbox output directories\n",
    "clean_dirs()\n",
    "\n",
    "shp = \"input/Lake Chad.geojson\"\n",
    "# shp = \"input/TCD_55KM_BASE.geojson\"\n",
    "adm0_b = gpd.read_file(shp)  # adm0 base\n",
    "adm0_b = adm0_b.dissolve()\n",
    "adm0 = adm0_b.buffer(0.2)  # adm0 with 20KM boundary buffer\n",
    "adm0 = crs_check(adm0)\n",
    "size = 0.25  # Grid cell size 0.5 ~ 55KM\n",
    "buffer = size / 2  # cell buffer around the centroid to create the cell\n",
    "grid = create_grid(adm0, size)\n",
    "\n",
    "# Calculate centroids and store in centroid list c[].\n",
    "c = []\n",
    "g = grid.centroid\n",
    "\n",
    "cell_id = 1\n",
    "for i in g:\n",
    "    c.append(\n",
    "        [round(i.x, 5), round(i.y, 5), cell_id, None]\n",
    "    )  # The array c[] has four values: x, y, cell_id and None. None will store the \"P\" or \"U\" error value\n",
    "    cell_id += 1\n",
    "\n",
    "# Splice c to test with a few cells\n",
    "c = c[:3]  # 0-9 cells\n",
    "\n",
    "view_input([grid, adm0], c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and Run Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Drive Storage\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loc</th>\n",
       "      <th>Size in GB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capacity</td>\n",
       "      <td>2199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Available</td>\n",
       "      <td>1838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Usage</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Drive Usage</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trash Usage</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Loc  Size in GB\n",
       "0     Capacity        2199\n",
       "1    Available        1838\n",
       "2        Usage         361\n",
       "3  Drive Usage         361\n",
       "4  Trash Usage           0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd.get_storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before running the execution, ensure all requirements have been met:\n",
      "1. Create appropriate folders in Google Drive and add their Folder IDs here\n",
      "2. Check input shapefile\n",
      "3. Check grid size\n",
      "    \n",
      "    \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Folder IDs verified? (y/n): y\n",
      "Input shapefile/geojson verified? (y/n): y\n",
      "Grid (size) verified? (y/n): y\n",
      "\n",
      "Begin execution for entire input shapefile/geojson? (y/n): y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting execution...\n",
      "\n",
      "\n",
      "\u001b[32mPROCESSING GRID CELL ID 1 NO. 1/3 CENTROID (13.07537, 14.39848)\u001b[0m\n",
      "Using pixel quality parameters for Sentinel 1\n",
      "Finding datasets\n",
      "    s1_rtc\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 62 time steps\n",
      "Uploading...\n",
      "\u001b[32mCELL_1_MAXFLOOD_MAX.tif UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mCELL_1_MAXFLOOD_MAX_META.json UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32mPROCESSING GRID CELL ID 2 NO. 2/3 CENTROID (13.32537, 14.39848)\u001b[0m\n",
      "Using pixel quality parameters for Sentinel 1\n",
      "Finding datasets\n",
      "    s1_rtc\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 62 time steps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Calls the checklist function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mexec_checks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 60\u001b[0m, in \u001b[0;36mexec_checks\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting execution...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# get e_log with centroids, cell_id and error message\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Calling gen_aoim will run the entire Application\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m e_log \u001b[38;5;241m=\u001b[39m \u001b[43mgen_aoim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(e_log))\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36mgen_aoim\u001b[0;34m(c, b)\u001b[0m\n\u001b[1;32m     18\u001b[0m     aoi_m\u001b[38;5;241m.\u001b[39mappend(define_area(i[\u001b[38;5;241m1\u001b[39m], i[\u001b[38;5;241m0\u001b[39m], buffer\u001b[38;5;241m=\u001b[39mb))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print(c, len(aoi_m))\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m e_log \u001b[38;5;241m=\u001b[39m \u001b[43miterate_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43maoi_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# return e_log to be run again\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m e_log\n",
      "Cell \u001b[0;32mIn[10], line 38\u001b[0m, in \u001b[0;36miterate_grid\u001b[0;34m(aoi_m, c)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Load Sentinel1 data\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     S1 \u001b[38;5;241m=\u001b[39m \u001b[43mload_ard\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproducts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms1_rtc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# measurements=[\"vv\", \"vh\"],\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeasurements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlat_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlon_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimerange\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_crs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEPSG:6933\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msolar_day\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnative\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Log error aoi centroids and keep looping\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     e_log\u001b[38;5;241m.\u001b[39mappend([g\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m0\u001b[39m], g\u001b[38;5;241m.\u001b[39my[\u001b[38;5;241m0\u001b[39m], i[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/deafrica_tools/datahandling.py:627\u001b[0m, in \u001b[0;36mload_ard\u001b[0;34m(dc, products, min_gooddata, categories_to_mask_ls, categories_to_mask_s2, categories_to_mask_s1, mask_filters, mask_pixel_quality, ls7_slc_off, predicate, dtype, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ds\u001b[38;5;241m.\u001b[39mtime)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m time steps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/xarray/core/dataset.py:1043\u001b[0m, in \u001b[0;36mDataset.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading and/or computation of this dataset's data\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;124;03mfrom disk or a remote source into memory and return a new dataset.\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;124;03mUnlike load, the original dataset is left unaltered.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;124;03mdask.compute\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/xarray/core/dataset.py:870\u001b[0m, in \u001b[0;36mDataset.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m chunkmanager \u001b[38;5;241m=\u001b[39m get_chunked_array_type(\u001b[38;5;241m*\u001b[39mlazy_data\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    869\u001b[0m \u001b[38;5;66;03m# evaluate all the chunked arrays simultaneously\u001b[39;00m\n\u001b[0;32m--> 870\u001b[0m evaluated_data: \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, Any], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mchunkmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlazy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lazy_data, evaluated_data, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/xarray/namedarray/daskmanager.py:86\u001b[0m, in \u001b[0;36mDaskManager.compute\u001b[0;34m(self, *data, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mdata: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m     83\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, _DType_co], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/dask/base.py:660\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 660\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calls the checklist function\n",
    "exec_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Error Cells\n",
    "e_grid = gpd.GeoDataFrame()\n",
    "for e in e_log:\n",
    "    point = Point(e[0], e[1])  # This takes x first and then y\n",
    "    gdf = gpd.GeoDataFrame(geometry=[point])\n",
    "    buffer = 0.25\n",
    "    cell = gpd.GeoDataFrame()\n",
    "    cell[\"geometry\"] = gdf.buffer(buffer, cap_style=\"square\")\n",
    "    e_grid = pd.concat([e_grid, cell])\n",
    "e_grid = e_grid.set_crs(\"epsg:4326\")  # e_grid with same cell size as main grid\n",
    "\n",
    "# e_grid_aoi = e_grid.dissolve()\n",
    "# e_grid_fine = create_grid(e_grid_aoi, size) # if require to change the size and make it finer\n",
    "\n",
    "view_input([e_grid], e_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run application for cells logged in e_log\n",
    "if len(e_log) > 0:\n",
    "    e_log = gen_aoim(e_log, size / 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

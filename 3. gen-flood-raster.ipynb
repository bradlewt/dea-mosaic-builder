{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE REQUIREMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# !pip install python-dotenv\n",
    "# load_dotenv()\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "import os, glob, warnings, datacube, rasterio, folium, json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as rio\n",
    "import pandas as pd\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "from scipy.ndimage import uniform_filter\n",
    "from scipy.ndimage import variance\n",
    "from skimage.filters import threshold_minimum\n",
    "from datacube.utils.geometry import Geometry\n",
    "\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.plotting import display_map, rgb\n",
    "from deafrica_tools.areaofinterest import define_area\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local modules\n",
    "from tools.gdrive import GDrive\n",
    "\n",
    "gd = GDrive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "\n",
    "Connect to the datacube so we can access DEA data.\n",
    "The `app` parameter is a unique name for the analysis which is based on the notebook file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"Radar_water_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-Drive Folder IDs and Timerange Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder ids by copying the ID from the g-drive folder url\n",
    "# TEST/\n",
    "F_MN_FID = \"1KBig_UZLT0fgFXsACMHQTb7c_Poao-wh\"\n",
    "F_MD_FID = \"1dWde-mzh8Sc9BIEMNUByq7Z6Yags0JZh\"\n",
    "\n",
    "PRF_MN_FID = \"1MVAMwv0E3sZ6qK8E93MlnGcPTr6yXge4\"\n",
    "PRF_MD_FID = \"1oROFY5hKi3w7pbkrCnHtGLIJT_nBraog\"\n",
    "\n",
    "POF_MN_FID = None\n",
    "POF_MD_FID = None\n",
    "\n",
    "FE_MN_FID = \"1RpE2-Pe-KihKdCk0TKLvTzZgrv5QGOeu\"\n",
    "FE_MD_FID = \"1KDTiwmmdaV5LBh6Dj1qAB5KTRNQu_-Xq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main time period of analysis\n",
    "timerange = (\"2024-02\", \"2024-09\")\n",
    "\n",
    "# Define sub-periods of analysis - should be within main time period\n",
    "pre_flood = [\"2024-02\", \"2024-03\", \"2024-04\"]\n",
    "flood = [\"2024-05\", \"2024-06\", \"2024-07\", \"2024-08\", \"2024-09\"]\n",
    "post_flood = []\n",
    "\n",
    "# Run 1. aoi-threshold.ipynb to get the value of th_aoi and store it here.\n",
    "th_aoi = -27.395682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Classifier Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply lee filtering on S1 image. Speckle Filter\n",
    "def lee_filter(da, size):\n",
    "    \"\"\"\n",
    "    Apply lee filter of specified window size.\n",
    "    Adapted from https://stackoverflow.com/questions/39785970/speckle-lee-filter-in-python\n",
    "\n",
    "    \"\"\"\n",
    "    img = da.values\n",
    "    img_mean = uniform_filter(img, size)\n",
    "    img_sqr_mean = uniform_filter(img**2, size)\n",
    "    img_variance = img_sqr_mean - img_mean**2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "\n",
    "    return img_output\n",
    "\n",
    "\n",
    "# Classifier Function\n",
    "def S1_water_classifier(da, threshold):\n",
    "    water_data_array = da < threshold\n",
    "    return water_data_array.to_dataset(name=\"s1_water\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operational Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate raster outputs\n",
    "def gen_output(\n",
    "    DS: xr.DataArray,\n",
    "    i: list,\n",
    "    poly: Geometry,\n",
    "    cell: int,\n",
    "    aoi_m: list,\n",
    "    period: Literal[\"preflood\", \"flood\", \"postflood\", \"flood_extents\"],\n",
    "    measure: Literal[\"mean\", \"median\"],\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Generates file names and uploads tp Google Drive\n",
    "\n",
    "    Parameters:\n",
    "    DS: xr.DataArray, required\n",
    "        Dataset to be converted into raster output.\n",
    "    i: list, required\n",
    "        Index of centroid list.\n",
    "    poly: Geometry, required\n",
    "        Polygon feature used to create the DS.\n",
    "    cell: int, required\n",
    "        Cell number being processed.\n",
    "    aoi_m: list, required\n",
    "        List of geojson feature collection (cells) that make up the entire grid.\n",
    "    period: Literal[\"preflood\", \"flood\", \"postflood\", \"flood_extents\"], required\n",
    "        Time period of the process - PRE_FLOOD, FLOOD.\n",
    "    measure: Literal[\"mean\", \"median\"], required\n",
    "        Central tendency measurement of the DS - mean, median.\n",
    "\n",
    "    Returns:\n",
    "    err: list\n",
    "        Error log list of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload.\n",
    "    \"\"\"\n",
    "\n",
    "    err = []\n",
    "\n",
    "    pm_dict = {\n",
    "        \"preflood_mean\": [pre_flood, PRF_MN_FID],\n",
    "        \"preflood_median\": [pre_flood, PRF_MD_FID],\n",
    "        \"flood_mean\": [flood, F_MN_FID],\n",
    "        \"flood_median\": [flood, F_MD_FID],\n",
    "        \"postflood_mean\": [post_flood, POF_MN_FID],\n",
    "        \"postflood_median\": [post_flood, POF_MD_FID],\n",
    "        \"flood_extents_mean\": [timerange, FE_MN_FID],\n",
    "        \"flood_extents_median\": [timerange, FE_MD_FID],\n",
    "    }\n",
    "\n",
    "    poly_gdf = gpd.GeoDataFrame(geometry=[poly], crs=poly.crs)\n",
    "    lat_range = (poly_gdf.total_bounds[1], poly_gdf.total_bounds[3])\n",
    "    lon_range = (poly_gdf.total_bounds[0], poly_gdf.total_bounds[2])\n",
    "    g = poly_gdf.centroid\n",
    "\n",
    "    data_val = \"CELL_\" + str(i[2]) + \"_{}_{}\".format(period.upper(), measure.upper())\n",
    "    data_name = data_val + \".tif\"\n",
    "    data_out = \"output/{}/\".format(period) + data_name\n",
    "    DS.rio.to_raster(data_out)\n",
    "\n",
    "    # preflood meta\n",
    "    data_dict = {\n",
    "        \"GRID_CELL_ID\": i[2],\n",
    "        \"start_time\": pm_dict[\"{}_{}\".format(period, measure)][0][0],  # pre_flood[0]\n",
    "        \"end_time\": pm_dict[\"{}_{}\".format(period, measure)][0][-1],  # pre_flood[-1]\n",
    "        \"lat\": lat_range,\n",
    "        \"lon\": lon_range,\n",
    "        \"centroid\": \"{}, {}\".format(g.y[0], g.x[0]),\n",
    "        \"crs\": str(poly.crs),\n",
    "    }\n",
    "\n",
    "    text_flie_name = data_val + \"_META.json\"\n",
    "    data_meta_path = \"output/{}/\".format(period) + text_flie_name\n",
    "    with open(data_meta_path, \"w\") as f:\n",
    "        json.dump(data_dict, f)\n",
    "\n",
    "    try:\n",
    "        gd.upload_files(\n",
    "            [data_out, data_meta_path], pm_dict[\"{}_{}\".format(period, measure)][1]\n",
    "        )  # pm_dict['preflood_mean'][1] = FOLDER_ID\n",
    "    except Exception as e:\n",
    "        err.append([g.x[0], g.y[0], i[2], \"U-PRF\"])\n",
    "        print(\n",
    "            \"\\n\\n\"\n",
    "            + \"\\033[31m\"\n",
    "            + \"ERROR UPLOADING GRID CELL ID {} NO.  {}/{} CENTROID ({}, {}). LOGGED CENTROID INFO in e_log\".format(\n",
    "                i[2], cell, len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)\n",
    "            )\n",
    "            + \"\\033[0m\"\n",
    "        )\n",
    "        print(\"UPLOAD ERROR: {}\".format(e))\n",
    "\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_elog(e_log: list) -> list:\n",
    "    \"\"\"\n",
    "    Writes a the error log json file and uploads it to the google drive folder ID, if specified.\n",
    "\n",
    "    Parameters:\n",
    "    e_log: list, required\n",
    "        Error log list of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload.\n",
    "\n",
    "    Returns:\n",
    "    e_log: list\n",
    "        Error log list having the same values as the input parameter\n",
    "    \"\"\"\n",
    "    e_log = np.array(e_log)\n",
    "    with open(\"error_centroids.json\", \"w\") as filehandle:\n",
    "        json.dump(e_log.tolist(), filehandle)\n",
    "\n",
    "    # read error log from disk\n",
    "    with open(\"error_centroids.json\") as f:\n",
    "        e_log = json.load(f)\n",
    "    for e in e_log:\n",
    "        e[0] = float(e[0])\n",
    "        e[1] = float(e[1])\n",
    "        e[2] = int(e[2])\n",
    "\n",
    "    try:\n",
    "        gd.upload_files([\"error_centroids.json\"], ERR_FOLDER_ID, False)\n",
    "    except Exception as e:\n",
    "        print(\"FAILED TO UPLOAD ERROR LOG FILE REASON:{}\".format(e))\n",
    "\n",
    "    return e_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the input grid\n",
    "def iterate_grid(aoi_m: list, c: list) -> list:\n",
    "    \"\"\"\n",
    "    Iterates through every feature (cell) in the AOI grid.\n",
    "\n",
    "    Parameters:\n",
    "    aoi_m: list, required\n",
    "        List of geojson feature collection (cells) that make up the entire grid.\n",
    "    c: list, required\n",
    "        List of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload, if executio fails\n",
    "\n",
    "    Returns:\n",
    "    e_log: list\n",
    "        Error log list of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload.\n",
    "    \"\"\"\n",
    "\n",
    "    e_log = []\n",
    "    cell = 1\n",
    "    for aoi, i in zip(aoi_m, c):\n",
    "        geopolygon = Geometry(aoi[\"features\"][0][\"geometry\"], crs=\"epsg:4326\")\n",
    "        geopolygon_gdf = gpd.GeoDataFrame(geometry=[geopolygon], crs=geopolygon.crs)\n",
    "        g = geopolygon_gdf.centroid\n",
    "        print(\n",
    "            \"\\n\\n\"\n",
    "            + \"\\033[32m\"\n",
    "            + \"PROCESSING GRID CELL ID {} NO. {}/{} CENTROID ({}, {})\".format(\n",
    "                i[2], cell, len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)\n",
    "            )\n",
    "            + \"\\033[0m\"\n",
    "        )\n",
    "\n",
    "        # Get the latitude and longitude range of the geopolygon\n",
    "        lat_range = (geopolygon_gdf.total_bounds[1], geopolygon_gdf.total_bounds[3])\n",
    "        lon_range = (geopolygon_gdf.total_bounds[0], geopolygon_gdf.total_bounds[2])\n",
    "\n",
    "        # Load Sentinel1 data\n",
    "        try:\n",
    "            S1 = load_ard(\n",
    "                dc=dc,\n",
    "                products=[\"s1_rtc\"],\n",
    "                # measurements=[\"vv\", \"vh\"],\n",
    "                measurements=[\"vh\"],\n",
    "                y=lat_range,\n",
    "                x=lon_range,\n",
    "                time=timerange,\n",
    "                output_crs=\"EPSG:6933\",\n",
    "                resolution=(-20, 20),\n",
    "                group_by=\"solar_day\",\n",
    "                dtype=\"native\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Log error aoi centroids and keep looping\n",
    "            e_log.append([g.x[0], g.y[0], i[2], \"P\"])\n",
    "            print(\n",
    "                \"\\n\\n\"\n",
    "                + \"\\033[31m\"\n",
    "                + \"ERROR PROCESSING GRID CELL {}/{} CENTROID ({}, {}). LOGGED CENTROID INFO in e_log\".format(\n",
    "                    i[2], len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)\n",
    "                )\n",
    "                + \"\\033[0m\"\n",
    "            )\n",
    "            print(\"PROCESS ERROR: {}\".format(e))\n",
    "            cell += 1\n",
    "            continue\n",
    "\n",
    "        # timesteps = [2, 4, 6, 9, 11]\n",
    "\n",
    "        # The lee filter above doesn't handle null values\n",
    "        # We therefore set null values to 0 before applying the filter\n",
    "        valid = np.isfinite(S1)\n",
    "        S1 = S1.where(valid, 0)\n",
    "\n",
    "        # Create a new entry in dataset corresponding to filtered VV and VH data\n",
    "        S1[\"filtered_vh\"] = S1.vh.groupby(\"time\").apply(lee_filter, size=7)\n",
    "\n",
    "        # Null pixels should remain null\n",
    "        S1[\"filtered_vh\"] = S1.filtered_vh.where(valid.vh)\n",
    "\n",
    "        # Convert the digital numbers to dB\n",
    "        S1[\"filtered_vh\"] = 10 * np.log10(S1.filtered_vh)\n",
    "\n",
    "        threshold_vh = th_aoi\n",
    "\n",
    "        S1[\"water\"] = S1_water_classifier(S1.filtered_vh, threshold_vh).s1_water\n",
    "        S1Water = S1.water\n",
    "        S1_BIN = S1Water.where(S1Water > 0)\n",
    "        FS1 = S1_BIN\n",
    "        PRFS1 = S1_BIN\n",
    "\n",
    "        # Creating outputs\n",
    "        # Export to raster - upload to g-drive - delete from sandbox\n",
    "\n",
    "        print(\"Uploading...\")\n",
    "        # -------------- preflood ----------------\n",
    "        if i[3] in [None, \"P\", \"U-PRF\"]:\n",
    "            S1_PRF_MD = PRFS1.sel(time=pre_flood, method=\"nearest\").median(dim=\"time\")\n",
    "            S1_PRF_MN = PRFS1.sel(time=pre_flood, method=\"nearest\").mean(dim=\"time\")\n",
    "            err = gen_output(\n",
    "                S1_PRF_MD, i, geopolygon, cell, aoi_m, \"preflood\", \"median\"\n",
    "            )\n",
    "            if len(err) > 0:\n",
    "                e_log.extend(err)\n",
    "            err = gen_output(S1_PRF_MN, i, geopolygon, cell, aoi_m, \"preflood\", \"mean\")\n",
    "            if len(err) > 0:\n",
    "                e_log.extend(err)\n",
    "\n",
    "        # --------------- flood ------------------\n",
    "        if i[3] in [None, \"P\", \"U-F\"]:\n",
    "            S1_F_MD = FS1.sel(time=flood, method=\"nearest\").median(dim=\"time\")\n",
    "            S1_F_MN = FS1.sel(time=flood, method=\"nearest\").mean(dim=\"time\")\n",
    "            err = gen_output(S1_F_MD, i, geopolygon, cell, aoi_m, \"flood\", \"median\")\n",
    "            if len(err) > 0:\n",
    "                e_log.extend(err)\n",
    "            err = gen_output(S1_F_MN, i, geopolygon, cell, aoi_m, \"flood\", \"mean\")\n",
    "            if len(err) > 0:\n",
    "                e_log.extend(err)\n",
    "\n",
    "        # ------------ flood-extents --------------\n",
    "        if i[3] in [None, \"P\", \"U-FE\"]:\n",
    "            S1_FE_MD = S1_F_MD - S1_PRF_MD\n",
    "            S1_FE_MN = S1_F_MN - S1_PRF_MN\n",
    "            err = gen_output(\n",
    "                S1_FE_MD, i, geopolygon, cell, aoi_m, \"flood_extents\", \"median\"\n",
    "            )\n",
    "            if len(err) > 0:\n",
    "                e_log.extend(err)\n",
    "            err = gen_output(\n",
    "                S1_FE_MN, i, geopolygon, cell, aoi_m, \"flood_extents\", \"mean\"\n",
    "            )\n",
    "            if len(err) > 0:\n",
    "                e_log.extend(err)\n",
    "\n",
    "        cell += 1\n",
    "        clear_output()\n",
    "\n",
    "    if len(e_log) == 0:\n",
    "        print(\n",
    "            \"\\n\\n\"\n",
    "            + \"\\033[32m\"\n",
    "            + \"GRID PROCESSED AND UPLOADED SUCCESSFULLY\"\n",
    "            + \"\\033[0m\"\n",
    "            + \"\\n\\n\"\n",
    "        )\n",
    "\n",
    "    # e_log = gen_elog(e_log)\n",
    "\n",
    "    # return e_log to be run again\n",
    "    return e_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crete the aoi-mosaic - aoi_m\n",
    "def gen_aoim(c: list, b: float) -> list:\n",
    "    \"\"\"\n",
    "    Generates the feature collection list (list of cells) using centroid coordinates and a buffer distance. Calls the main iterator for execution as well.\n",
    "\n",
    "    Parameters:\n",
    "    c: list, required\n",
    "        List of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload, if executio fails.\n",
    "    b: float, required\n",
    "        Cell half-dimension in degrees (EPSG:4326). Creates a cell by adding this distance to the centroid coordinates.\n",
    "\n",
    "    Returns:\n",
    "    e_log: list\n",
    "        Error log list of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload.\n",
    "    \"\"\"\n",
    "    aoi_m = []\n",
    "    for i in c:\n",
    "        aoi_m.append(define_area(i[1], i[0], buffer=b))\n",
    "    # print(c, len(aoi_m))\n",
    "    e_log = iterate_grid(aoi_m, c)\n",
    "\n",
    "    # return e_log to be run again\n",
    "    return e_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize input file\n",
    "def view_input(gdf_list: list[gpd.GeoDataFrame], grid_c: list) -> None:\n",
    "    \"\"\"\n",
    "    Visualizes cells and respective IDs  on a basemap.\n",
    "\n",
    "    Parameters:\n",
    "    gdf_list:list[gpd.GeoDataFrame], required\n",
    "        List of geodataframes to be visualized.\n",
    "    grid_c:list, required\n",
    "        List of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload, if executio fails.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Visualizing data...\")\n",
    "    p = gdf_list[0].dissolve()\n",
    "    center = p.centroid\n",
    "    map = folium.Map(location=[center.y, center.x], tiles=\"CartoDB Positron\")\n",
    "\n",
    "    for gdf in gdf_list:\n",
    "        folium.GeoJson(gdf, name=\"{}\".format(gdf)).add_to(map)\n",
    "\n",
    "    for c in grid_c:\n",
    "        folium.Marker(\n",
    "            location=[c[1], c[0]],\n",
    "            popup=f\"Centroid: {c[1]}, {c[0]}\",\n",
    "            icon=folium.DivIcon(\n",
    "                icon_size=(10, 10),\n",
    "                icon_anchor=(0, 0),\n",
    "                html='<div style=\"font-size: 10pt\">{}</div>'.format(c[2]),\n",
    "            ),\n",
    "        ).add_to(map)\n",
    "\n",
    "    bounds = gdf_list[0].total_bounds.tolist()\n",
    "    map.fit_bounds([bounds[:2][::-1], bounds[2:][::-1]])\n",
    "    display(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid\n",
    "from shapely import intersection as intersect\n",
    "\n",
    "\n",
    "def create_grid(adm0: gpd.GeoDataFrame, size: float) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Divides adm0 AOI vectorfile into square grid based on size\n",
    "\n",
    "    Parameters:\n",
    "    adm0:gpd.GeoDataFrame, required\n",
    "        AMD0 GeoDataFrame created from ADM0 input vector file\n",
    "    size:float, required\n",
    "        Grid cell size in degrees (EPSG:4326)\n",
    "\n",
    "    Returns:\n",
    "    grid: gpd.GeoDataFrame\n",
    "        The generated grid GeoDataFrame\n",
    "    \"\"\"\n",
    "    bounds = adm0.bounds\n",
    "    minx = bounds.minx[0]  # only 1 feature at the 0th index\n",
    "    miny = bounds.miny[0]\n",
    "    maxx = bounds.maxx[0]\n",
    "    maxy = bounds.maxy[0]\n",
    "\n",
    "    grid = gpd.GeoDataFrame()\n",
    "    for x0 in np.arange(minx, maxx, size):\n",
    "        for y0 in np.arange(miny, maxy, size):\n",
    "            x1 = x0 + size\n",
    "            y1 = y0 + size\n",
    "            d = {\"geometry\": [Polygon([(x0, y0), (x1, y0), (x1, y1), (x0, y1)])]}\n",
    "            cell = gpd.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
    "            flag = adm0.intersection(cell)\n",
    "            if flag[0].is_empty == False:\n",
    "                grid = pd.concat([grid, cell])\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CRS and convert to 4326 if required\n",
    "def crs_check(shp: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Checks input GeoDataFrame CRS and converts to EPSG 4326, if different.\n",
    "\n",
    "    Parameters:\n",
    "    shp: gpd.GeoDataFrame, required\n",
    "        Input GeoDataFrame to check.\n",
    "\n",
    "    Returns:\n",
    "    shp: gpd.GeoDataFrame\n",
    "        As is or converted GeoDataFrame.\n",
    "    \"\"\"\n",
    "    if shp.crs != \"EPSG:4326\":\n",
    "        print(\"Added ADM0 CRS is {}. Converting to EPSG:4326...\".format(shp.crs))\n",
    "        shp = shp.to_crs(\"EPSG:4326\")\n",
    "        if shp.crs == \"EPSG:4326\":\n",
    "            print(\"Done\")\n",
    "\n",
    "    return shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_inp(x: str) -> None:\n",
    "    \"\"\"\n",
    "    Checks if input is \"y\" or \"n\"\n",
    "\n",
    "    Parameters:\n",
    "    x: str, required\n",
    "        String input to be checked\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if x not in [\"y\", \"n\"]:\n",
    "        raise ValueError(\"Invalid input, must be 'y' or 'n'\")\n",
    "    elif x == \"n\":\n",
    "        raise RuntimeError(\n",
    "            \"Excecution terminated. Make necessary changes before running again\"\n",
    "        )\n",
    "\n",
    "\n",
    "def exec_checks() -> None:\n",
    "    \"\"\"\n",
    "    Performs checks and Run the entire application\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    inst = \"\"\"\n",
    "Before running the execution, ensure all requirements have been met:\n",
    "1. Create appropriate folders in Google Drive and add their Folder IDs here\n",
    "2. Check input shapefile\n",
    "3. Check grid size\n",
    "    \n",
    "    \"\"\"\n",
    "    print(inst)\n",
    "\n",
    "    x = input(\"Folder IDs verified? (y/n):\")\n",
    "    check_inp(x)\n",
    "\n",
    "    x = input(\"Input shapefile/geojson verified? (y/n):\")\n",
    "    check_inp(x)\n",
    "\n",
    "    input(\"Grid (size) verified? (y/n):\")\n",
    "    check_inp(x)\n",
    "\n",
    "    z = input(\"\\nBegin execution for entire input shapefile/geojson? (y/n):\")\n",
    "    if z not in [\"y\", \"n\"]:\n",
    "        raise ValueError(\"Invalid input, must be 'y' or 'n'\")\n",
    "    elif z == \"n\":\n",
    "        raise RuntimeError(\n",
    "            \"Excecution terminated. Make necessary changes before running again\"\n",
    "        )\n",
    "    elif z == \"y\":\n",
    "        print(\"Starting execution...\")\n",
    "        # get e_log with centroids, cell_id and error message\n",
    "        # Calling gen_aoim will run the entire Application\n",
    "        e_log = gen_aoim(c, buffer)\n",
    "        print(len(e_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_files(path: str, ext: str) -> None:\n",
    "    \"\"\"\n",
    "    Deletes all files with specified extention at specified folder path\n",
    "\n",
    "    Parameters:\n",
    "    path:str, required\n",
    "        Folder path.\n",
    "    ext:str, required\n",
    "        File extension. \"*\" for all files.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    res_files = False\n",
    "    loc = os.path.join(path, ext)\n",
    "    files = glob.glob(loc)\n",
    "    if len(files) > 0:\n",
    "        res_files = True\n",
    "        print(\"Found {} files. Deleting...\".format(len(files)))\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "\n",
    "    return res_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dirs() -> None:\n",
    "    \"\"\"\n",
    "    Creates directories if they dont exist or deletes residual files if they exist.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    dirs_exist = False\n",
    "    dir_dict = {\n",
    "        \"sd_flood\": \"output/flood\",\n",
    "        \"sd_preflood\": \"output/preflood\",\n",
    "        \"sd_flood_extents\": \"output/flood_extents\",\n",
    "    }\n",
    "\n",
    "    for k in dir_dict:\n",
    "        if not os.path.exists(dir_dict[k]):\n",
    "            os.makedirs(dir_dict[k])\n",
    "        else:\n",
    "            dirs_exist = True\n",
    "            r = del_files(dir_dict[k], \"*\")\n",
    "\n",
    "    if dirs_exist:\n",
    "        print(\"Output folders alredy exist.\")\n",
    "    else:\n",
    "        print(\"Output folders created.\")\n",
    "\n",
    "    if not r:\n",
    "        print(\"No residual files to delete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Gridded Vector File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folders alredy exist.\n",
      "No residual files to delete.\n",
      "Visualizing data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap-glyphicons.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_9aabf4dbad1bc8ff12cbaf47bbc3e53b {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_9aabf4dbad1bc8ff12cbaf47bbc3e53b&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_9aabf4dbad1bc8ff12cbaf47bbc3e53b = L.map(\n",
       "                &quot;map_9aabf4dbad1bc8ff12cbaf47bbc3e53b&quot;,\n",
       "                {\n",
       "                    center: [13.450370788999997, 14.523475456],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 10,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_64f2382fd56b9da72bdbb7dbdfc81840 = L.tileLayer(\n",
       "                &quot;https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}{r}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors \\u0026copy; \\u003ca href=\\&quot;https://carto.com/attributions\\&quot;\\u003eCARTO\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 20, &quot;maxZoom&quot;: 20, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abcd&quot;, &quot;tms&quot;: false}\n",
       "            );\n",
       "        \n",
       "    \n",
       "            tile_layer_64f2382fd56b9da72bdbb7dbdfc81840.addTo(map_9aabf4dbad1bc8ff12cbaf47bbc3e53b);\n",
       "        \n",
       "    \n",
       "\n",
       "        function geo_json_34b17879ad6b436b290b982800c34073_onEachFeature(feature, layer) {\n",
       "            layer.on({\n",
       "            });\n",
       "        };\n",
       "        var geo_json_34b17879ad6b436b290b982800c34073 = L.geoJson(null, {\n",
       "                onEachFeature: geo_json_34b17879ad6b436b290b982800c34073_onEachFeature,\n",
       "            \n",
       "        });\n",
       "\n",
       "        function geo_json_34b17879ad6b436b290b982800c34073_add (data) {\n",
       "            geo_json_34b17879ad6b436b290b982800c34073\n",
       "                .addData(data);\n",
       "        }\n",
       "            geo_json_34b17879ad6b436b290b982800c34073_add({&quot;bbox&quot;: [14.273475456, 12.950370789, 14.773475456, 13.950370789], &quot;features&quot;: [{&quot;bbox&quot;: [14.273475456, 12.950370789, 14.523475456, 13.200370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.273475456, 12.950370789], [14.523475456, 12.950370789], [14.523475456, 13.200370789], [14.273475456, 13.200370789], [14.273475456, 12.950370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.273475456, 13.200370789, 14.523475456, 13.450370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.273475456, 13.200370789], [14.523475456, 13.200370789], [14.523475456, 13.450370789], [14.273475456, 13.450370789], [14.273475456, 13.200370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.273475456, 13.450370789, 14.523475456, 13.700370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.273475456, 13.450370789], [14.523475456, 13.450370789], [14.523475456, 13.700370789], [14.273475456, 13.700370789], [14.273475456, 13.450370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.273475456, 13.700370789, 14.523475456, 13.950370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.273475456, 13.700370789], [14.523475456, 13.700370789], [14.523475456, 13.950370789], [14.273475456, 13.950370789], [14.273475456, 13.700370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.523475456, 12.950370789, 14.773475456, 13.200370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.523475456, 12.950370789], [14.773475456, 12.950370789], [14.773475456, 13.200370789], [14.523475456, 13.200370789], [14.523475456, 12.950370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.523475456, 13.200370789, 14.773475456, 13.450370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.523475456, 13.200370789], [14.773475456, 13.200370789], [14.773475456, 13.450370789], [14.523475456, 13.450370789], [14.523475456, 13.200370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.523475456, 13.450370789, 14.773475456, 13.700370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.523475456, 13.450370789], [14.773475456, 13.450370789], [14.773475456, 13.700370789], [14.523475456, 13.700370789], [14.523475456, 13.450370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.523475456, 13.700370789, 14.773475456, 13.950370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.523475456, 13.700370789], [14.773475456, 13.700370789], [14.773475456, 13.950370789], [14.523475456, 13.950370789], [14.523475456, 13.700370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}], &quot;type&quot;: &quot;FeatureCollection&quot;});\n",
       "        geo_json_34b17879ad6b436b290b982800c34073.setStyle(function(feature) {return feature.properties.style;});\n",
       "\n",
       "        \n",
       "    \n",
       "            geo_json_34b17879ad6b436b290b982800c34073.addTo(map_9aabf4dbad1bc8ff12cbaf47bbc3e53b);\n",
       "        \n",
       "    \n",
       "\n",
       "        function geo_json_8c28009e1bb5870d0db2b211c4baaef1_onEachFeature(feature, layer) {\n",
       "            layer.on({\n",
       "            });\n",
       "        };\n",
       "        var geo_json_8c28009e1bb5870d0db2b211c4baaef1 = L.geoJson(null, {\n",
       "                onEachFeature: geo_json_8c28009e1bb5870d0db2b211c4baaef1_onEachFeature,\n",
       "            \n",
       "        });\n",
       "\n",
       "        function geo_json_8c28009e1bb5870d0db2b211c4baaef1_add (data) {\n",
       "            geo_json_8c28009e1bb5870d0db2b211c4baaef1\n",
       "                .addData(data);\n",
       "        }\n",
       "            geo_json_8c28009e1bb5870d0db2b211c4baaef1_add({&quot;bbox&quot;: [14.273475456, 12.950370789, 14.773475456, 13.850370789], &quot;features&quot;: [{&quot;bbox&quot;: [14.273475456, 12.950370789, 14.773475456, 13.850370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.473475456, 12.950370789], [14.453872027934088, 12.95133384366556], [14.434457391596773, 12.954213732919355], [14.415418520549107, 12.958982721853559], [14.396938769526981, 12.965594882497744], [14.3791961086348, 12.97398653613033], [14.36236140939608, 12.98407686653949], [14.34659679916727, 12.995768698327453], [14.33205409976269, 13.008949432762691], [14.318873365327452, 13.023492132167272], [14.30718153353949, 13.03925674239608], [14.297091203130329, 13.0560914416348], [14.288699549497743, 13.073834102526982], [14.282087388853558, 13.092313853549108], [14.277318399919354, 13.111352724596774], [14.27443851066556, 13.130767360934088], [14.273475456, 13.150370789], [14.273475456, 13.650370789], [14.27443851066556, 13.669974217065912], [14.277318399919354, 13.689388853403226], [14.282087388853558, 13.708427724450893], [14.288699549497743, 13.726907475473018], [14.297091203130329, 13.7446501363652], [14.30718153353949, 13.76148483560392], [14.318873365327452, 13.777249445832728], [14.33205409976269, 13.79179214523731], [14.346596799167271, 13.804972879672547], [14.36236140939608, 13.81666471146051], [14.3791961086348, 13.82675504186967], [14.396938769526981, 13.835146695502257], [14.415418520549107, 13.841758856146441], [14.434457391596773, 13.846527845080645], [14.453872027934088, 13.84940773433444], [14.473475456, 13.850370789], [14.573475456, 13.850370789], [14.593078884065912, 13.84940773433444], [14.612493520403227, 13.846527845080645], [14.631532391450893, 13.841758856146441], [14.650012142473019, 13.835146695502257], [14.6677548033652, 13.82675504186967], [14.684589502603922, 13.81666471146051], [14.700354112832729, 13.804972879672547], [14.71489681223731, 13.79179214523731], [14.728077546672548, 13.777249445832728], [14.73976937846051, 13.76148483560392], [14.749859708869671, 13.7446501363652], [14.758251362502257, 13.726907475473018], [14.764863523146442, 13.708427724450893], [14.769632512080646, 13.689388853403226], [14.77251240133444, 13.669974217065912], [14.773475456, 13.650370789], [14.773475456, 13.150370789], [14.77251240133444, 13.130767360934088], [14.769632512080646, 13.111352724596774], [14.764863523146442, 13.092313853549108], [14.758251362502257, 13.073834102526982], [14.749859708869671, 13.0560914416348], [14.73976937846051, 13.03925674239608], [14.728077546672548, 13.023492132167272], [14.71489681223731, 13.008949432762691], [14.700354112832729, 12.995768698327453], [14.684589502603922, 12.98407686653949], [14.6677548033652, 12.97398653613033], [14.650012142473019, 12.965594882497744], [14.631532391450893, 12.958982721853559], [14.612493520403227, 12.954213732919355], [14.593078884065912, 12.95133384366556], [14.573475456, 12.950370789], [14.473475456, 12.950370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}], &quot;type&quot;: &quot;FeatureCollection&quot;});\n",
       "        geo_json_8c28009e1bb5870d0db2b211c4baaef1.setStyle(function(feature) {return feature.properties.style;});\n",
       "\n",
       "        \n",
       "    \n",
       "            geo_json_8c28009e1bb5870d0db2b211c4baaef1.addTo(map_9aabf4dbad1bc8ff12cbaf47bbc3e53b);\n",
       "        \n",
       "    \n",
       "            var marker_8111e25716102de83c0c52bececb275b = L.marker(\n",
       "                [13.07537, 14.39848],\n",
       "                {}\n",
       "            ).addTo(map_9aabf4dbad1bc8ff12cbaf47bbc3e53b);\n",
       "        \n",
       "    \n",
       "            var div_icon_af74e7580ddd0e0f35d590a2500ab181 = L.divIcon({&quot;className&quot;: &quot;empty&quot;, &quot;html&quot;: &quot;\\u003cdiv style=\\&quot;font-size: 10pt\\&quot;\\u003e1\\u003c/div\\u003e&quot;, &quot;iconAnchor&quot;: [0, 0], &quot;iconSize&quot;: [10, 10]});\n",
       "            marker_8111e25716102de83c0c52bececb275b.setIcon(div_icon_af74e7580ddd0e0f35d590a2500ab181);\n",
       "        \n",
       "    \n",
       "        var popup_32248b42f1361468988dbc4f78255a5c = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_4942a03d48c8cea6c32665a6b6b702e2 = $(`&lt;div id=&quot;html_4942a03d48c8cea6c32665a6b6b702e2&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Centroid: 13.07537, 14.39848&lt;/div&gt;`)[0];\n",
       "                popup_32248b42f1361468988dbc4f78255a5c.setContent(html_4942a03d48c8cea6c32665a6b6b702e2);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_8111e25716102de83c0c52bececb275b.bindPopup(popup_32248b42f1361468988dbc4f78255a5c)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_cefe4053524ed4d34c18833c2bc6ab48 = L.marker(\n",
       "                [13.32537, 14.39848],\n",
       "                {}\n",
       "            ).addTo(map_9aabf4dbad1bc8ff12cbaf47bbc3e53b);\n",
       "        \n",
       "    \n",
       "            var div_icon_7b958ab861b688e4a9ca931e150df015 = L.divIcon({&quot;className&quot;: &quot;empty&quot;, &quot;html&quot;: &quot;\\u003cdiv style=\\&quot;font-size: 10pt\\&quot;\\u003e2\\u003c/div\\u003e&quot;, &quot;iconAnchor&quot;: [0, 0], &quot;iconSize&quot;: [10, 10]});\n",
       "            marker_cefe4053524ed4d34c18833c2bc6ab48.setIcon(div_icon_7b958ab861b688e4a9ca931e150df015);\n",
       "        \n",
       "    \n",
       "        var popup_dfb5c38a32d3096ef2f234b27e01de01 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_4c734c53fa57da3b6e669a1fbbe8a05f = $(`&lt;div id=&quot;html_4c734c53fa57da3b6e669a1fbbe8a05f&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Centroid: 13.32537, 14.39848&lt;/div&gt;`)[0];\n",
       "                popup_dfb5c38a32d3096ef2f234b27e01de01.setContent(html_4c734c53fa57da3b6e669a1fbbe8a05f);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_cefe4053524ed4d34c18833c2bc6ab48.bindPopup(popup_dfb5c38a32d3096ef2f234b27e01de01)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_e7d3c8bbfcad27d548ec3a39311d3569 = L.marker(\n",
       "                [13.57537, 14.39848],\n",
       "                {}\n",
       "            ).addTo(map_9aabf4dbad1bc8ff12cbaf47bbc3e53b);\n",
       "        \n",
       "    \n",
       "            var div_icon_980172c1a4bc15cc6d58f0ed6dce6d88 = L.divIcon({&quot;className&quot;: &quot;empty&quot;, &quot;html&quot;: &quot;\\u003cdiv style=\\&quot;font-size: 10pt\\&quot;\\u003e3\\u003c/div\\u003e&quot;, &quot;iconAnchor&quot;: [0, 0], &quot;iconSize&quot;: [10, 10]});\n",
       "            marker_e7d3c8bbfcad27d548ec3a39311d3569.setIcon(div_icon_980172c1a4bc15cc6d58f0ed6dce6d88);\n",
       "        \n",
       "    \n",
       "        var popup_71418f6968bf22c301964adb77397091 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_e8cff81d9b7b4483255fa71346acf688 = $(`&lt;div id=&quot;html_e8cff81d9b7b4483255fa71346acf688&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Centroid: 13.57537, 14.39848&lt;/div&gt;`)[0];\n",
       "                popup_71418f6968bf22c301964adb77397091.setContent(html_e8cff81d9b7b4483255fa71346acf688);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_e7d3c8bbfcad27d548ec3a39311d3569.bindPopup(popup_71418f6968bf22c301964adb77397091)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            map_9aabf4dbad1bc8ff12cbaf47bbc3e53b.fitBounds(\n",
       "                [[12.950370789, 14.273475456], [13.950370789, 14.773475456]],\n",
       "                {}\n",
       "            );\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f373ec6d810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load file from sandbox disc. file should be present in 'input' folder\n",
    "# grid = gpd.read_file(\"input/Lake Chad.geojson\")\n",
    "# grid = gpd.read_file(\"input/TCD_55KM_4CTEST.geojson\")\n",
    "# grid = gpd.read_file(\"input/TCD_55KM_BASE.geojson\")\n",
    "# grid = gpd.read_file(\"input/TCD_55KM_ERR.geojson\")\n",
    "\n",
    "# Create and clean sandbox output directories\n",
    "clean_dirs()\n",
    "\n",
    "shp = \"input/Lake Chad.geojson\"\n",
    "# shp = \"input/TCD_55KM_BASE.geojson\"\n",
    "adm0_b = gpd.read_file(shp)  # adm0 base\n",
    "adm0_b = adm0_b.dissolve()\n",
    "adm0 = adm0_b.buffer(0.2)  # adm0 with 20KM boundary buffer\n",
    "adm0 = crs_check(adm0)\n",
    "size = 0.25  # Grid cell size 0.5 ~ 55KM\n",
    "buffer = size / 2  # cell buffer around the centroid to create the cell\n",
    "grid = create_grid(adm0, size)\n",
    "\n",
    "# Calculate centroids and store in centroid list c[].\n",
    "c = []\n",
    "g = grid.centroid\n",
    "\n",
    "cell_id = 1\n",
    "for i in g:\n",
    "    c.append(\n",
    "        [round(i.x, 5), round(i.y, 5), cell_id, None]\n",
    "    )  # The array c[] has four values: x, y, cell_id and None. None will store the \"P\" or \"U\" error value\n",
    "    cell_id += 1\n",
    "\n",
    "# Splice c to test with a few cells\n",
    "c = c[:3]  # 0-9 cells\n",
    "\n",
    "view_input([grid, adm0], c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and Run Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32mGRID PROCESSED AND UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Calls the checklist function\n",
    "exec_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'e_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43me_log\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'e_log' is not defined"
     ]
    }
   ],
   "source": [
    "e_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Error Cells\n",
    "e_grid = gpd.GeoDataFrame()\n",
    "for e in e_log:\n",
    "    point = Point(e[0], e[1])  # This takes x first and then y\n",
    "    gdf = gpd.GeoDataFrame(geometry=[point])\n",
    "    buffer = 0.25\n",
    "    cell = gpd.GeoDataFrame()\n",
    "    cell[\"geometry\"] = gdf.buffer(buffer, cap_style=\"square\")\n",
    "    e_grid = pd.concat([e_grid, cell])\n",
    "e_grid = e_grid.set_crs(\"epsg:4326\")  # e_grid with same cell size as main grid\n",
    "\n",
    "# e_grid_aoi = e_grid.dissolve()\n",
    "# e_grid_fine = create_grid(e_grid_aoi, size) # if require to change the size and make it finer\n",
    "\n",
    "view_input([e_grid], e_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run application for cells logged in e_log\n",
    "if len(e_log) > 0:\n",
    "    e_log = gen_aoim(e_log, size / 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

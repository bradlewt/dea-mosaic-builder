{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Cell\n",
    "## [Jump to outputs](#Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add shapefile\n",
    "shp = \"input/Lake Chad.geojson\"\n",
    "# shp = \"input/CMR_ADM0_20KM-B_3857.geojson\"\n",
    "buffer = 0  # boundary buffer\n",
    "cell_size = 0.1  # grid cell size\n",
    "\n",
    "# Run 1. aoi-threshold.ipynb to get the value of th_aoi and store it here.\n",
    "th_aoi = -21\n",
    "# Set test_cells value if you need to test with initial 'n' cells instead of the entire grid\n",
    "test_cells = 2\n",
    "# Set run_checks to True if you want to run checks\n",
    "run_checks = False\n",
    "\n",
    "# Create folder ids by copying the ID from the g-drive folder url. If not using a particular ID, set as None\n",
    "F_MN_FID = \"1Tq_piYp8TCNBf3-qaEoTy6IZ_OreppvL\"\n",
    "F_MD_FID = None\n",
    "PRF_MN_FID = None\n",
    "PRF_MD_FID = None\n",
    "POF_MN_FID = None\n",
    "POF_MD_FID = None\n",
    "FE_MN_FID = None\n",
    "FE_MD_FID = None\n",
    "MF_MX_FID = \"1rLnPb_hrXdve-mfy5wSjsU2TNd7HfZQu\"\n",
    "\n",
    "cwc_name = \"CMR_CWC_1123-1124\"\n",
    "\n",
    "# Define main time period of analysis\n",
    "timerange = (\"2023-11\", \"2024-11\")\n",
    "# Define sub-periods of analysis - should be within main time period\n",
    "pre_flood = []\n",
    "flood = [\"2023-11\", \"2024-11\"]\n",
    "post_flood = []\n",
    "max_flood = []\n",
    "# Analysis periods\n",
    "a_periods = [\"flood\"]\n",
    "# Required Analysis measurements\n",
    "a_measures = [\"mean\"]  # a_measures = [\"mean\", \"median\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# !pip install python-dotenv\n",
    "# load_dotenv()\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "import os, glob, warnings, datacube, rasterio, folium, json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as rio\n",
    "import pandas as pd\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "from scipy.ndimage import uniform_filter\n",
    "from scipy.ndimage import variance\n",
    "from skimage.filters import threshold_minimum\n",
    "from datacube.utils.geometry import Geometry\n",
    "\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.plotting import display_map, rgb\n",
    "from deafrica_tools.areaofinterest import define_area\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Google Drive (GCS) module\n",
    "from tools.gdrive import GDrive\n",
    "\n",
    "gd = GDrive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Drive Storage\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loc</th>\n",
       "      <th>Size in GB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capacity</td>\n",
       "      <td>2199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Available</td>\n",
       "      <td>1837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Usage</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Drive Usage</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trash Usage</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Loc  Size in GB\n",
       "0     Capacity        2199\n",
       "1    Available        1837\n",
       "2        Usage         362\n",
       "3  Drive Usage         362\n",
       "4  Trash Usage           0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd.get_storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the datacube\n",
    "\n",
    "Connect to the datacube so we can access DEA data.\n",
    "The `app` parameter is a unique name for the analysis which is based on the notebook file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"Radar_water_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and Classifier Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply lee filtering on S1 image. Speckle Filter\n",
    "def lee_filter(da, size):\n",
    "    \"\"\"\n",
    "    Apply lee filter of specified window size.\n",
    "    Adapted from https://stackoverflow.com/questions/39785970/speckle-lee-filter-in-python\n",
    "\n",
    "    \"\"\"\n",
    "    img = da.values\n",
    "    img_mean = uniform_filter(img, size)\n",
    "    img_sqr_mean = uniform_filter(img**2, size)\n",
    "    img_variance = img_sqr_mean - img_mean**2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "\n",
    "    return img_output\n",
    "\n",
    "\n",
    "# Classifier Function\n",
    "def S1_water_classifier(da, threshold):\n",
    "    water_data_array = da < threshold\n",
    "    return water_data_array.to_dataset(name=\"s1_water\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operational Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_months(timerange: tuple) -> list[date]:\n",
    "    \"\"\"\n",
    "    Converts the timerange into a list of months.\n",
    "\n",
    "    Parameters:\n",
    "    timerange (\"YYYY-MM\", \"YYYY-MM\"): tuple, required\n",
    "        Timerange of the analysis. Start month to end month.\n",
    "\n",
    "    Returns:\n",
    "    months: list[date]\n",
    "    \"\"\"\n",
    "    syear, smonth = timerange[0].split(\"-\")\n",
    "    syear = int(syear)\n",
    "    smonth = int(smonth)\n",
    "    start = date(syear, smonth, 30)\n",
    "\n",
    "    eyear, emonth = timerange[1].split(\"-\")\n",
    "    eyear = int(eyear)\n",
    "    emonth = int(emonth)\n",
    "    end = date(eyear, emonth, 30)\n",
    "    months = []\n",
    "    while start <= end:\n",
    "        months.append(start.strftime(\"%Y-%m\"))\n",
    "        start = start + relativedelta(months=+1)\n",
    "\n",
    "    return months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate raster outputs\n",
    "def gen_output(\n",
    "    DS: xr.DataArray,\n",
    "    i: list,\n",
    "    poly: Geometry,\n",
    "    cell: int,\n",
    "    aoi_m: list,\n",
    "    period: Literal[\"preflood\", \"flood\", \"postflood\", \"flood_extents\", \"maxflood\"],\n",
    "    measure: Literal[\"mean\", \"median\", \"max\"],\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Generates file names and uploads tp Google Drive\n",
    "\n",
    "    Parameters:\n",
    "    DS: xr.DataArray, required\n",
    "        Dataset to be converted into raster output.\n",
    "    i: list, required\n",
    "        Index of centroid list.\n",
    "    poly: Geometry, required\n",
    "        Polygon feature used to create the DS.\n",
    "    cell: int, required\n",
    "        Cell number being processed.\n",
    "    aoi_m: list, required\n",
    "        List of geojson feature collection (cells) that make up the entire grid.\n",
    "    period: Literal[\"preflood\", \"flood\", \"postflood\", \"flood_extents\"], required\n",
    "        Time period of the process - PRE_FLOOD, FLOOD.\n",
    "    measure: Literal[\"mean\", \"median\"], required\n",
    "        Central tendency measurement of the DS - mean, median.\n",
    "\n",
    "    Returns:\n",
    "    err: list\n",
    "        Error log list of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload.\n",
    "    \"\"\"\n",
    "\n",
    "    err = []\n",
    "\n",
    "    pm_dict = {\n",
    "        \"preflood_mean\": [pre_flood, PRF_MN_FID],\n",
    "        \"preflood_median\": [pre_flood, PRF_MD_FID],\n",
    "        \"flood_mean\": [flood, F_MN_FID],\n",
    "        \"flood_median\": [flood, F_MD_FID],\n",
    "        \"postflood_mean\": [post_flood, POF_MN_FID],\n",
    "        \"postflood_median\": [post_flood, POF_MD_FID],\n",
    "        \"flood_extents_mean\": [timerange, FE_MN_FID],\n",
    "        \"flood_extents_median\": [timerange, FE_MD_FID],\n",
    "        \"maxflood_max\": [timerange, MF_MX_FID],\n",
    "    }\n",
    "\n",
    "    poly_gdf = gpd.GeoDataFrame(geometry=[poly], crs=poly.crs)\n",
    "    lat_range = (poly_gdf.total_bounds[1], poly_gdf.total_bounds[3])\n",
    "    lon_range = (poly_gdf.total_bounds[0], poly_gdf.total_bounds[2])\n",
    "    g = poly_gdf.centroid\n",
    "\n",
    "    data_val = \"CELL_\" + str(i[2]) + \"_{}_{}\".format(period.upper(), measure.upper())\n",
    "    data_name = data_val + \".tif\"\n",
    "    data_out = \"output/{}/\".format(period) + data_name\n",
    "    DS.rio.to_raster(data_out)\n",
    "\n",
    "    # preflood meta\n",
    "    data_dict = {\n",
    "        \"GRID_CELL_ID\": i[2],\n",
    "        \"start_time\": pm_dict[\"{}_{}\".format(period, measure)][0][0],  # pre_flood[0]\n",
    "        \"end_time\": pm_dict[\"{}_{}\".format(period, measure)][0][-1],  # pre_flood[-1]\n",
    "        \"lat\": lat_range,\n",
    "        \"lon\": lon_range,\n",
    "        \"centroid\": \"{}, {}\".format(g.y[0], g.x[0]),\n",
    "        \"crs\": str(poly.crs),\n",
    "    }\n",
    "\n",
    "    text_flie_name = data_val + \"_META.json\"\n",
    "    data_meta_path = \"output/{}/\".format(period) + text_flie_name\n",
    "    with open(data_meta_path, \"w\") as f:\n",
    "        json.dump(data_dict, f)\n",
    "\n",
    "    try:\n",
    "        gd.upload_files(\n",
    "            [data_out, data_meta_path], pm_dict[\"{}_{}\".format(period, measure)][1]\n",
    "        )  # pm_dict['preflood_mean'][1] = FOLDER_ID\n",
    "    except Exception as e:\n",
    "        err.append([g.x[0], g.y[0], i[2], \"U-PRF\"])\n",
    "        print(\n",
    "            \"\\n\\n\"\n",
    "            + \"\\033[31m\"\n",
    "            + \"ERROR UPLOADING GRID CELL ID {} NO.  {}/{} CENTROID ({}, {}). LOGGED CENTROID INFO in e_log\".format(\n",
    "                i[2], cell, len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)\n",
    "            )\n",
    "            + \"\\033[0m\"\n",
    "        )\n",
    "        print(\"UPLOAD ERROR: {}\".format(e))\n",
    "\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_elog(e_log: list) -> list:\n",
    "    \"\"\"\n",
    "    Writes a the error log json file and uploads it to the google drive folder ID, if specified.\n",
    "\n",
    "    Parameters:\n",
    "    e_log: list, required\n",
    "        Error log list of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload.\n",
    "\n",
    "    Returns:\n",
    "    e_log: list\n",
    "        Error log list having the same values as the input parameter\n",
    "    \"\"\"\n",
    "    e_log = np.array(e_log)\n",
    "    with open(\"error_centroids.json\", \"w\") as filehandle:\n",
    "        json.dump(e_log.tolist(), filehandle)\n",
    "\n",
    "    # read error log from disk\n",
    "    with open(\"error_centroids.json\") as f:\n",
    "        e_log = json.load(f)\n",
    "    for e in e_log:\n",
    "        e[0] = float(e[0])\n",
    "        e[1] = float(e[1])\n",
    "        e[2] = int(e[2])\n",
    "\n",
    "    try:\n",
    "        gd.upload_files([\"error_centroids.json\"], ERR_FOLDER_ID, False)\n",
    "    except Exception as e:\n",
    "        print(\"FAILED TO UPLOAD ERROR LOG FILE REASON:{}\".format(e))\n",
    "\n",
    "    return e_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_table():\n",
    "    # Init pd.DataFrame\n",
    "    headers = [\"cell_id\", \"total\", \"lat\", \"lon\", \"datasets\"]\n",
    "    months = get_months(timerange)\n",
    "    headers.extend(months)\n",
    "    count_table = pd.DataFrame(columns=headers)\n",
    "    return count_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the input grid\n",
    "def iterate_grid(aoi_m: list, c: list, count_table) -> list:\n",
    "    \"\"\"\n",
    "    Iterates through every feature (cell) in the AOI grid.\n",
    "\n",
    "    Parameters:\n",
    "    aoi_m: list, required\n",
    "        List of geojson feature collection (cells) that make up the entire grid.\n",
    "    c: list, required\n",
    "        List of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload, if executio fails\n",
    "    count_table: pd.DataFrame, optional\n",
    "        Water pixel count table.\n",
    "\n",
    "    Returns:\n",
    "    e_log: list\n",
    "        Error log list of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload.\n",
    "    \"\"\"\n",
    "\n",
    "    e_log = []\n",
    "    cell = 1\n",
    "    months = months = get_months(timerange)\n",
    "\n",
    "    # Run the main iterator\n",
    "    for aoi, i in zip(aoi_m, c):\n",
    "        geopolygon = Geometry(aoi[\"features\"][0][\"geometry\"], crs=\"epsg:4326\")\n",
    "        geopolygon_gdf = gpd.GeoDataFrame(geometry=[geopolygon], crs=geopolygon.crs)\n",
    "        g = geopolygon_gdf.centroid\n",
    "        print(\n",
    "            \"\\n\\n\"\n",
    "            + \"\\033[32m\"\n",
    "            + \"PROCESSING GRID CELL ID {} NO. {}/{} CENTROID ({}, {})\".format(\n",
    "                i[2], cell, len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)\n",
    "            )\n",
    "            + \"\\033[0m\"\n",
    "        )\n",
    "\n",
    "        # Get the latitude and longitude range of the geopolygon\n",
    "        lat_range = (geopolygon_gdf.total_bounds[1], geopolygon_gdf.total_bounds[3])\n",
    "        lon_range = (geopolygon_gdf.total_bounds[0], geopolygon_gdf.total_bounds[2])\n",
    "\n",
    "        # Load Sentinel1 data\n",
    "        try:\n",
    "            S1 = load_ard(\n",
    "                dc=dc,\n",
    "                products=[\"s1_rtc\"],\n",
    "                # measurements=[\"vv\", \"vh\"],\n",
    "                measurements=[\"vh\"],\n",
    "                y=lat_range,\n",
    "                x=lon_range,\n",
    "                time=timerange,\n",
    "                output_crs=\"EPSG:6933\",\n",
    "                resolution=(-20, 20),\n",
    "                group_by=\"solar_day\",\n",
    "                dtype=\"native\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Log error aoi centroids and keep looping\n",
    "            e_log.append([g.x[0], g.y[0], i[2], \"P\"])\n",
    "            print(\n",
    "                \"\\n\\n\"\n",
    "                + \"\\033[31m\"\n",
    "                + \"ERROR PROCESSING GRID CELL {}/{} CENTROID ({}, {}). LOGGED CENTROID INFO in e_log\".format(\n",
    "                    i[2], len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)\n",
    "                )\n",
    "                + \"\\033[0m\"\n",
    "            )\n",
    "            print(\"PROCESS ERROR: {}\".format(e))\n",
    "            cell += 1\n",
    "            continue\n",
    "\n",
    "        datasets = S1.time\n",
    "        dn = len(datasets.time)\n",
    "\n",
    "        # Initialize row for the count_table\n",
    "        count_row = [i[2], len(aoi_m), round(g.y[0], 5), round(g.x[0], 5), dn]\n",
    "\n",
    "        # timesteps = [2, 4, 6, 9, 11]\n",
    "\n",
    "        # The lee filter above doesn't handle null values\n",
    "        # We therefore set null values to 0 before applying the filter\n",
    "        valid = np.isfinite(S1)\n",
    "        S1 = S1.where(valid, 0)\n",
    "\n",
    "        # Create a new entry in dataset corresponding to filtered VV and VH data\n",
    "        S1[\"filtered_vh\"] = S1.vh.groupby(\"time\").apply(lee_filter, size=7)\n",
    "\n",
    "        # Null pixels should remain null\n",
    "        S1[\"filtered_vh\"] = S1.filtered_vh.where(valid.vh)\n",
    "\n",
    "        # Convert the digital numbers to dB\n",
    "        S1[\"filtered_vh\"] = 10 * np.log10(S1.filtered_vh)\n",
    "\n",
    "        threshold_vh = th_aoi\n",
    "\n",
    "        S1[\"water\"] = S1_water_classifier(S1.filtered_vh, threshold_vh).s1_water\n",
    "        S1Water = S1.water\n",
    "        S1_BIN = S1Water.where(S1Water > 0)\n",
    "        FS1 = S1_BIN\n",
    "        PRFS1 = S1_BIN\n",
    "\n",
    "        # Generate CSV\n",
    "        print(\"Generating row data...\")\n",
    "        for month in months:\n",
    "            S1_month = S1_BIN.sel(time=[month], method=\"nearest\").mean(dim=\"time\")\n",
    "            # Get count of all pixels that are water and not np.nan\n",
    "            count = np.count_nonzero(~np.isnan(S1_month.values))\n",
    "            count_row.append(count)\n",
    "        print(count_row)\n",
    "        print(\"Adding to table\")\n",
    "        count_table = pd.concat(\n",
    "            [pd.DataFrame([count_row], columns=count_table.columns), count_table],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        count_table.to_csv(\"output/csv/{}.csv\".format(cwc_name), index=False, mode=\"w\")\n",
    "\n",
    "        # Creating outputs\n",
    "        # Export to raster - upload to g-drive - delete from sandbox\n",
    "        print(\"Analysis periods:\")\n",
    "        for p in a_periods:\n",
    "            print(p)\n",
    "\n",
    "        print(\"\\nUploading...\")\n",
    "        # -------------- maxflood ----------------\n",
    "        if \"max_flood\" in a_periods:\n",
    "            if i[3] in [None, \"P\", \"U-PRF\"]:\n",
    "                S1_MX = S1_BIN.sel(time=max_flood, method=\"nearest\").max(dim=\"time\")\n",
    "                err = gen_output(S1_MX, i, geopolygon, cell, aoi_m, \"maxflood\", \"max\")\n",
    "\n",
    "        # # -------------- preflood ----------------\n",
    "        if \"pre_flood\" in a_periods:\n",
    "            if i[3] in [None, \"P\", \"U-PRF\"]:\n",
    "                S1_PRF_MD = PRFS1.sel(time=pre_flood, method=\"nearest\").median(\n",
    "                    dim=\"time\"\n",
    "                )\n",
    "                S1_PRF_MN = PRFS1.sel(time=pre_flood, method=\"nearest\").mean(dim=\"time\")\n",
    "                if \"median\" in a_measures:\n",
    "                    err = gen_output(\n",
    "                        S1_PRF_MD, i, geopolygon, cell, aoi_m, \"preflood\", \"median\"\n",
    "                    )\n",
    "                    if len(err) > 0:\n",
    "                        e_log.extend(err)\n",
    "                if \"mean\" in a_measures:\n",
    "                    err = gen_output(\n",
    "                        S1_PRF_MN, i, geopolygon, cell, aoi_m, \"preflood\", \"mean\"\n",
    "                    )\n",
    "                    if len(err) > 0:\n",
    "                        e_log.extend(err)\n",
    "\n",
    "        # # --------------- flood ------------------\n",
    "        if \"flood\" in a_periods:\n",
    "            if i[3] in [None, \"P\", \"U-F\"]:\n",
    "                S1_F_MD = FS1.sel(time=flood, method=\"nearest\").median(dim=\"time\")\n",
    "                S1_F_MN = FS1.sel(time=flood, method=\"nearest\").mean(dim=\"time\")\n",
    "                if \"median\" in a_measures:\n",
    "                    err = gen_output(\n",
    "                        S1_F_MD, i, geopolygon, cell, aoi_m, \"flood\", \"median\"\n",
    "                    )\n",
    "                    if len(err) > 0:\n",
    "                        e_log.extend(err)\n",
    "                if \"mean\" in a_measures:\n",
    "                    err = gen_output(\n",
    "                        S1_F_MN, i, geopolygon, cell, aoi_m, \"flood\", \"mean\"\n",
    "                    )\n",
    "                    if len(err) > 0:\n",
    "                        e_log.extend(err)\n",
    "\n",
    "        # # ------------ flood-extents --------------\n",
    "        if \"flood_extents\" in a_periods:\n",
    "            if i[3] in [None, \"P\", \"U-FE\"]:\n",
    "                S1_FE_MD = S1_F_MD - S1_PRF_MD\n",
    "                S1_FE_MN = S1_F_MN - S1_PRF_MN\n",
    "                if \"median\" in a_measures:\n",
    "                    err = gen_output(\n",
    "                        S1_FE_MD, i, geopolygon, cell, aoi_m, \"flood_extents\", \"median\"\n",
    "                    )\n",
    "                    if len(err) > 0:\n",
    "                        e_log.extend(err)\n",
    "                if \"mean\" in a_measures:\n",
    "                    err = gen_output(\n",
    "                        S1_FE_MN, i, geopolygon, cell, aoi_m, \"flood_extents\", \"mean\"\n",
    "                    )\n",
    "                    if len(err) > 0:\n",
    "                        e_log.extend(err)\n",
    "\n",
    "        cell += 1\n",
    "        clear_output()\n",
    "\n",
    "    if len(e_log) == 0:\n",
    "        print(\n",
    "            \"\\n\\n\"\n",
    "            + \"\\033[32m\"\n",
    "            + \"GRID PROCESSED AND UPLOADED SUCCESSFULLY\"\n",
    "            + \"\\033[0m\"\n",
    "            + \"\\n\\n\"\n",
    "        )\n",
    "    else:\n",
    "        e_log = gen_elog(e_log)\n",
    "\n",
    "    # return e_log to be run again\n",
    "    return e_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crete the aoi-mosaic - aoi_m\n",
    "def gen_aoim(c: list, b: float, count_table) -> list:\n",
    "    \"\"\"\n",
    "    Generates the feature collection list (list of cells) using centroid coordinates and a buffer distance. Calls the main iterator for execution as well.\n",
    "\n",
    "    Parameters:\n",
    "    c: list, required\n",
    "        List of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload, if executio fails.\n",
    "    b: float, required\n",
    "        Cell half-dimension in degrees (EPSG:4326). Creates a cell by adding this distance to the centroid coordinates.\n",
    "    count_table: pd.DataFrame, optional\n",
    "        Water pixel count table.\n",
    "\n",
    "    Returns:\n",
    "    e_log: list\n",
    "        Error log list of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload.\n",
    "    \"\"\"\n",
    "    aoi_m = []\n",
    "    for i in c:\n",
    "        aoi_m.append(define_area(i[1], i[0], buffer=b))\n",
    "    # print(c, len(aoi_m))\n",
    "    e_log = iterate_grid(aoi_m, c, count_table)\n",
    "\n",
    "    # return e_log to be run again\n",
    "    return e_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize input file\n",
    "def view_input(gdf_list: list[gpd.GeoDataFrame], grid_c: list) -> None:\n",
    "    \"\"\"\n",
    "    Visualizes cells and respective IDs  on a basemap.\n",
    "\n",
    "    Parameters:\n",
    "    gdf_list:list[gpd.GeoDataFrame], required\n",
    "        List of geodataframes to be visualized.\n",
    "    grid_c:list, required\n",
    "        List of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload, if executio fails.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Visualizing data...\")\n",
    "    p = gdf_list[0].dissolve()\n",
    "    center = p.centroid\n",
    "    map = folium.Map(location=[center.y, center.x], tiles=\"CartoDB Positron\")\n",
    "\n",
    "    for gdf in gdf_list:\n",
    "        folium.GeoJson(gdf, name=\"{}\".format(gdf)).add_to(map)\n",
    "\n",
    "    for c in grid_c:\n",
    "        folium.Marker(\n",
    "            location=[c[1], c[0]],\n",
    "            popup=f\"Centroid: {c[1]}, {c[0]}\",\n",
    "            icon=folium.DivIcon(\n",
    "                icon_size=(10, 10),\n",
    "                icon_anchor=(0, 0),\n",
    "                html='<div style=\"font-size: 10pt\">{}</div>'.format(c[2]),\n",
    "            ),\n",
    "        ).add_to(map)\n",
    "\n",
    "    bounds = gdf_list[0].total_bounds.tolist()\n",
    "    map.fit_bounds([bounds[:2][::-1], bounds[2:][::-1]])\n",
    "    display(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid\n",
    "def create_grid(adm0: gpd.GeoDataFrame, size: float) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Divides adm0 AOI vectorfile into square grid based on size\n",
    "\n",
    "    Parameters:\n",
    "    adm0:gpd.GeoDataFrame, required\n",
    "        AMD0 GeoDataFrame created from ADM0 input vector file\n",
    "    size:float, required\n",
    "        Grid cell size in degrees (EPSG:4326)\n",
    "\n",
    "    Returns:\n",
    "    grid: gpd.GeoDataFrame\n",
    "        The generated grid GeoDataFrame\n",
    "    \"\"\"\n",
    "    bounds = adm0.bounds\n",
    "    minx = bounds.minx[0]  # only 1 feature at the 0th index\n",
    "    miny = bounds.miny[0]\n",
    "    maxx = bounds.maxx[0]\n",
    "    maxy = bounds.maxy[0]\n",
    "\n",
    "    grid = gpd.GeoDataFrame()\n",
    "    for x0 in np.arange(minx, maxx, size):\n",
    "        for y0 in np.arange(miny, maxy, size):\n",
    "            x1 = x0 + size\n",
    "            y1 = y0 + size\n",
    "            d = {\"geometry\": [Polygon([(x0, y0), (x1, y0), (x1, y1), (x0, y1)])]}\n",
    "            cell = gpd.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
    "            flag = adm0.intersection(cell)\n",
    "            if flag[0].is_empty == False:\n",
    "                grid = pd.concat([grid, cell])\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CRS and convert to 4326 if required\n",
    "def crs_check(shp: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Checks input GeoDataFrame CRS and converts to EPSG 4326, if different.\n",
    "\n",
    "    Parameters:\n",
    "    shp: gpd.GeoDataFrame, required\n",
    "        Input GeoDataFrame to check.\n",
    "\n",
    "    Returns:\n",
    "    shp: gpd.GeoDataFrame\n",
    "        As is or converted GeoDataFrame.\n",
    "    \"\"\"\n",
    "    if shp.crs != \"EPSG:4326\":\n",
    "        print(\"Added ADM0 CRS is {}. Converting to EPSG:4326...\".format(shp.crs))\n",
    "        shp = shp.to_crs(\"EPSG:4326\")\n",
    "        if shp.crs == \"EPSG:4326\":\n",
    "            print(\"Done\")\n",
    "\n",
    "    return shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_inp(x: str) -> None:\n",
    "    \"\"\"\n",
    "    Checks if input is \"y\" or \"n\"\n",
    "\n",
    "    Parameters:\n",
    "    x: str, required\n",
    "        String input to be checked\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if x not in [\"y\", \"n\"]:\n",
    "        raise ValueError(\"Invalid input, must be 'y' or 'n'\")\n",
    "    elif x == \"n\":\n",
    "        raise RuntimeError(\n",
    "            \"Excecution terminated. Make necessary changes before running again\"\n",
    "        )\n",
    "\n",
    "\n",
    "def exec_checks(c, buffer, count_table) -> list:\n",
    "    \"\"\"\n",
    "    Performs checks and Run the entire application\n",
    "\n",
    "    Parameters:\n",
    "    c: list, required\n",
    "        List of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload, if executio fails.\n",
    "    buffer: float, required\n",
    "        Cell half-dimension in degrees (EPSG:4326). Creates a cell by adding this distance to the centroid coordinates.\n",
    "\n",
    "    Returns:\n",
    "    Returns:\n",
    "    e_log: list\n",
    "        Error log list of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload.\n",
    "    \"\"\"\n",
    "\n",
    "    inst = \"\"\"\n",
    "Before running the execution, ensure all requirements have been met:\n",
    "1. Create appropriate folders in Google Drive and add their Folder IDs here\n",
    "2. Check input shapefile\n",
    "3. Check grid size\n",
    "    \n",
    "    \"\"\"\n",
    "    print(inst)\n",
    "\n",
    "    x = input(\"Folder IDs verified? (y/n):\")\n",
    "    check_inp(x)\n",
    "\n",
    "    x = input(\"Input shapefile/geojson verified? (y/n):\")\n",
    "    check_inp(x)\n",
    "\n",
    "    input(\"Grid (size) verified? (y/n):\")\n",
    "    check_inp(x)\n",
    "\n",
    "    z = input(\"\\nBegin execution for entire input shapefile/geojson? (y/n):\")\n",
    "    if z not in [\"y\", \"n\"]:\n",
    "        raise ValueError(\"Invalid input, must be 'y' or 'n'\")\n",
    "    elif z == \"n\":\n",
    "        raise RuntimeError(\n",
    "            \"Excecution terminated. Make necessary changes before running again\"\n",
    "        )\n",
    "    elif z == \"y\":\n",
    "        print(\"Starting execution...\")\n",
    "        # get e_log with centroids, cell_id and error message\n",
    "        # Calling gen_aoim will run the entire iteration process\n",
    "        e_log = gen_aoim(c, buffer, count_table)\n",
    "        print(len(e_log))\n",
    "        return e_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_files(path: str, ext: str) -> None:\n",
    "    \"\"\"\n",
    "    Deletes all files with specified extention at specified folder path\n",
    "\n",
    "    Parameters:\n",
    "    path:str, required\n",
    "        Folder path.\n",
    "    ext:str, required\n",
    "        File extension. \"*\" for all files.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    res_files = False\n",
    "    loc = os.path.join(path, ext)\n",
    "    files = glob.glob(loc)\n",
    "    if len(files) > 0:\n",
    "        res_files = True\n",
    "        print(\"Found {} files. Deleting...\".format(len(files)))\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "\n",
    "    return res_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dirs() -> None:\n",
    "    \"\"\"\n",
    "    Creates directories if they dont exist or deletes residual files if they exist.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    dirs_exist = False\n",
    "    dir_dict = {\n",
    "        \"sd_flood\": \"output/flood\",\n",
    "        \"sd_preflood\": \"output/preflood\",\n",
    "        \"sd_flood_extents\": \"output/flood_extents\",\n",
    "    }\n",
    "\n",
    "    for k in dir_dict:\n",
    "        if not os.path.exists(dir_dict[k]):\n",
    "            os.makedirs(dir_dict[k])\n",
    "        else:\n",
    "            dirs_exist = True\n",
    "            r = del_files(dir_dict[k], \"*\")\n",
    "\n",
    "    if dirs_exist:\n",
    "        print(\"Output folders alredy exist.\")\n",
    "    else:\n",
    "        print(\"Output folders created.\")\n",
    "\n",
    "    if not r:\n",
    "        print(\"No residual files to delete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shapefile(centroids, filename, preview=False):\n",
    "    # Export grid polygon\n",
    "    o_grid = gpd.GeoDataFrame()\n",
    "    for i in centroids:\n",
    "        point = Point(i[0], i[1])  # This takes x first and then y\n",
    "        gdf = gpd.GeoDataFrame(geometry=[point])\n",
    "        buffer = cell_size / 2\n",
    "        cell = gpd.GeoDataFrame()\n",
    "        cell[\"geometry\"] = gdf.buffer(buffer, cap_style=\"square\")\n",
    "        cell.set_geometry(\"geometry\")\n",
    "        cell[\"cell_id\"] = i[2]\n",
    "        o_grid = pd.concat([o_grid, cell])\n",
    "\n",
    "    o_grid = o_grid.set_crs(\"epsg:4326\")\n",
    "    o_grid.to_file(\"output/shape/{}_grid.geojson\".format(filename), driver=\"GeoJSON\")\n",
    "    if preview:\n",
    "        view_input([o_grid], centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_adm(\n",
    "    shp: str, boundary_buffer: float, cell_size: float, test_cells: int = None\n",
    ") -> list | gpd.GeoDataFrame | pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes the input ADM file. File does not have to be ADM0 and can be any vectorfile.\n",
    "\n",
    "    Parameters:\n",
    "    shp: str, required\n",
    "        Path of input vectorfile. Will be converted to EPSG:4326.\n",
    "    boundary_buffer: float, required\n",
    "        Outer boundary buffer to be given to the input vector file in degrees (EPSG:4326)\n",
    "    cell_size: float, required\n",
    "        Grid cell size in degrees (EPSG:4326)\n",
    "\n",
    "    Returns:\n",
    "    c: list\n",
    "        List of [x, y, cell_id and None]. None will store the error \"P\" - Processing or \"U\" - Upload, if executio fails.\n",
    "    grid: GeoDataFrame\n",
    "        Grid file generated from input file\n",
    "    adm_df: Dataframe\n",
    "        Input file information\n",
    "    \"\"\"\n",
    "    adm0_b = gpd.read_file(shp)  # adm0 base\n",
    "    adm0_b = adm0_b.dissolve()\n",
    "    adm0_buf = adm0_b.buffer(boundary_buffer)  # adm0 with 20KM boundary buffer\n",
    "    adm0 = crs_check(adm0_buf)\n",
    "    size = cell_size  # Grid cell size 0.5 ~ 55KM\n",
    "    buffer = size / 2  # cell buffer around the centroid to create the cell\n",
    "\n",
    "    grid = create_grid(adm0, size)\n",
    "    # Calculate centroids and store in centroid list c[].\n",
    "    c = []\n",
    "    g = grid.centroid\n",
    "\n",
    "    cell_id = 1\n",
    "    for i in g:\n",
    "        c.append(\n",
    "            [round(i.x, 5), round(i.y, 5), cell_id, None]\n",
    "        )  # The array c[] has four values: x, y, cell_id and None. None will store the \"P\" or \"U\" error value\n",
    "        cell_id += 1\n",
    "\n",
    "    n = len(c)\n",
    "\n",
    "    if test_cells != None:\n",
    "        c = c[:test_cells]\n",
    "    else:\n",
    "        test_cells = 0\n",
    "\n",
    "    view_input([grid, adm0], c)\n",
    "\n",
    "    adm_data = {\n",
    "        \"Parameter\": [\n",
    "            \"Input File Path\",\n",
    "            \"Area\",\n",
    "            \"Area with Buffer\",\n",
    "            \"Cell Size\",\n",
    "            \"Total Cells\",\n",
    "            \"Test Cells\",\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            shp,\n",
    "            \"{} KM2\".format(\n",
    "                round((adm0_b.to_crs(\"EPSG:3857\").area).iloc[0] / (10**6), 2)\n",
    "            ),\n",
    "            \"{} KM2\".format(\n",
    "                round((adm0.to_crs(\"EPSG:3857\").area).iloc[0] / (10**6), 2)\n",
    "            ),\n",
    "            \"{} deg\".format(cell_size),\n",
    "            n,\n",
    "            test_cells,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    adm_df = pd.DataFrame(adm_data)\n",
    "    adm_df.style.set_caption(\"Input Data\")\n",
    "\n",
    "    return (c, grid, adm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "## [Jump to config](#Config-Cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap-glyphicons.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_816c36d5b1143ced3a999b3ca380aedb {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_816c36d5b1143ced3a999b3ca380aedb&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_816c36d5b1143ced3a999b3ca380aedb = L.map(\n",
       "                &quot;map_816c36d5b1143ced3a999b3ca380aedb&quot;,\n",
       "                {\n",
       "                    center: [13.400370789, 14.573475456],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 10,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_339f88d37cc58ed5ef70bc1550ad0bd6 = L.tileLayer(\n",
       "                &quot;https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}{r}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors \\u0026copy; \\u003ca href=\\&quot;https://carto.com/attributions\\&quot;\\u003eCARTO\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 20, &quot;maxZoom&quot;: 20, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abcd&quot;, &quot;tms&quot;: false}\n",
       "            );\n",
       "        \n",
       "    \n",
       "            tile_layer_339f88d37cc58ed5ef70bc1550ad0bd6.addTo(map_816c36d5b1143ced3a999b3ca380aedb);\n",
       "        \n",
       "    \n",
       "\n",
       "        function geo_json_f51d0fc912d02b60c2e0cc7a917e7f1f_onEachFeature(feature, layer) {\n",
       "            layer.on({\n",
       "            });\n",
       "        };\n",
       "        var geo_json_f51d0fc912d02b60c2e0cc7a917e7f1f = L.geoJson(null, {\n",
       "                onEachFeature: geo_json_f51d0fc912d02b60c2e0cc7a917e7f1f_onEachFeature,\n",
       "            \n",
       "        });\n",
       "\n",
       "        function geo_json_f51d0fc912d02b60c2e0cc7a917e7f1f_add (data) {\n",
       "            geo_json_f51d0fc912d02b60c2e0cc7a917e7f1f\n",
       "                .addData(data);\n",
       "        }\n",
       "            geo_json_f51d0fc912d02b60c2e0cc7a917e7f1f_add({&quot;bbox&quot;: [14.473475456, 13.150370789, 14.673475455999998, 13.650370788999998], &quot;features&quot;: [{&quot;bbox&quot;: [14.473475456, 13.150370789, 14.573475455999999, 13.250370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.473475456, 13.150370789], [14.573475455999999, 13.150370789], [14.573475455999999, 13.250370789], [14.473475456, 13.250370789], [14.473475456, 13.150370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.473475456, 13.250370789, 14.573475455999999, 13.350370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.473475456, 13.250370789], [14.573475455999999, 13.250370789], [14.573475455999999, 13.350370789], [14.473475456, 13.350370789], [14.473475456, 13.250370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.473475456, 13.350370789, 14.573475455999999, 13.450370788999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.473475456, 13.350370789], [14.573475455999999, 13.350370789], [14.573475455999999, 13.450370788999999], [14.473475456, 13.450370788999999], [14.473475456, 13.350370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.473475456, 13.450370788999999, 14.573475455999999, 13.550370788999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.473475456, 13.450370788999999], [14.573475455999999, 13.450370788999999], [14.573475455999999, 13.550370788999999], [14.473475456, 13.550370788999999], [14.473475456, 13.450370788999999]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.473475456, 13.550370788999999, 14.573475455999999, 13.650370788999998], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.473475456, 13.550370788999999], [14.573475455999999, 13.550370788999999], [14.573475455999999, 13.650370788999998], [14.473475456, 13.650370788999998], [14.473475456, 13.550370788999999]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.573475455999999, 13.150370789, 14.673475455999998, 13.250370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.573475455999999, 13.150370789], [14.673475455999998, 13.150370789], [14.673475455999998, 13.250370789], [14.573475455999999, 13.250370789], [14.573475455999999, 13.150370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.573475455999999, 13.250370789, 14.673475455999998, 13.350370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.573475455999999, 13.250370789], [14.673475455999998, 13.250370789], [14.673475455999998, 13.350370789], [14.573475455999999, 13.350370789], [14.573475455999999, 13.250370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.573475455999999, 13.350370789, 14.673475455999998, 13.450370788999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.573475455999999, 13.350370789], [14.673475455999998, 13.350370789], [14.673475455999998, 13.450370788999999], [14.573475455999999, 13.450370788999999], [14.573475455999999, 13.350370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.573475455999999, 13.450370788999999, 14.673475455999998, 13.550370788999999], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.573475455999999, 13.450370788999999], [14.673475455999998, 13.450370788999999], [14.673475455999998, 13.550370788999999], [14.573475455999999, 13.550370788999999], [14.573475455999999, 13.450370788999999]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [14.573475455999999, 13.550370788999999, 14.673475455999998, 13.650370788999998], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.573475455999999, 13.550370788999999], [14.673475455999998, 13.550370788999999], [14.673475455999998, 13.650370788999998], [14.573475455999999, 13.650370788999998], [14.573475455999999, 13.550370788999999]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}], &quot;type&quot;: &quot;FeatureCollection&quot;});\n",
       "        geo_json_f51d0fc912d02b60c2e0cc7a917e7f1f.setStyle(function(feature) {return feature.properties.style;});\n",
       "\n",
       "        \n",
       "    \n",
       "            geo_json_f51d0fc912d02b60c2e0cc7a917e7f1f.addTo(map_816c36d5b1143ced3a999b3ca380aedb);\n",
       "        \n",
       "    \n",
       "\n",
       "        function geo_json_3c80d4eac25e019246d5a17d65a0fa42_onEachFeature(feature, layer) {\n",
       "            layer.on({\n",
       "            });\n",
       "        };\n",
       "        var geo_json_3c80d4eac25e019246d5a17d65a0fa42 = L.geoJson(null, {\n",
       "                onEachFeature: geo_json_3c80d4eac25e019246d5a17d65a0fa42_onEachFeature,\n",
       "            \n",
       "        });\n",
       "\n",
       "        function geo_json_3c80d4eac25e019246d5a17d65a0fa42_add (data) {\n",
       "            geo_json_3c80d4eac25e019246d5a17d65a0fa42\n",
       "                .addData(data);\n",
       "        }\n",
       "            geo_json_3c80d4eac25e019246d5a17d65a0fa42_add({&quot;bbox&quot;: [14.473475456, 13.150370789, 14.573475456, 13.650370789], &quot;features&quot;: [{&quot;bbox&quot;: [14.473475456, 13.150370789, 14.573475456, 13.650370789], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[14.473475456, 13.150370789], [14.473475456, 13.250370789], [14.473475456, 13.350370789], [14.473475456, 13.450370789], [14.473475456, 13.550370789], [14.473475456, 13.650370789], [14.573475456, 13.650370789], [14.573475456, 13.550370789], [14.573475456, 13.450370789], [14.573475456, 13.350370789], [14.573475456, 13.250370789], [14.573475456, 13.150370789], [14.473475456, 13.150370789]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {}, &quot;type&quot;: &quot;Feature&quot;}], &quot;type&quot;: &quot;FeatureCollection&quot;});\n",
       "        geo_json_3c80d4eac25e019246d5a17d65a0fa42.setStyle(function(feature) {return feature.properties.style;});\n",
       "\n",
       "        \n",
       "    \n",
       "            geo_json_3c80d4eac25e019246d5a17d65a0fa42.addTo(map_816c36d5b1143ced3a999b3ca380aedb);\n",
       "        \n",
       "    \n",
       "            var marker_9259603bac7efda048c90e0884fee70b = L.marker(\n",
       "                [13.20037, 14.52348],\n",
       "                {}\n",
       "            ).addTo(map_816c36d5b1143ced3a999b3ca380aedb);\n",
       "        \n",
       "    \n",
       "            var div_icon_bc4a7951636bd24e7fc9b9e9ea4b5fb1 = L.divIcon({&quot;className&quot;: &quot;empty&quot;, &quot;html&quot;: &quot;\\u003cdiv style=\\&quot;font-size: 10pt\\&quot;\\u003e1\\u003c/div\\u003e&quot;, &quot;iconAnchor&quot;: [0, 0], &quot;iconSize&quot;: [10, 10]});\n",
       "            marker_9259603bac7efda048c90e0884fee70b.setIcon(div_icon_bc4a7951636bd24e7fc9b9e9ea4b5fb1);\n",
       "        \n",
       "    \n",
       "        var popup_a8a781ee21761d1afd8765b3d534e152 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_b875a8c4ad580455f1297c7a05270984 = $(`&lt;div id=&quot;html_b875a8c4ad580455f1297c7a05270984&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Centroid: 13.20037, 14.52348&lt;/div&gt;`)[0];\n",
       "                popup_a8a781ee21761d1afd8765b3d534e152.setContent(html_b875a8c4ad580455f1297c7a05270984);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_9259603bac7efda048c90e0884fee70b.bindPopup(popup_a8a781ee21761d1afd8765b3d534e152)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_b42d26e31ab5f3c60f5f34e6172c270d = L.marker(\n",
       "                [13.30037, 14.52348],\n",
       "                {}\n",
       "            ).addTo(map_816c36d5b1143ced3a999b3ca380aedb);\n",
       "        \n",
       "    \n",
       "            var div_icon_2521263397ca14f0137ac62ce4ffcb6b = L.divIcon({&quot;className&quot;: &quot;empty&quot;, &quot;html&quot;: &quot;\\u003cdiv style=\\&quot;font-size: 10pt\\&quot;\\u003e2\\u003c/div\\u003e&quot;, &quot;iconAnchor&quot;: [0, 0], &quot;iconSize&quot;: [10, 10]});\n",
       "            marker_b42d26e31ab5f3c60f5f34e6172c270d.setIcon(div_icon_2521263397ca14f0137ac62ce4ffcb6b);\n",
       "        \n",
       "    \n",
       "        var popup_cc613eff3a05d9e61179f60a116e7b51 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_48f4ad268c1358c77d52694a0d950e09 = $(`&lt;div id=&quot;html_48f4ad268c1358c77d52694a0d950e09&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Centroid: 13.30037, 14.52348&lt;/div&gt;`)[0];\n",
       "                popup_cc613eff3a05d9e61179f60a116e7b51.setContent(html_48f4ad268c1358c77d52694a0d950e09);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_b42d26e31ab5f3c60f5f34e6172c270d.bindPopup(popup_cc613eff3a05d9e61179f60a116e7b51)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            map_816c36d5b1143ced3a999b3ca380aedb.fitBounds(\n",
       "                [[13.150370789, 14.473475456], [13.650370788999998, 14.673475455999998]],\n",
       "                {}\n",
       "            );\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7efc4b205b40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Input File Path</td>\n",
       "      <td>input/Lake Chad.geojson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Area</td>\n",
       "      <td>636.94 KM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Area with Buffer</td>\n",
       "      <td>636.94 KM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cell Size</td>\n",
       "      <td>0.1 deg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total Cells</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Test Cells</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Parameter                    Value\n",
       "0   Input File Path  input/Lake Chad.geojson\n",
       "1              Area               636.94 KM2\n",
       "2  Area with Buffer               636.94 KM2\n",
       "3         Cell Size                  0.1 deg\n",
       "4       Total Cells                       10\n",
       "5        Test Cells                        2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load file from sandbox disc. file should be present in 'input' folder\n",
    "c, grid, adm_df = add_adm(\n",
    "    shp, buffer, cell_size, test_cells\n",
    ")  # (shp, boundary_buffer, cell_size)\n",
    "adm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and Run Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32mGRID PROCESSED AND UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Init Table\n",
    "count_table = gen_table()\n",
    "\n",
    "# Calls the checklist function\n",
    "if run_checks:\n",
    "    e_log = exec_checks(c, cell_size / 2, count_table)\n",
    "else:\n",
    "    e_log = gen_aoim(c, cell_size / 2, count_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.misc import get_maxmonth\n",
    "\n",
    "get_maxmonth(\"output/csv/{}.csv\".format(cwc_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processed and error shapefiles as grids in output/shape/\n",
    "create_shapefile(c, \"processed_file\")\n",
    "if len(e_log) > 0:\n",
    "    create_shapefile(e_log, \"error_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run application for cells logged in e_log, if required\n",
    "if len(e_log) > 0:\n",
    "    e_log = gen_aoim(e_log, cell_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

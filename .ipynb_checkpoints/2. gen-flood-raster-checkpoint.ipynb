{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE REQUIREMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# !pip install python-dotenv\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings \n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as rio\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy.ndimage.filters import uniform_filter\n",
    "from scipy.ndimage.measurements import variance\n",
    "from skimage.filters import threshold_minimum\n",
    "from datacube.utils.geometry import Geometry\n",
    "\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.plotting import display_map, rgb\n",
    "from deafrica_tools.areaofinterest import define_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-Drive Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import google.auth\n",
    "\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# If modifying these scopes, delete the file credentials.\n",
    "SCOPES = [\"https://www.googleapis.com/auth/drive\"]\n",
    "credential_path = '../Supplementary_data/DriveCredentials/credentials.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "\n",
    "Connect to the datacube so we can access DEA data.\n",
    "The `app` parameter is a unique name for the analysis which is based on the notebook file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"Radar_water_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G-DRIVE FUNCTION DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token():\n",
    "    ''''\n",
    "        credential: provide the json creditials you would get from google service.\n",
    "    '''\n",
    "    creds = None\n",
    "    creds = service_account.Credentials.from_service_account_file(credential_path, scopes=SCOPES)\n",
    "    return creds\n",
    "\n",
    "def list_gdrive():\n",
    "    '''\n",
    "    List the 10 recent files from the google drive\n",
    "    '''\n",
    "    creds = create_token()\n",
    "    try:\n",
    "        service = build(\"drive\", \"v3\", credentials=creds)\n",
    "      \n",
    "        results = (service.files().list(pageSize=20, fields=\"nextPageToken, files(id, name)\").execute())\n",
    "        items = results.get(\"files\", [])\n",
    "\n",
    "        if not items:\n",
    "            print(\"No files found.\")\n",
    "            return\n",
    "        print(\"Files:\")\n",
    "        for item in items:\n",
    "            print(f\"{item['name']} ({item['id']})\")\n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "\n",
    "\n",
    "def upload_to_gdrive(file_paths, folder_id=None):\n",
    "    '''\n",
    "        Uploading files to google drive\n",
    "    '''\n",
    "    creds = create_token()\n",
    "\n",
    "    for f_path in file_paths:\n",
    "        try:\n",
    "            # create drive api client\n",
    "            service = build(\"drive\", \"v3\", credentials=creds)\n",
    "            folder_path = '../Supplementary_data/DriveCredentials/{}'.format(folder_id)\n",
    "            \n",
    "            f_name = os.path.basename(f_path)\n",
    "            file_metadata = {\"name\": f_name, \"parents\": [folder_id]}\n",
    "            media = MediaFileUpload(f_path, resumable=True)\n",
    "            # pylint: disable=maybe-no-member\n",
    "            \n",
    "            file = (service.files().create(body=file_metadata, media_body=media).execute())\n",
    "            print('\\033[32m' + \"FILE UPLOADED SUCCESSFULLY\" + '\\033[0m')\n",
    "    \n",
    "            # DELETE FROM SANDBOX TO SAVE DISC MEMORY\n",
    "            os.remove(f_path)\n",
    "        except HttpError as error:\n",
    "            print(f\"An error occurred: {error}\")\n",
    "            file = None\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUSTOMIZE DATA\n",
    "In the next cells define the flood dates, add the threshold value and grid vector file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Flooding Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE PERIODS\n",
    "#timeframe\n",
    "timerange = ('2024-02', '2024-09')\n",
    "\n",
    "pre_flood = ['2024-02', '2024-03', '2024-04'] # 3 MONTHS PRIOR\n",
    "flood = ['2024-05', '2024-06', '2024-07', '2024-08', '2024-09'] # MONTHS DURING\n",
    "# post_flood = ['2024-09-10', '2024-10-15']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Threshold from `1.aoi-threshold.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_aoi = -27.395682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload file and calculate centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FILE FROM SANDBOX\n",
    "grid = gpd.read_file(\"input/Lake Chad.geojson\")\n",
    "# grid = gpd.read_file(\"input/TCD_55KM_4CTEST.geojson\")\n",
    "\n",
    "# Calculate centroids and store in centroid list c[]. The array c[] will be used to loop all grid cells\n",
    "c = [] # INIT EMPTY GRID CENTROID\n",
    "g = grid.centroid # STORE ALL GRID CENTROIDS in g\n",
    "for i in g:\n",
    "    c.append([round(i.x, 5), round(i.y, 5)]) # EXTRACT x and y FROM POINT(x,y) AND APPEND TO c[]\n",
    "\n",
    "# AOI MOSAIC COVERING LAKE CHAD\n",
    "aoi_m = []\n",
    "for i in c:\n",
    "    aoi_m.append(define_area(i[1], i[0], buffer=0.05))\n",
    "    # aoi_m.append(define_area(i[1], i[0], buffer=0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RWD FUNCTION DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speckle Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to apply lee filtering on S1 image \n",
    "def lee_filter(da, size):\n",
    "    \"\"\"\n",
    "    Apply lee filter of specified window size.\n",
    "    Adapted from https://stackoverflow.com/questions/39785970/speckle-lee-filter-in-python\n",
    "\n",
    "    \"\"\"\n",
    "    img = da.values\n",
    "    img_mean = uniform_filter(img, size)\n",
    "    img_sqr_mean = uniform_filter(img**2, size)\n",
    "    img_variance = img_sqr_mean - img_mean**2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "    \n",
    "    return img_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S1_water_classifier(da, threshold):\n",
    "    water_data_array = da < threshold\n",
    "    return water_data_array.to_dataset(name=\"s1_water\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main For Loop Interation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32mPROCESSING GRID CELL 1/5\u001b[0m\n",
      "Using pixel quality parameters for Sentinel 1\n",
      "Finding datasets\n",
      "    s1_rtc\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 38 time steps\n",
      "\u001b[32mFILE UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mFILE UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mFILE UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mFILE UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32mPROCESSING GRID CELL 2/5\u001b[0m\n",
      "Using pixel quality parameters for Sentinel 1\n",
      "Finding datasets\n",
      "    s1_rtc\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 38 time steps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m lon_range \u001b[38;5;241m=\u001b[39m (geopolygon_gdf\u001b[38;5;241m.\u001b[39mtotal_bounds[\u001b[38;5;241m0\u001b[39m], geopolygon_gdf\u001b[38;5;241m.\u001b[39mtotal_bounds[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# LOAD SENTINEL-1 DATA\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m S1 \u001b[38;5;241m=\u001b[39m \u001b[43mload_ard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproducts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms1_rtc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmeasurements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlat_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlon_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimerange\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_crs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEPSG:6933\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgroup_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msolar_day\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnative\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# INIT TIMESTEPS \u001b[39;00m\n\u001b[1;32m     26\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m11\u001b[39m]\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/deafrica_tools/datahandling.py:627\u001b[0m, in \u001b[0;36mload_ard\u001b[0;34m(dc, products, min_gooddata, categories_to_mask_ls, categories_to_mask_s2, categories_to_mask_s1, mask_filters, mask_pixel_quality, ls7_slc_off, predicate, dtype, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ds\u001b[38;5;241m.\u001b[39mtime)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m time steps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/xarray/core/dataset.py:1043\u001b[0m, in \u001b[0;36mDataset.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading and/or computation of this dataset's data\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;124;03mfrom disk or a remote source into memory and return a new dataset.\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;124;03mUnlike load, the original dataset is left unaltered.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;124;03mdask.compute\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/xarray/core/dataset.py:870\u001b[0m, in \u001b[0;36mDataset.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m chunkmanager \u001b[38;5;241m=\u001b[39m get_chunked_array_type(\u001b[38;5;241m*\u001b[39mlazy_data\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    869\u001b[0m \u001b[38;5;66;03m# evaluate all the chunked arrays simultaneously\u001b[39;00m\n\u001b[0;32m--> 870\u001b[0m evaluated_data: \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, Any], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mchunkmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlazy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lazy_data, evaluated_data, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/xarray/namedarray/daskmanager.py:86\u001b[0m, in \u001b[0;36mDaskManager.compute\u001b[0;34m(self, *data, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mdata: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m     83\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, _DType_co], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/dask/base.py:660\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 660\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_val = 1\n",
    "\n",
    "for aoi in aoi_m:\n",
    "    print(\"\\n\\n\"+ '\\033[32m' + \"PROCESSING GRID CELL {}/{}\".format(grid_val, len(aoi_m)) + '\\033[0m')\n",
    "    geopolygon = Geometry(aoi[\"features\"][0][\"geometry\"], crs=\"epsg:4326\")\n",
    "    geopolygon_gdf = gpd.GeoDataFrame(geometry=[geopolygon], crs=geopolygon.crs)\n",
    "\n",
    "    # Get the latitude and longitude range of the geopolygon\n",
    "    lat_range = (geopolygon_gdf.total_bounds[1], geopolygon_gdf.total_bounds[3])\n",
    "    lon_range = (geopolygon_gdf.total_bounds[0], geopolygon_gdf.total_bounds[2])\n",
    "\n",
    "    # LOAD SENTINEL-1 DATA\n",
    "    S1 = load_ard(dc=dc,\n",
    "                products=[\"s1_rtc\"],\n",
    "                measurements=['vv', 'vh'],\n",
    "                y=lat_range,\n",
    "                x=lon_range,\n",
    "                time=timerange,\n",
    "                output_crs = \"EPSG:6933\",\n",
    "                resolution = (-20,20),\n",
    "                group_by=\"solar_day\",\n",
    "                dtype='native'\n",
    "                )\n",
    "    \n",
    "    # INIT TIMESTEPS \n",
    "    timesteps = [2,4,6,9,11]\n",
    "\n",
    "    # VH/VV is a potentially useful third feature after VV and VH \n",
    "    # S1['vh/vv'] = S1.vh/S1.vv  \n",
    "\n",
    "    # MEDIAN VALUES FOR SIMILAR RANGE FOR VISUALIZATION\n",
    "    # med_s1 = S1[['vv','vh','vh/vv']].median()  \n",
    "\n",
    "    # The lee filter above doesn't handle null values\n",
    "    # We therefore set null values to 0 before applying the filter\n",
    "    valid = np.isfinite(S1)\n",
    "    S1 = S1.where(valid, 0)\n",
    "\n",
    "    # Create a new entry in dataset corresponding to filtered VV and VH data\n",
    "    # S1[\"filtered_vv\"] = S1.vv.groupby(\"time\").apply(lee_filter, size=7)\n",
    "    S1[\"filtered_vh\"] = S1.vh.groupby(\"time\").apply(lee_filter, size=7)\n",
    "\n",
    "    # Null pixels should remain null\n",
    "    # S1['filtered_vv'] = S1.filtered_vv.where(valid.vv)\n",
    "    S1['filtered_vh'] = S1.filtered_vh.where(valid.vh)   \n",
    "\n",
    "    # Convert the digital numbers to dB\n",
    "    # S1['filtered_vv'] = 10 * np.log10(S1.filtered_vv)\n",
    "    S1['filtered_vh'] = 10 * np.log10(S1.filtered_vh)\n",
    "\n",
    "    threshold_vh = th_aoi\n",
    "\n",
    "    S1['water'] = S1_water_classifier(S1.filtered_vh, threshold_vh).s1_water\n",
    "    FS1 = S1.water\n",
    "    PRFS1 = S1.water\n",
    "\n",
    "    \n",
    "    # CREATE OUTPUTS\n",
    "    # EXPORT TO RASTERS - UPLOAD TO G-DRIVE - DELETE FROM SANDBOX\n",
    "    #PREFLOOD\n",
    "    S1_PreFlood = PRFS1.sel(time = pre_flood, method = 'nearest').mean(dim = 'time')\n",
    "    preflood_val = 'CELL_' + str(grid_val) + '_PRE_FLOOD_MEAN'\n",
    "    preflood_name = preflood_val +'.tif'\n",
    "    preflood_out = 'output/preflood/' + preflood_name\n",
    "    S1_PreFlood.rio.to_raster(preflood_out)\n",
    "    #PREFLOOD META\n",
    "    prf_meta_text = '### Meta Data - GRID CELL ID = ' + str(grid_val) + ' ###' + '\\n' 'Time Range' + pre_flood[0] + ' - ' + pre_flood[-1] + '\\n' + 'Lat Range' +  str(lat_range) + ' Lon Range' + str(lon_range) + '\\n' + 'Coordinate Reference System : ' + str(geopolygon.crs)\n",
    "    text_flie_name = preflood_val + '_META.txt'\n",
    "    prf_meta_path = 'output/preflood/' + text_flie_name\n",
    "    with open(prf_meta_path, mode='w') as f:\n",
    "         f.write(prf_meta_text)\n",
    "    upload_to_gdrive([preflood_out, prf_meta_path], os.getenv(\"PREFLOOD_ID\"))\n",
    "\n",
    "    \n",
    "    #FLOOD\n",
    "    S1_Flood = FS1.sel(time = flood, method = 'nearest').mean(dim = 'time')\n",
    "    flood_val = 'CELL_' + str(grid_val) + '_FLOOD_MEAN'\n",
    "    flood_name = flood_val + '.tif'\n",
    "    flood_out = 'output/flood/' + flood_name\n",
    "    S1_Flood.rio.to_raster(flood_out)\n",
    "    #FLOOD META\n",
    "    f_meta_text = '### Meta Data - GRID CELL ID = ' + str(grid_val) + ' ###' + '\\n' 'Time Range' + flood[0] + ' - ' + flood[-1] + '\\n' + 'Lat Range' +  str(lat_range) + ' Lon Range' + str(lon_range) + '\\n' + 'Coordinate Reference System : ' + str(geopolygon.crs)\n",
    "    text_flie_name = flood_val + '_META.txt'\n",
    "    f_meta_path = 'output/flood/' + text_flie_name\n",
    "    with open(f_meta_path, mode='w') as f:\n",
    "         f.write(f_meta_text)\n",
    "    upload_to_gdrive([flood_out, f_meta_path], os.getenv(\"FLOOD_ID\"))\n",
    "\n",
    "    grid_val += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Outputs To Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['flood', 'preflood', 'postflood']\n",
    "\n",
    "for dir in dirs:\n",
    "    loc = \"output/\" + dir\n",
    "    out = \"output/{}/Merged_{}.tif\".format(dir, dir)\n",
    "    extension = \"*.tif\"\n",
    "    q = os.path.join(loc, extension)\n",
    "    files = glob.glob(q)\n",
    "\n",
    "    r =[]\n",
    "    for f in files:\n",
    "        s = rasterio.open(f)\n",
    "        r.append(s)\n",
    "    if len(r)>0:\n",
    "        mosaic, out_trans = merge(r)\n",
    "        out_meta = s.meta.copy()\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                    \"height\": mosaic.shape[1],\n",
    "                    \"width\": mosaic.shape[2],\n",
    "                    \"transform\": out_trans\n",
    "                    })\n",
    "        with rasterio.open(out, \"w\", **out_meta) as dest:\n",
    "            dest.write(mosaic)\n",
    "            # upload_to_gdrive(out, \"flood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

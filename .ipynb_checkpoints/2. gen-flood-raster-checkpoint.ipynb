{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE REQUIREMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# !pip install python-dotenv\n",
    "# load_dotenv()\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "import os, glob, warnings, datacube, rasterio, json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as rio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "\n",
    "from scipy.ndimage import uniform_filter\n",
    "from scipy.ndimage import variance\n",
    "from skimage.filters import threshold_minimum\n",
    "from datacube.utils.geometry import Geometry\n",
    "\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.plotting import display_map, rgb\n",
    "from deafrica_tools.areaofinterest import define_area\n",
    "\n",
    " # G-Drive Dependencies\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G-Drive Scopes and Credentials\n",
    "# 'creds' folder contains credentials.json - service account, credentials_2.json - Desktop APP and token.json - user account.\n",
    "SCOPES = [\"https://www.googleapis.com/auth/drive\"]\n",
    "credential_path = \"../creds/credentials_2.json\"\n",
    "u_credential_path = \"../creds/token.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "\n",
    "Connect to the datacube so we can access DEA data.\n",
    "The `app` parameter is a unique name for the analysis which is based on the notebook file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"Radar_water_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-Drive Folder IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder ids by copying the ID from the g-drive folder url\n",
    "\n",
    "FLOOD_MEAN_ID = \"196YsHy1SXjNDnja6LjVhhlvoIt-Jg91l\"\n",
    "FLOOD_MEDIAN_ID = \"1Qhum99pKi1Qyon8hcp4K8S_DGaJh5a5e\"\n",
    "PREFLOOD_MEAN_ID = \"1K0KqGlLxdUCsXg4771k_uf_x2UwbHlqa\"\n",
    "PREFLOOD_MEDIAN_ID = \"1Ovu5Q58xZRGpKsvowVr49klOrgVoZ0XQ\"\n",
    "\n",
    "FLOOD_MEAN_TEST_ID = \"1bv76i244wyJzfpWN57wM7Yh4jQ38gove\"\n",
    "PREFLOOD_MEAN_TEST_ID = \"1msS6VuX_8UptluKIuQG5PJNOrwqYtacY\"\n",
    "\n",
    "REF_TEST_PRF_ID = \"1LhCOlJ2Dt3HEGjuQFx_fIbBsZAS6eA-A\"\n",
    "REF_TEST_F_ID = \"1hmrlnQuh3-w_mGwCh0apCUoKd2BwXoyA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-Drive Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_token():\n",
    "    # Creates a user token. On first run, run locally to generate token.json and add to root.\n",
    "    creds = None\n",
    "\n",
    "    if os.path.exists(u_credential_path):\n",
    "        creds = Credentials.from_authorized_user_file(u_credential_path, SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(credential_path, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "\n",
    "    # Save the credentials for the next run\n",
    "    with open(u_credential_path, \"w\") as token:\n",
    "        token.write(creds.to_json())\n",
    "    return creds\n",
    "\n",
    "\n",
    "def create_token():\n",
    "    # Creates service token. Will be depreciated\n",
    "    creds = None\n",
    "    creds = service_account.Credentials.from_service_account_file(\n",
    "        credential_path, scopes=SCOPES\n",
    "    )\n",
    "    return creds\n",
    "\n",
    "\n",
    "def list_gdrive():\n",
    "    # Lists all the files and folders in the root directory of g-drive.\n",
    "    u_creds = create_user_token()\n",
    "    try:\n",
    "        service = build(\"drive\", \"v3\", credentials=u_creds)\n",
    "        results = (\n",
    "            service.files()\n",
    "            .list(pageSize=20, fields=\"nextPageToken, files(id, name)\")\n",
    "            .execute()\n",
    "        )\n",
    "        items = results.get(\"files\", [])\n",
    "        if not items:\n",
    "            print(\"No files found.\")\n",
    "            return\n",
    "        print(\"Files:\")\n",
    "        for item in items:\n",
    "            print(f\"{item['name']} ({item['id']})\")\n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "\n",
    "\n",
    "def upload_to_gdrive(file_paths, folder_id=None):\n",
    "    # Uploads list of files in file_paths to folder_id\n",
    "    u_creds = create_user_token()\n",
    "    for f_path in file_paths:\n",
    "        try:\n",
    "            # Create g-drive API client using desktop app and user credentials\n",
    "            service = build(\"drive\", \"v3\", credentials=u_creds)\n",
    "            folder_path = \"../Supplementary_data/DriveCredentials/{}\".format(folder_id)\n",
    "\n",
    "            f_name = os.path.basename(f_path)\n",
    "            file_metadata = {\"name\": f_name, \"parents\": [folder_id]}\n",
    "            media = MediaFileUpload(f_path, resumable=True)\n",
    "            # pylint: disable=maybe-no-member\n",
    "            file = (\n",
    "                service.files().create(body=file_metadata, media_body=media).execute()\n",
    "            )\n",
    "            print(\"\\033[32m\" + \"{} UPLOADED SUCCESSFULLY\".format(f_name) + \"\\033[0m\")\n",
    "\n",
    "            # Delete from sandbox to save disc space\n",
    "            os.remove(f_path)\n",
    "        except HttpError as error:\n",
    "            print(f\"An error occurred: {error}\")\n",
    "            file = None\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Classifier Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply lee filtering on S1 image. Speckle Filter\n",
    "def lee_filter(da, size):\n",
    "    \"\"\"\n",
    "    Apply lee filter of specified window size.\n",
    "    Adapted from https://stackoverflow.com/questions/39785970/speckle-lee-filter-in-python\n",
    "\n",
    "    \"\"\"\n",
    "    img = da.values\n",
    "    img_mean = uniform_filter(img, size)\n",
    "    img_sqr_mean = uniform_filter(img**2, size)\n",
    "    img_variance = img_sqr_mean - img_mean**2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "\n",
    "    return img_output\n",
    "\n",
    "# Classifier Function\n",
    "def S1_water_classifier(da, threshold):\n",
    "    water_data_array = da < threshold\n",
    "    return water_data_array.to_dataset(name=\"s1_water\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main time period of analysis\n",
    "timerange = ('2024-02', '2024-09')\n",
    "\n",
    "# Define sub-periods of analysis - should be within main time period\n",
    "pre_flood = ['2024-02', '2024-03', '2024-04'] \n",
    "flood = ['2024-05', '2024-06', '2024-07', '2024-08', '2024-09'] \n",
    "\n",
    "# Run 1. aoi-threshold.ipynb to get the value of th_aoi and store it here.\n",
    "th_aoi = -27.395682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_grid(aoi_m, c):\n",
    "    e_log = []\n",
    "    \n",
    "    for aoi, i in zip(aoi_m, c):\n",
    "        geopolygon = Geometry(aoi[\"features\"][0][\"geometry\"], crs=\"epsg:4326\")\n",
    "        geopolygon_gdf = gpd.GeoDataFrame(geometry=[geopolygon], crs=geopolygon.crs)\n",
    "        g = geopolygon_gdf.centroid\n",
    "        print(\n",
    "            \"\\n\\n\"\n",
    "            + \"\\033[32m\"\n",
    "            + \"PROCESSING GRID CELL {}/{} CENTROID ({}, {})\".format(\n",
    "                i[2], len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)\n",
    "            )\n",
    "            + \"\\033[0m\"\n",
    "        )\n",
    "    \n",
    "        # Get the latitude and longitude range of the geopolygon\n",
    "        lat_range = (geopolygon_gdf.total_bounds[1], geopolygon_gdf.total_bounds[3])\n",
    "        lon_range = (geopolygon_gdf.total_bounds[0], geopolygon_gdf.total_bounds[2])\n",
    "    \n",
    "        # Load Sentinel1 data\n",
    "        try:\n",
    "            S1 = load_ard(\n",
    "                dc=dc,\n",
    "                products=[\"s1_rtc\"],\n",
    "                measurements=[\"vv\", \"vh\"],\n",
    "                y=lat_range,\n",
    "                x=lon_range,\n",
    "                time=timerange,\n",
    "                output_crs=\"EPSG:6933\",\n",
    "                resolution=(-20, 20),\n",
    "                group_by=\"solar_day\",\n",
    "                dtype=\"native\",\n",
    "            )\n",
    "        except:\n",
    "            # Log error aoi centroids and keep looping\n",
    "            e_log.append([g.x[0], g.y[0], i[2], \"P\"])\n",
    "            print(\n",
    "                \"\\n\\n\"\n",
    "                + \"\\033[31m\"\n",
    "                + \"ERROR PROCESSING GRID CELL {}/{} CENTROID ({}, {}). LOGGED CENTROID INFO in e_log\".format(\n",
    "                    i[2], len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)\n",
    "                )\n",
    "                + \"\\033[0m\"\n",
    "            )\n",
    "            continue\n",
    "    \n",
    "        timesteps = [2, 4, 6, 9, 11]\n",
    "    \n",
    "        # The lee filter above doesn't handle null values\n",
    "        # We therefore set null values to 0 before applying the filter\n",
    "        valid = np.isfinite(S1)\n",
    "        S1 = S1.where(valid, 0)\n",
    "    \n",
    "        # Create a new entry in dataset corresponding to filtered VV and VH data\n",
    "        S1[\"filtered_vh\"] = S1.vh.groupby(\"time\").apply(lee_filter, size=7)\n",
    "    \n",
    "        # Null pixels should remain null\n",
    "        S1[\"filtered_vh\"] = S1.filtered_vh.where(valid.vh)\n",
    "    \n",
    "        # Convert the digital numbers to dB\n",
    "        S1[\"filtered_vh\"] = 10 * np.log10(S1.filtered_vh)\n",
    "    \n",
    "        threshold_vh = th_aoi\n",
    "    \n",
    "        S1[\"water\"] = S1_water_classifier(S1.filtered_vh, threshold_vh).s1_water\n",
    "        FS1 = S1.water\n",
    "        PRFS1 = S1.water\n",
    "    \n",
    "        # Creting outputs\n",
    "        # Export to raster - upload to g-drive - delete from sandbox\n",
    "        # --------------------------------------- preflood ----------------------------------------------\n",
    "        S1_PreFlood = PRFS1.sel(time=pre_flood, method=\"nearest\").mean(dim=\"time\")\n",
    "        preflood_val = \"CELL_\" + str(i[2]) + \"_PRE_FLOOD_MEAN\"\n",
    "        preflood_name = preflood_val + \".tif\"\n",
    "        preflood_out = \"output/preflood/\" + preflood_name\n",
    "        S1_PreFlood.rio.to_raster(preflood_out)\n",
    "        \n",
    "        # preflood meta\n",
    "        prf_meta_text = \"### Meta Data - GRID CELL ID = \" + str(\n",
    "            i[2]\n",
    "        ) + \" ###\" + \"\\n\" \"Time Range: \" + pre_flood[0] + \" - \" + pre_flood[\n",
    "            -1\n",
    "        ] + \"\\n\" + \"Lat Range: \" + str(\n",
    "            lat_range\n",
    "        ) + \" Lon Range: \" + str(\n",
    "            lon_range\n",
    "        ) + \"\\n\" + \"Coordinate Reference System: \" + str(\n",
    "            geopolygon.crs\n",
    "        )\n",
    "        \n",
    "        text_flie_name = preflood_val + \"_META.txt\"\n",
    "        prf_meta_path = \"output/preflood/\" + text_flie_name\n",
    "        with open(prf_meta_path, mode=\"w\") as f:\n",
    "            f.write(prf_meta_text)\n",
    "        try:\n",
    "            upload_to_gdrive([preflood_out, prf_meta_path], REF_TEST_PRF_ID)\n",
    "        except:\n",
    "            e_log.append([g.x[0], g.y[0], i[2], \"U-PRF\"])\n",
    "            print(\n",
    "                \"\\n\\n\"\n",
    "                + \"\\033[31m\"\n",
    "                + \"ERROR UPLOADING GRID CELL {}/{} CENTROID ({}, {}). LOGGED CENTROID INFO in e_log\".format(\n",
    "                    i[2], len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)\n",
    "                )\n",
    "                + \"\\033[0m\"\n",
    "            )\n",
    "            continue\n",
    "    \n",
    "        # ----------------------------------------- flood ----------------------------------------------\n",
    "        S1_Flood = FS1.sel(time=flood, method=\"nearest\").mean(dim=\"time\")\n",
    "        flood_val = \"CELL_\" + str(i[2]) + \"_FLOOD_MEAN\"\n",
    "        flood_name = flood_val + \".tif\"\n",
    "        flood_out = \"output/flood/\" + flood_name\n",
    "        S1_Flood.rio.to_raster(flood_out)\n",
    "        \n",
    "        # flood meta\n",
    "        f_meta_text = \"### Meta Data - GRID CELL ID = \" + str(\n",
    "            i[2]\n",
    "        ) + \" ###\" + \"\\n\" \"Time Range: \" + flood[0] + \" - \" + flood[\n",
    "            -1\n",
    "        ] + \"\\n\" + \"Lat Range: \" + str(\n",
    "            lat_range\n",
    "        ) + \" Lon Range: \" + str(\n",
    "            lon_range\n",
    "        ) + \"\\n\" + \"Coordinate Reference System: \" + str(\n",
    "            geopolygon.crs\n",
    "        )\n",
    "        \n",
    "        text_flie_name = flood_val + \"_META.txt\"\n",
    "        f_meta_path = \"output/flood/\" + text_flie_name\n",
    "        with open(f_meta_path, mode=\"w\") as f:\n",
    "            f.write(f_meta_text)\n",
    "        try:\n",
    "            upload_to_gdrive([flood_out, f_meta_path], REF_TEST_F_ID)\n",
    "        except:\n",
    "            e_log.append([g.x[0], g.y[0], i[2], \"U-F\"])\n",
    "            print(\n",
    "                \"\\n\\n\"\n",
    "                + \"\\033[31m\"\n",
    "                + \"ERROR UPLOADING GRID CELL {}/{} CENTROID ({}, {}). LOGGED CENTROID INFO in e_log\".format(\n",
    "                    i[2], len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)\n",
    "                )\n",
    "                + \"\\033[0m\"\n",
    "            )\n",
    "            continue\n",
    "    \n",
    "        clear_output()\n",
    "    \n",
    "    if len(e_log) == 0:\n",
    "        print(\"\\n\\n\" + \"\\033[32m\" + \"GRID PROCESSED AND UPLOADED SUCCESSFULLY\" + \"\\033[0m\" + \"\\n\\n\")\n",
    "\n",
    "    # return e_log to be run again\n",
    "    return e_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crete the aoi-mosaic - aoi_m\n",
    "def gen_aoim(c, b):\n",
    "    aoi_m = []\n",
    "    for i in c:\n",
    "        aoi_m.append(define_area(i[1], i[0], buffer=b))\n",
    "    print(c, len(aoi_m))\n",
    "    e_log = iterate_grid(aoi_m, c)\n",
    "\n",
    "    # return e_log to be run again\n",
    "    return e_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload file and calculate centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32mGRID PROCESSED AND UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\n",
      "\n",
      "[] 0\n"
     ]
    }
   ],
   "source": [
    "# Load file from sandbox disc. file should be present in 'input' folder\n",
    "\n",
    "grid = gpd.read_file(\"input/Lake Chad.geojson\")\n",
    "# grid = gpd.read_file(\"input/TCD_55KM_4CTEST.geojson\")\n",
    "# grid = gpd.read_file(\"input/TCD_55KM_BASE.geojson\")\n",
    "\n",
    "# Calculate centroids and store in centroid list c[]. \n",
    "# The array c[] has four values - x, y, cell_id and None. None will store the \"P\" or \"U\" error value\n",
    "c = []\n",
    "g = grid.centroid\n",
    "cell_id = 1\n",
    "for i in g:\n",
    "    c.append([round(i.x, 5), round(i.y, 5), cell_id, None])\n",
    "    cell_id +=1\n",
    "\n",
    "# get e_log with centroids, cell_id and error message\n",
    "e_log = gen_aoim(c, 0.05)\n",
    "\n",
    "# print error log\n",
    "print(e_log, len(e_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "e_log = np.array(e_log)\n",
    "with open('error_centroids.json', 'w') as filehandle:\n",
    "    json.dump(e_log.tolist(), filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Outputs To Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['flood', 'preflood', 'postflood']\n",
    "\n",
    "for dir in dirs:\n",
    "    loc = \"output/\" + dir\n",
    "    out = \"output/{}/Merged_{}.tif\".format(dir, dir)\n",
    "    extension = \"*.tif\"\n",
    "    q = os.path.join(loc, extension)\n",
    "    files = glob.glob(q)\n",
    "\n",
    "    r =[]\n",
    "    for f in files:\n",
    "        s = rasterio.open(f)\n",
    "        r.append(s)\n",
    "    if len(r)>0:\n",
    "        mosaic, out_trans = merge(r)\n",
    "        out_meta = s.meta.copy()\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                    \"height\": mosaic.shape[1],\n",
    "                    \"width\": mosaic.shape[2],\n",
    "                    \"transform\": out_trans\n",
    "                    })\n",
    "        with rasterio.open(out, \"w\", **out_meta) as dest:\n",
    "            dest.write(mosaic)\n",
    "            # upload_to_gdrive(out, \"flood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'storageQuota': {'limit': '2199023255552',\n",
       "  'usage': '33351827375',\n",
       "  'usageInDrive': '33351827375',\n",
       "  'usageInDriveTrash': '0'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creds = create_user_token()\n",
    "service = build(\"drive\", \"v3\", credentials=creds)\n",
    "results = service.files().list(fields = \"files(id)\", pageSize = 1000).execute()\n",
    "items = results.get('files')\n",
    "print(len(items))\n",
    "storage = service.about().get(fields = \"storageQuota\").execute()\n",
    "storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in items:\n",
    "#     try:\n",
    "#         response = service.files().delete(fileId=item['id']).execute()\n",
    "#         print('File Deleted')\n",
    "#     except HttpError as error:\n",
    "#         print(f\"An error occurred: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE REQUIREMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "!pip install python-dotenv\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings \n",
    "import datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as rio\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy.ndimage.filters import uniform_filter\n",
    "from scipy.ndimage.measurements import variance\n",
    "from skimage.filters import threshold_minimum\n",
    "from datacube.utils.geometry import Geometry\n",
    "\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.plotting import display_map, rgb\n",
    "from deafrica_tools.areaofinterest import define_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-Drive Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import google.auth\n",
    "\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# If modifying these scopes, delete the file credentials.\n",
    "SCOPES = [\"https://www.googleapis.com/auth/drive\"]\n",
    "credential_path = '../Supplementary_data/DriveCredentials/credentials.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "\n",
    "Connect to the datacube so we can access DEA data.\n",
    "The `app` parameter is a unique name for the analysis which is based on the notebook file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"Radar_water_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G-DRIVE FUNCTION DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token():\n",
    "    ''''\n",
    "        credential: provide the json creditials you would get from google service.\n",
    "    '''\n",
    "    creds = None\n",
    "    creds = service_account.Credentials.from_service_account_file(credential_path, scopes=SCOPES)\n",
    "    return creds\n",
    "\n",
    "def list_gdrive():\n",
    "    '''\n",
    "    List the 10 recent files from the google drive\n",
    "    '''\n",
    "    creds = create_token()\n",
    "    try:\n",
    "        service = build(\"drive\", \"v3\", credentials=creds)\n",
    "      \n",
    "        results = (service.files().list(pageSize=20, fields=\"nextPageToken, files(id, name)\").execute())\n",
    "        items = results.get(\"files\", [])\n",
    "\n",
    "        if not items:\n",
    "            print(\"No files found.\")\n",
    "            return\n",
    "        print(\"Files:\")\n",
    "        for item in items:\n",
    "            print(f\"{item['name']} ({item['id']})\")\n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "\n",
    "\n",
    "def upload_to_gdrive(file_paths, folder_id=None):\n",
    "    '''\n",
    "        Uploading files to google drive\n",
    "    '''\n",
    "    creds = create_token()\n",
    "\n",
    "    for f_path in file_paths:\n",
    "        try:\n",
    "            # create drive api client\n",
    "            service = build(\"drive\", \"v3\", credentials=creds)\n",
    "            folder_path = '../Supplementary_data/DriveCredentials/{}'.format(folder_id)\n",
    "            \n",
    "            f_name = os.path.basename(f_path)\n",
    "            file_metadata = {\"name\": f_name, \"parents\": [folder_id]}\n",
    "            media = MediaFileUpload(f_path, resumable=True)\n",
    "            # pylint: disable=maybe-no-member\n",
    "            \n",
    "            file = (service.files().create(body=file_metadata, media_body=media).execute())\n",
    "            print('\\033[32m' + \"{} UPLOADED SUCCESSFULLY\".format(f_name) + '\\033[0m')\n",
    "    \n",
    "            # DELETE FROM SANDBOX TO SAVE DISC MEMORY\n",
    "            os.remove(f_path)\n",
    "        except HttpError as error:\n",
    "            print(f\"An error occurred: {error}\")\n",
    "            file = None\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUSTOMIZE DATA\n",
    "In the next cells define the flood dates, add the threshold value and grid vector file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Flooding Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE PERIODS\n",
    "#timeframe\n",
    "timerange = ('2024-02', '2024-09')\n",
    "\n",
    "pre_flood = ['2024-02', '2024-03', '2024-04'] # 3 MONTHS PRIOR\n",
    "flood = ['2024-05', '2024-06', '2024-07', '2024-08', '2024-09'] # MONTHS DURING\n",
    "# post_flood = ['2024-09-10', '2024-10-15']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Threshold from `1.aoi-threshold.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_aoi = -27.395682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload file and calculate centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FILE FROM SANDBOX\n",
    "grid = gpd.read_file(\"input/Lake Chad.geojson\")\n",
    "# grid = gpd.read_file(\"input/TCD_55KM_BASE.geojson\")\n",
    "\n",
    "# Calculate centroids and store in centroid list c[]. The array c[] will be used to loop all grid cells\n",
    "c = [] # INIT EMPTY GRID CENTROID\n",
    "g = grid.centroid # STORE ALL GRID CENTROIDS in g\n",
    "for i in g:\n",
    "    c.append([round(i.x, 5), round(i.y, 5)]) # EXTRACT x and y FROM POINT(x,y) AND APPEND TO c[]\n",
    "\n",
    "# AOI MOSAIC COVERING LAKE CHAD\n",
    "aoi_m = []\n",
    "for i in c:\n",
    "    aoi_m.append(define_area(i[1], i[0], buffer=0.05))\n",
    "    # aoi_m.append(define_area(i[1], i[0], buffer=0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RWD FUNCTION DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speckle Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to apply lee filtering on S1 image \n",
    "def lee_filter(da, size):\n",
    "    \"\"\"\n",
    "    Apply lee filter of specified window size.\n",
    "    Adapted from https://stackoverflow.com/questions/39785970/speckle-lee-filter-in-python\n",
    "\n",
    "    \"\"\"\n",
    "    img = da.values\n",
    "    img_mean = uniform_filter(img, size)\n",
    "    img_sqr_mean = uniform_filter(img**2, size)\n",
    "    img_variance = img_sqr_mean - img_mean**2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "    \n",
    "    return img_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S1_water_classifier(da, threshold):\n",
    "    water_data_array = da < threshold\n",
    "    return water_data_array.to_dataset(name=\"s1_water\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main For Loop Interation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32mPROCESSING GRID CELL 1/5 CENTROID (13.60037, 14.52348)\u001b[0m\n",
      "Using pixel quality parameters for Sentinel 1\n",
      "Finding datasets\n",
      "    s1_rtc\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 38 time steps\n",
      "\u001b[32mCELL_1_PRE_FLOOD_MEDIAN.tif UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mCELL_1_PRE_FLOOD_MEDIAN_META.txt UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mCELL_1_FLOOD_MEDIAN.tif UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mCELL_1_FLOOD_MEDIAN_META.txt UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32mPROCESSING GRID CELL 2/5 CENTROID (13.50037, 14.52348)\u001b[0m\n",
      "Using pixel quality parameters for Sentinel 1\n",
      "Finding datasets\n",
      "    s1_rtc\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 38 time steps\n",
      "\n",
      "\n",
      "\u001b[32mPROCESSING GRID CELL 2/5 CENTROID (13.40037, 14.52348)\u001b[0m\n",
      "Using pixel quality parameters for Sentinel 1\n",
      "Finding datasets\n",
      "    s1_rtc\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 38 time steps\n",
      "\u001b[32mCELL_2_PRE_FLOOD_MEDIAN.tif UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mCELL_2_PRE_FLOOD_MEDIAN_META.txt UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mCELL_2_FLOOD_MEDIAN.tif UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mCELL_2_FLOOD_MEDIAN_META.txt UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32mPROCESSING GRID CELL 3/5 CENTROID (13.30037, 14.52348)\u001b[0m\n",
      "Using pixel quality parameters for Sentinel 1\n",
      "Finding datasets\n",
      "    s1_rtc\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 38 time steps\n",
      "\u001b[32mCELL_3_PRE_FLOOD_MEDIAN.tif UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mCELL_3_PRE_FLOOD_MEDIAN_META.txt UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mCELL_3_FLOOD_MEDIAN.tif UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mCELL_3_FLOOD_MEDIAN_META.txt UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32mPROCESSING GRID CELL 4/5 CENTROID (13.20037, 14.52348)\u001b[0m\n",
      "Using pixel quality parameters for Sentinel 1\n",
      "Finding datasets\n",
      "    s1_rtc\n",
      "Applying pixel quality/cloud mask\n",
      "Loading 38 time steps\n",
      "\u001b[32mCELL_4_PRE_FLOOD_MEDIAN.tif UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mCELL_4_PRE_FLOOD_MEDIAN_META.txt UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mCELL_4_FLOOD_MEDIAN.tif UPLOADED SUCCESSFULLY\u001b[0m\n",
      "\u001b[32mCELL_4_FLOOD_MEDIAN_META.txt UPLOADED SUCCESSFULLY\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "grid_val = 1\n",
    "e_log =[]\n",
    "\n",
    "for aoi in aoi_m:\n",
    "    geopolygon = Geometry(aoi[\"features\"][0][\"geometry\"], crs=\"epsg:4326\")\n",
    "    geopolygon_gdf = gpd.GeoDataFrame(geometry=[geopolygon], crs=geopolygon.crs)\n",
    "    g = geopolygon_gdf.centroid\n",
    "    print(\"\\n\\n\"+ '\\033[32m' + \"PROCESSING GRID CELL {}/{} CENTROID ({}, {})\".format(grid_val, len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)) + '\\033[0m')\n",
    "\n",
    "    # Get the latitude and longitude range of the geopolygon\n",
    "    lat_range = (geopolygon_gdf.total_bounds[1], geopolygon_gdf.total_bounds[3])\n",
    "    lon_range = (geopolygon_gdf.total_bounds[0], geopolygon_gdf.total_bounds[2])\n",
    "\n",
    "    # LOAD SENTINEL-1 DATA\n",
    "    try:\n",
    "        S1 = load_ard(dc=dc,\n",
    "                    products=[\"s1_rtc\"],\n",
    "                    measurements=['vv', 'vh'],\n",
    "                    y=lat_range,\n",
    "                    x=lon_range,\n",
    "                    time=timerange,\n",
    "                    output_crs = \"EPSG:6933\",\n",
    "                    resolution = (-20,20),\n",
    "                    group_by=\"solar_day\",\n",
    "                    dtype='native'\n",
    "                    )\n",
    "    except:\n",
    "        # LOG ERROR aoi CENTROIDS AND CONTINUE LOOPING\n",
    "        e_log.append([g.y[0], g.x[0]])\n",
    "        print(\"\\n\\n\"+ '\\033[31m' + \"ERROR PROCESSING GRID CELL {}/{} CENTROID ({}, {}). LOGGED CENTROID INFO in e_log\".format(grid_val, len(aoi_m), round(g.y[0], 5), round(g.x[0], 5)) + '\\033[0m')\n",
    "        continue\n",
    "    \n",
    "    # INIT TIMESTEPS \n",
    "    timesteps = [2,4,6,9,11] \n",
    "\n",
    "    # The lee filter above doesn't handle null values\n",
    "    # We therefore set null values to 0 before applying the filter\n",
    "    valid = np.isfinite(S1)\n",
    "    S1 = S1.where(valid, 0)\n",
    "\n",
    "    # Create a new entry in dataset corresponding to filtered VV and VH data\n",
    "    # S1[\"filtered_vv\"] = S1.vv.groupby(\"time\").apply(lee_filter, size=7)\n",
    "    S1[\"filtered_vh\"] = S1.vh.groupby(\"time\").apply(lee_filter, size=7)\n",
    "\n",
    "    # Null pixels should remain null\n",
    "    # S1['filtered_vv'] = S1.filtered_vv.where(valid.vv)\n",
    "    S1['filtered_vh'] = S1.filtered_vh.where(valid.vh)   \n",
    "\n",
    "    # Convert the digital numbers to dB\n",
    "    # S1['filtered_vv'] = 10 * np.log10(S1.filtered_vv)\n",
    "    S1['filtered_vh'] = 10 * np.log10(S1.filtered_vh)\n",
    "\n",
    "    threshold_vh = th_aoi\n",
    "\n",
    "    S1['water'] = S1_water_classifier(S1.filtered_vh, threshold_vh).s1_water\n",
    "    FS1 = S1.water\n",
    "    PRFS1 = S1.water\n",
    "\n",
    "    \n",
    "    # CREATE OUTPUTS\n",
    "    # EXPORT TO RASTERS - UPLOAD TO G-DRIVE - DELETE FROM SANDBOX\n",
    "    #PREFLOOD\n",
    "    S1_PreFlood = PRFS1.sel(time = pre_flood, method = 'nearest').median(dim = 'time')\n",
    "    preflood_val = 'CELL_' + str(grid_val) + '_PRE_FLOOD_MEDIAN'\n",
    "    preflood_name = preflood_val +'.tif'\n",
    "    preflood_out = 'output/preflood/' + preflood_name\n",
    "    S1_PreFlood.rio.to_raster(preflood_out)\n",
    "    #PREFLOOD META\n",
    "    prf_meta_text = '### Meta Data - GRID CELL ID = ' + str(grid_val) + ' ###' + '\\n' 'Time Range: ' + pre_flood[0] + ' - ' + pre_flood[-1] + '\\n' + 'Lat Range: ' +  str(lat_range) + ' Lon Range: ' + str(lon_range) + '\\n' + 'Coordinate Reference System: ' + str(geopolygon.crs)\n",
    "    text_flie_name = preflood_val + '_META.txt'\n",
    "    prf_meta_path = 'output/preflood/' + text_flie_name\n",
    "    with open(prf_meta_path, mode='w') as f:\n",
    "         f.write(prf_meta_text)\n",
    "    upload_to_gdrive([preflood_out, prf_meta_path], os.getenv(\"PREFLOOD_ID\"))\n",
    "\n",
    "    \n",
    "    #FLOOD\n",
    "    S1_Flood = FS1.sel(time = flood, method = 'nearest').median(dim = 'time')\n",
    "    flood_val = 'CELL_' + str(grid_val) + '_FLOOD_MEDIAN'\n",
    "    flood_name = flood_val + '.tif'\n",
    "    flood_out = 'output/flood/' + flood_name\n",
    "    S1_Flood.rio.to_raster(flood_out)\n",
    "    #FLOOD META\n",
    "    f_meta_text = '### Meta Data - GRID CELL ID = ' + str(grid_val) + ' ###' + '\\n' 'Time Range: ' + flood[0] + ' - ' + flood[-1] + '\\n' + 'Lat Range: ' +  str(lat_range) + ' Lon Range: ' + str(lon_range) + '\\n' + 'Coordinate Reference System: ' + str(geopolygon.crs)\n",
    "    text_flie_name = flood_val + '_META.txt'\n",
    "    f_meta_path = 'output/flood/' + text_flie_name\n",
    "    with open(f_meta_path, mode='w') as f:\n",
    "         f.write(f_meta_text)\n",
    "    upload_to_gdrive([flood_out, f_meta_path], os.getenv(\"FLOOD_ID\"))\n",
    "\n",
    "    grid_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'e_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# PRINT ERROR DATA\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43me_log\u001b[49m, \u001b[38;5;28mlen\u001b[39m(e_log))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'e_log' is not defined"
     ]
    }
   ],
   "source": [
    "# PRINT ERROR DATA\n",
    "print(e_log, len(e_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Outputs To Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['flood', 'preflood', 'postflood']\n",
    "\n",
    "for dir in dirs:\n",
    "    loc = \"output/\" + dir\n",
    "    out = \"output/{}/Merged_{}.tif\".format(dir, dir)\n",
    "    extension = \"*.tif\"\n",
    "    q = os.path.join(loc, extension)\n",
    "    files = glob.glob(q)\n",
    "\n",
    "    r =[]\n",
    "    for f in files:\n",
    "        s = rasterio.open(f)\n",
    "        r.append(s)\n",
    "    if len(r)>0:\n",
    "        mosaic, out_trans = merge(r)\n",
    "        out_meta = s.meta.copy()\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                    \"height\": mosaic.shape[1],\n",
    "                    \"width\": mosaic.shape[2],\n",
    "                    \"transform\": out_trans\n",
    "                    })\n",
    "        with rasterio.open(out, \"w\", **out_meta) as dest:\n",
    "            dest.write(mosaic)\n",
    "            # upload_to_gdrive(out, \"flood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
